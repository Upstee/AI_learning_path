# å®æˆ˜é¡¹ç›®ç»¼åˆ

## 1. è¯¾ç¨‹æ¦‚è¿°

### è¯¾ç¨‹ç›®æ ‡
1. æ•´åˆç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„çŸ¥è¯†ï¼Œå®Œæˆç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ é¡¹ç›®
2. æŒæ¡å®Œæ•´çš„æ•°æ®ç§‘å­¦å·¥ä½œæµç¨‹ï¼ˆé—®é¢˜å®šä¹‰ã€æ•°æ®æ”¶é›†ã€æ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€éƒ¨ç½²ï¼‰
3. èƒ½å¤Ÿå¤„ç†çœŸå®ä¸–ç•Œçš„æ•°æ®é—®é¢˜ï¼ˆç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€ç±»åˆ«ä¸å¹³è¡¡ç­‰ï¼‰
4. èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡
5. èƒ½å¤Ÿä¼˜åŒ–æ¨¡å‹æ€§èƒ½å¹¶è§£é‡Šç»“æœ
6. èƒ½å¤Ÿæ’°å†™é¡¹ç›®æŠ¥å‘Šå’Œå±•ç¤ºæˆæœ

### é¢„è®¡å­¦ä¹ æ—¶é—´
- **ç†è®ºå­¦ä¹ **ï¼š8-10å°æ—¶
- **é¡¹ç›®å®è·µ**ï¼š40-60å°æ—¶
- **æŠ¥å‘Šæ’°å†™**ï¼š10-15å°æ—¶
- **æ€»è®¡**ï¼š58-85å°æ—¶ï¼ˆçº¦2-3ä¸ªæœˆï¼‰

### éš¾åº¦ç­‰çº§
- **ä¸­ç­‰åˆ°é«˜çº§** - éœ€è¦ç»¼åˆè¿ç”¨æ‰€æœ‰çŸ¥è¯†

### è¯¾ç¨‹å®šä½
- **å‰ç½®è¯¾ç¨‹**ï¼š01_ç›‘ç£å­¦ä¹ ã€02_æ— ç›‘ç£å­¦ä¹ ã€03_æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–
- **åç»­è¯¾ç¨‹**ï¼š05_æ·±åº¦å­¦ä¹ åŸºç¡€
- **åœ¨ä½“ç³»ä¸­çš„ä½ç½®**ï¼šç»¼åˆåº”ç”¨æ‰€æœ‰æœºå™¨å­¦ä¹ çŸ¥è¯†ï¼Œä¸ºå®é™…å·¥ä½œåšå‡†å¤‡

### å­¦å®Œèƒ½åšä»€ä¹ˆ
- èƒ½å¤Ÿç‹¬ç«‹å®Œæˆç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ é¡¹ç›®
- èƒ½å¤Ÿå¤„ç†çœŸå®ä¸–ç•Œçš„æ•°æ®é—®é¢˜
- èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œè¯„ä¼°æ–¹æ³•
- èƒ½å¤Ÿä¼˜åŒ–æ¨¡å‹å¹¶è§£é‡Šç»“æœ
- èƒ½å¤Ÿæ’°å†™ä¸“ä¸šçš„é¡¹ç›®æŠ¥å‘Š

---

## 2. å‰ç½®çŸ¥è¯†æ£€æŸ¥

### å¿…å¤‡å‰ç½®æ¦‚å¿µæ¸…å•
- **ç›‘ç£å­¦ä¹ **ï¼šåˆ†ç±»ã€å›å½’ç®—æ³•
- **æ— ç›‘ç£å­¦ä¹ **ï¼šèšç±»ã€é™ç»´
- **æ¨¡å‹è¯„ä¼°**ï¼šè¯„ä¼°æŒ‡æ ‡ã€äº¤å‰éªŒè¯
- **æ•°æ®å¤„ç†**ï¼šæ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹
- **Pythonç¼–ç¨‹**ï¼šNumPyã€Pandasã€scikit-learn

### å›é¡¾é“¾æ¥/è·³è½¬
- å¦‚æœä¸ç†Ÿæ‚‰ç›‘ç£å­¦ä¹ ï¼š`04_æœºå™¨å­¦ä¹ åŸºç¡€/01_ç›‘ç£å­¦ä¹ /`
- å¦‚æœä¸ç†Ÿæ‚‰æ— ç›‘ç£å­¦ä¹ ï¼š`04_æœºå™¨å­¦ä¹ åŸºç¡€/02_æ— ç›‘ç£å­¦ä¹ /`
- å¦‚æœä¸ç†Ÿæ‚‰æ¨¡å‹è¯„ä¼°ï¼š`04_æœºå™¨å­¦ä¹ åŸºç¡€/03_æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–/`
- å¦‚æœä¸ç†Ÿæ‚‰æ•°æ®å¤„ç†ï¼š`03_æ•°æ®å¤„ç†åŸºç¡€/`

### å…¥é—¨å°æµ‹

**é€‰æ‹©é¢˜**ï¼ˆæ¯é¢˜2åˆ†ï¼Œå…±10åˆ†ï¼‰

1. æ•°æ®ç§‘å­¦é¡¹ç›®çš„ç¬¬ä¸€ä¸ªæ­¥éª¤é€šå¸¸æ˜¯ï¼Ÿ
   A. æ¨¡å‹è®­ç»ƒ  B. é—®é¢˜å®šä¹‰  C. æ•°æ®æ”¶é›†  D. æ¨¡å‹è¯„ä¼°
   **ç­”æ¡ˆ**ï¼šB

2. ç‰¹å¾å·¥ç¨‹çš„ä¸»è¦ç›®çš„æ˜¯ï¼Ÿ
   A. å¢åŠ æ•°æ®é‡  B. æé«˜æ¨¡å‹æ€§èƒ½  C. å‡å°‘è®¡ç®—é‡  D. å¯è§†åŒ–æ•°æ®
   **ç­”æ¡ˆ**ï¼šB

3. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜çš„æ–¹æ³•ä¸åŒ…æ‹¬ï¼Ÿ
   A. è¿‡é‡‡æ ·  B. æ¬ é‡‡æ ·  C. ç±»åˆ«æƒé‡  D. åˆ é™¤å°‘æ•°ç±»
   **ç­”æ¡ˆ**ï¼šD

4. æ¨¡å‹éƒ¨ç½²åéœ€è¦ï¼Ÿ
   A. æŒç»­ç›‘æ§  B. å®šæœŸæ›´æ–°  C. æ€§èƒ½è¯„ä¼°  D. ä»¥ä¸Šéƒ½æ˜¯
   **ç­”æ¡ˆ**ï¼šD

5. é¡¹ç›®æŠ¥å‘Šåº”è¯¥åŒ…æ‹¬ï¼Ÿ
   A. é—®é¢˜å®šä¹‰  B. æ–¹æ³•è¯´æ˜  C. ç»“æœåˆ†æ  D. ä»¥ä¸Šéƒ½æ˜¯
   **ç­”æ¡ˆ**ï¼šD

**è¯„åˆ†æ ‡å‡†**ï¼šâ‰¥8åˆ†ï¼ˆ80%ï¼‰ä¸ºé€šè¿‡

---

## 3. æ ¸å¿ƒçŸ¥è¯†ç‚¹è¯¦è§£

### 3.1 æ•°æ®ç§‘å­¦å·¥ä½œæµç¨‹

#### 1. é—®é¢˜å®šä¹‰
- **ä¸šåŠ¡ç†è§£**ï¼šç†è§£ä¸šåŠ¡éœ€æ±‚å’Œç›®æ ‡
- **é—®é¢˜è½¬åŒ–**ï¼šå°†ä¸šåŠ¡é—®é¢˜è½¬åŒ–ä¸ºæŠ€æœ¯é—®é¢˜
- **æˆåŠŸæŒ‡æ ‡**ï¼šå®šä¹‰æˆåŠŸçš„è¯„ä¼°æ ‡å‡†

#### 2. æ•°æ®æ”¶é›†
- **æ•°æ®æº**ï¼šå†…éƒ¨æ•°æ®ã€å¤–éƒ¨æ•°æ®ã€å…¬å¼€æ•°æ®é›†
- **æ•°æ®è´¨é‡**ï¼šå®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€ä¸€è‡´æ€§
- **æ•°æ®é‡**ï¼šæ ·æœ¬æ•°é‡ã€ç‰¹å¾æ•°é‡

#### 3. æ•°æ®æ¢ç´¢ï¼ˆEDAï¼‰
- **æè¿°æ€§ç»Ÿè®¡**ï¼šå‡å€¼ã€æ–¹å·®ã€åˆ†å¸ƒ
- **å¯è§†åŒ–**ï¼šç›´æ–¹å›¾ã€æ•£ç‚¹å›¾ã€ç›¸å…³æ€§çŸ©é˜µ
- **æ•°æ®è´¨é‡æ£€æŸ¥**ï¼šç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€é‡å¤å€¼

#### 4. æ•°æ®é¢„å¤„ç†
- **ç¼ºå¤±å€¼å¤„ç†**ï¼šåˆ é™¤ã€å¡«å……ã€æ’å€¼
- **å¼‚å¸¸å€¼å¤„ç†**ï¼šè¯†åˆ«ã€å¤„ç†ã€åˆ†æ
- **æ•°æ®è½¬æ¢**ï¼šæ ‡å‡†åŒ–ã€å½’ä¸€åŒ–ã€ç¼–ç 

#### 5. ç‰¹å¾å·¥ç¨‹
- **ç‰¹å¾åˆ›å»º**ï¼šç»„åˆç‰¹å¾ã€å¤šé¡¹å¼ç‰¹å¾
- **ç‰¹å¾é€‰æ‹©**ï¼šç›¸å…³æ€§åˆ†æã€é‡è¦æ€§åˆ†æ
- **ç‰¹å¾å˜æ¢**ï¼šå¯¹æ•°å˜æ¢ã€Box-Coxå˜æ¢

#### 6. æ¨¡å‹è®­ç»ƒ
- **æ¨¡å‹é€‰æ‹©**ï¼šæ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ç®—æ³•
- **è¶…å‚æ•°è°ƒä¼˜**ï¼šç½‘æ ¼æœç´¢ã€éšæœºæœç´¢
- **äº¤å‰éªŒè¯**ï¼šKæŠ˜äº¤å‰éªŒè¯

#### 7. æ¨¡å‹è¯„ä¼°
- **è¯„ä¼°æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ã€AUC
- **éªŒè¯é›†è¯„ä¼°**ï¼šç‹¬ç«‹éªŒè¯é›†æµ‹è¯•
- **æ¨¡å‹è§£é‡Š**ï¼šç‰¹å¾é‡è¦æ€§ã€SHAPå€¼

#### 8. æ¨¡å‹éƒ¨ç½²
- **æ¨¡å‹ä¿å­˜**ï¼šåºåˆ—åŒ–æ¨¡å‹
- **APIå¼€å‘**ï¼šRESTful API
- **ç›‘æ§ç»´æŠ¤**ï¼šæ€§èƒ½ç›‘æ§ã€æ¨¡å‹æ›´æ–°

---

### 3.2 å¸¸è§é—®é¢˜å¤„ç†

#### ç¼ºå¤±å€¼å¤„ç†

**æ–¹æ³•**ï¼š
1. **åˆ é™¤**ï¼šåˆ é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„è¡Œ/åˆ—
2. **å¡«å……**ï¼šå‡å€¼ã€ä¸­ä½æ•°ã€ä¼—æ•°å¡«å……
3. **æ’å€¼**ï¼šçº¿æ€§æ’å€¼ã€æ ·æ¡æ’å€¼
4. **é¢„æµ‹**ï¼šä½¿ç”¨æ¨¡å‹é¢„æµ‹ç¼ºå¤±å€¼

#### å¼‚å¸¸å€¼å¤„ç†

**è¯†åˆ«æ–¹æ³•**ï¼š
- Z-scoreæ–¹æ³•
- IQRæ–¹æ³•
- Isolation Forest
- DBSCAN

**å¤„ç†æ–¹æ³•**ï¼š
- åˆ é™¤
- æ›¿æ¢ï¼ˆæˆªæ–­ã€Winsorizationï¼‰
- ä¿ç•™ï¼ˆå¦‚æœæ˜¯çœŸå®å¼‚å¸¸ï¼‰

#### ç±»åˆ«ä¸å¹³è¡¡

**æ–¹æ³•**ï¼š
1. **è¿‡é‡‡æ ·**ï¼šSMOTEã€ADASYN
2. **æ¬ é‡‡æ ·**ï¼šéšæœºæ¬ é‡‡æ ·ã€Tomek Links
3. **ç±»åˆ«æƒé‡**ï¼šè°ƒæ•´ç±»åˆ«æƒé‡
4. **é›†æˆæ–¹æ³•**ï¼šä½¿ç”¨ä¸å¹³è¡¡å­¦ä¹ çš„é›†æˆæ–¹æ³•

#### ç‰¹å¾é€‰æ‹©

**æ–¹æ³•**ï¼š
1. **è¿‡æ»¤æ³•**ï¼šç›¸å…³æ€§ã€å¡æ–¹æ£€éªŒã€äº’ä¿¡æ¯
2. **åŒ…è£…æ³•**ï¼šé€’å½’ç‰¹å¾æ¶ˆé™¤ã€å‰å‘é€‰æ‹©
3. **åµŒå…¥æ³•**ï¼šL1æ­£åˆ™åŒ–ã€æ ‘æ¨¡å‹é‡è¦æ€§

---

### 3.3 é¡¹ç›®ç»“æ„

```
é¡¹ç›®åç§°/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/     # å¤„ç†åçš„æ•°æ®
â”‚   â””â”€â”€ external/      # å¤–éƒ¨æ•°æ®
â”œâ”€â”€ notebooks/         # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_modeling.ipynb
â”‚   â””â”€â”€ 04_evaluation.ipynb
â”œâ”€â”€ src/               # æºä»£ç 
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ visualization/
â”œâ”€â”€ models/            # ä¿å­˜çš„æ¨¡å‹
â”œâ”€â”€ reports/           # æŠ¥å‘Š
â”œâ”€â”€ requirements.txt   # ä¾èµ–
â””â”€â”€ README.md          # é¡¹ç›®è¯´æ˜
```

---

## 4. Pythonä»£ç å®è·µ

### 4.1 å®Œæ•´é¡¹ç›®æ¨¡æ¿

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1. æ•°æ®åŠ è½½
def load_data(file_path):
    """åŠ è½½æ•°æ®"""
    data = pd.read_csv(file_path)
    return data

# 2. æ•°æ®æ¢ç´¢
def explore_data(data):
    """æ•°æ®æ¢ç´¢æ€§åˆ†æ"""
    print("æ•°æ®å½¢çŠ¶:", data.shape)
    print("\næ•°æ®ä¿¡æ¯:")
    print(data.info())
    print("\næè¿°æ€§ç»Ÿè®¡:")
    print(data.describe())
    print("\nç¼ºå¤±å€¼:")
    print(data.isnull().sum())
    
    # å¯è§†åŒ–
    plt.figure(figsize=(12, 8))
    data.hist(bins=50, figsize=(12, 8))
    plt.tight_layout()
    plt.show()

# 3. æ•°æ®é¢„å¤„ç†
def preprocess_data(data):
    """æ•°æ®é¢„å¤„ç†"""
    # å¤„ç†ç¼ºå¤±å€¼
    data = data.fillna(data.mean())
    
    # å¤„ç†å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    data = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]
    
    return data

# 4. ç‰¹å¾å·¥ç¨‹
def feature_engineering(data):
    """ç‰¹å¾å·¥ç¨‹"""
    # åˆ›å»ºæ–°ç‰¹å¾
    # data['new_feature'] = data['feature1'] * data['feature2']
    
    # ç‰¹å¾é€‰æ‹©
    # è¿™é‡Œå¯ä»¥æ ¹æ®ç›¸å…³æ€§æˆ–å…¶ä»–æ–¹æ³•é€‰æ‹©ç‰¹å¾
    
    return data

# 5. æ¨¡å‹è®­ç»ƒ
def train_model(X_train, y_train):
    """è®­ç»ƒæ¨¡å‹"""
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    # è®­ç»ƒæ¨¡å‹
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train_scaled, y_train)
    
    return model, scaler

# 6. æ¨¡å‹è¯„ä¼°
def evaluate_model(model, scaler, X_test, y_test):
    """è¯„ä¼°æ¨¡å‹"""
    X_test_scaled = scaler.transform(X_test)
    y_pred = model.predict(X_test_scaled)
    
    print("åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred))
    
    print("\næ··æ·†çŸ©é˜µ:")
    cm = confusion_matrix(y_test, y_pred)
    print(cm)
    
    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.show()

# ä¸»æµç¨‹
if __name__ == "__main__":
    # 1. åŠ è½½æ•°æ®
    data = load_data('data.csv')
    
    # 2. æ•°æ®æ¢ç´¢
    explore_data(data)
    
    # 3. æ•°æ®é¢„å¤„ç†
    data = preprocess_data(data)
    
    # 4. ç‰¹å¾å·¥ç¨‹
    data = feature_engineering(data)
    
    # 5. å‡†å¤‡æ•°æ®
    X = data.drop('target', axis=1)
    y = data['target']
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # 6. è®­ç»ƒæ¨¡å‹
    model, scaler = train_model(X_train, y_train)
    
    # 7. è¯„ä¼°æ¨¡å‹
    evaluate_model(model, scaler, X_test, y_test)
```

---

## 5. åŠ¨æ‰‹ç»ƒä¹ ï¼ˆåˆ†å±‚æ¬¡ï¼‰

### åŸºç¡€ç»ƒä¹ ï¼ˆ3-5é¢˜ï¼‰âš ï¸ã€å¿…é¡»è‡³å°‘3é¢˜ï¼Œéš¾åº¦é€’å¢ã€‘

#### ç»ƒä¹ 1ï¼šå®Œæˆä¸€ä¸ªç®€å•çš„åˆ†ç±»é¡¹ç›®
**ç›®æ ‡**ï¼šä½¿ç”¨Irisæ•°æ®é›†å®Œæˆä¸€ä¸ªå®Œæ•´çš„åˆ†ç±»é¡¹ç›®

**è¦æ±‚**ï¼š
1. æ•°æ®åŠ è½½å’Œæ¢ç´¢
2. æ•°æ®é¢„å¤„ç†
3. æ¨¡å‹è®­ç»ƒ
4. æ¨¡å‹è¯„ä¼°
5. ç»“æœå¯è§†åŒ–

**éš¾åº¦**ï¼šâ­â­

---

#### ç»ƒä¹ 2ï¼šå¤„ç†çœŸå®æ•°æ®é›†
**ç›®æ ‡**ï¼šä½¿ç”¨çœŸå®æ•°æ®é›†ï¼ˆå¦‚Titanicã€House Pricesï¼‰å®Œæˆé¡¹ç›®

**è¦æ±‚**ï¼š
1. å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
2. ç‰¹å¾å·¥ç¨‹
3. æ¨¡å‹è®­ç»ƒå’Œè°ƒä¼˜
4. æ¨¡å‹è¯„ä¼°
5. æ’°å†™ç®€è¦æŠ¥å‘Š

**éš¾åº¦**ï¼šâ­â­â­

---

#### ç»ƒä¹ 3ï¼šå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
**ç›®æ ‡**ï¼šåœ¨ç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®é›†ä¸Šå®Œæˆåˆ†ç±»é¡¹ç›®

**è¦æ±‚**ï¼š
1. è¯†åˆ«ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
2. ä½¿ç”¨è¿‡é‡‡æ ·/æ¬ é‡‡æ ·æ–¹æ³•
3. è°ƒæ•´ç±»åˆ«æƒé‡
4. æ¯”è¾ƒä¸åŒæ–¹æ³•çš„æ•ˆæœ
5. é€‰æ‹©æœ€ä¼˜æ–¹æ³•

**éš¾åº¦**ï¼šâ­â­â­

---

### è¿›é˜¶ç»ƒä¹ ï¼ˆ2-3é¢˜ï¼‰âš ï¸ã€å¿…é¡»è‡³å°‘2é¢˜ï¼Œéš¾åº¦é€’å¢ã€‘

#### ç»ƒä¹ 1ï¼šç«¯åˆ°ç«¯å›å½’é¡¹ç›®
**ç›®æ ‡**ï¼šå®Œæˆä¸€ä¸ªå®Œæ•´çš„å›å½’é¡¹ç›®ï¼ˆå¦‚æˆ¿ä»·é¢„æµ‹ï¼‰

**è¦æ±‚**ï¼š
1. æ•°æ®æ”¶é›†å’Œæ¢ç´¢
2. ç‰¹å¾å·¥ç¨‹ï¼ˆåˆ›å»ºæ–°ç‰¹å¾ï¼‰
3. å¤šä¸ªæ¨¡å‹å¯¹æ¯”
4. è¶…å‚æ•°è°ƒä¼˜
5. æ¨¡å‹é›†æˆ
6. ç»“æœè§£é‡Šå’ŒæŠ¥å‘Š

**éš¾åº¦**ï¼šâ­â­â­â­

---

#### ç»ƒä¹ 2ï¼šå¤šåˆ†ç±»é¡¹ç›®
**ç›®æ ‡**ï¼šå®Œæˆä¸€ä¸ªå¤šåˆ†ç±»é¡¹ç›®ï¼ˆå¦‚æ‰‹å†™æ•°å­—è¯†åˆ«ï¼‰

**è¦æ±‚**ï¼š
1. æ•°æ®é¢„å¤„ç†å’Œå¢å¼º
2. ç‰¹å¾å·¥ç¨‹
3. å¤šä¸ªæ¨¡å‹å¯¹æ¯”
4. è¶…å‚æ•°è°ƒä¼˜
5. æ¨¡å‹é›†æˆ
6. é”™è¯¯åˆ†æ

**éš¾åº¦**ï¼šâ­â­â­â­

---

### æŒ‘æˆ˜ç»ƒä¹ ï¼ˆ1-2é¢˜ï¼‰âš ï¸ã€å¿…é¡»è‡³å°‘1é¢˜ã€‘

#### ç»ƒä¹ 1ï¼šå®Œæ•´çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿ
**ç›®æ ‡**ï¼šæ„å»ºä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼ŒåŒ…æ‹¬æ•°æ®ç®¡é“ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€éƒ¨ç½²

**è¦æ±‚**ï¼š
1. å®Œæ•´çš„æ•°æ®ç§‘å­¦å·¥ä½œæµç¨‹
2. è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹
3. æ¨¡å‹é€‰æ‹©å’Œè°ƒä¼˜
4. æ¨¡å‹è§£é‡Šï¼ˆSHAPå€¼ï¼‰
5. APIå¼€å‘ï¼ˆFlask/FastAPIï¼‰
6. æ¨¡å‹ç›‘æ§
7. å®Œæ•´çš„é¡¹ç›®æ–‡æ¡£

**éš¾åº¦**ï¼šâ­â­â­â­â­

---

## 6. å®é™…æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæˆ¿ä»·é¢„æµ‹ç³»ç»Ÿï¼ˆç®€å•é¡¹ç›®ï¼‰

**ä¸šåŠ¡èƒŒæ™¯**ï¼šé¢„æµ‹æˆ¿å±‹ä»·æ ¼ï¼Œå¸®åŠ©ä¹°å®¶å’Œå–å®¶åšå‡ºå†³ç­–ã€‚

**é—®é¢˜æŠ½è±¡**ï¼š
- è¾“å…¥ï¼šæˆ¿å±‹ç‰¹å¾ï¼ˆé¢ç§¯ã€ä½ç½®ã€æˆ¿é¾„ç­‰ï¼‰
- è¾“å‡ºï¼šæˆ¿å±‹ä»·æ ¼
- ç±»å‹ï¼šå›å½’é—®é¢˜

**ç«¯åˆ°ç«¯å®ç°**ï¼š
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# 1. æ•°æ®åŠ è½½
data = pd.read_csv('house_prices.csv')

# 2. æ•°æ®æ¢ç´¢
print(data.info())
print(data.describe())

# 3. æ•°æ®é¢„å¤„ç†
# å¤„ç†ç¼ºå¤±å€¼
data = data.fillna(data.mean())

# ç‰¹å¾é€‰æ‹©
features = ['area', 'bedrooms', 'bathrooms', 'age', 'location_score']
X = data[features]
y = data['price']

# 4. åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 5. æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 6. è®­ç»ƒæ¨¡å‹
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# 7. é¢„æµ‹å’Œè¯„ä¼°
y_pred = model.predict(X_test_scaled)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.2f}")
print(f"RÂ²: {r2:.4f}")

# 8. å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('çœŸå®ä»·æ ¼')
plt.ylabel('é¢„æµ‹ä»·æ ¼')
plt.title('æˆ¿ä»·é¢„æµ‹ç»“æœ')
plt.show()
```

**ç»“æœè§£è¯»**ï¼š
- æ¨¡å‹èƒ½å¤Ÿè¾ƒå¥½åœ°é¢„æµ‹æˆ¿ä»·
- RMSEå’ŒRÂ²åˆ†æ•°æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½
- å¯ä»¥é€šè¿‡ç‰¹å¾é‡è¦æ€§åˆ†æå“ªäº›å› ç´ æœ€é‡è¦

**æ”¹è¿›æ–¹å‘**ï¼š
1. æ›´å¤šç‰¹å¾å·¥ç¨‹
2. å°è¯•ä¸åŒæ¨¡å‹
3. è¶…å‚æ•°è°ƒä¼˜
4. æ¨¡å‹é›†æˆ

---

### æ¡ˆä¾‹2ï¼šå®¢æˆ·æµå¤±é¢„æµ‹ï¼ˆä¸­ç­‰é¡¹ç›®ï¼‰

**ä¸šåŠ¡èƒŒæ™¯**ï¼šé¢„æµ‹å“ªäº›å®¢æˆ·å¯èƒ½ä¼šæµå¤±ï¼Œä»¥ä¾¿é‡‡å–æŒ½ç•™æªæ–½ã€‚

**é—®é¢˜æŠ½è±¡**ï¼š
- è¾“å…¥ï¼šå®¢æˆ·ç‰¹å¾ï¼ˆä½¿ç”¨æ—¶é•¿ã€æ¶ˆè´¹é‡‘é¢ã€æŠ•è¯‰æ¬¡æ•°ç­‰ï¼‰
- è¾“å‡ºï¼šæ˜¯å¦æµå¤±ï¼ˆäºŒåˆ†ç±»ï¼‰
- ç±»å‹ï¼šåˆ†ç±»é—®é¢˜ï¼ˆå¯èƒ½ç±»åˆ«ä¸å¹³è¡¡ï¼‰

**ç«¯åˆ°ç«¯å®ç°**ï¼š
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

# 1. æ•°æ®åŠ è½½
data = pd.read_csv('customer_churn.csv')

# 2. æ•°æ®æ¢ç´¢
print("ç±»åˆ«åˆ†å¸ƒ:")
print(data['churn'].value_counts())
print(f"\næµå¤±ç‡: {data['churn'].mean():.2%}")

# 3. æ•°æ®é¢„å¤„ç†
features = ['tenure', 'monthly_charges', 'total_charges', 'contract', 'payment_method']
X = pd.get_dummies(data[features], drop_first=True)
y = data['churn']

# 4. åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 5. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆä½¿ç”¨SMOTEï¼‰
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(f"è¿‡é‡‡æ ·åè®­ç»ƒé›†å¤§å°: {X_train_resampled.shape[0]}")

# 6. æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)

# 7. è®­ç»ƒæ¨¡å‹
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train_resampled)

# 8. é¢„æµ‹å’Œè¯„ä¼°
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)[:, 1]

print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))

print(f"\nROC-AUC: {roc_auc_score(y_test, y_proba):.4f}")

# 9. ROCæ›²çº¿
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROCæ›²çº¿ (AUC = {roc_auc_score(y_test, y_proba):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('å‡æ­£ä¾‹ç‡')
plt.ylabel('çœŸæ­£ä¾‹ç‡')
plt.title('ROCæ›²çº¿')
plt.legend()
plt.grid(True)
plt.show()

# 10. ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nç‰¹å¾é‡è¦æ€§:")
print(feature_importance)
```

**ç»“æœè§£è¯»**ï¼š
- æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¯èƒ½æµå¤±çš„å®¢æˆ·
- ROC-AUCåˆ†æ•°æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½
- ç‰¹å¾é‡è¦æ€§æ˜¾ç¤ºå“ªäº›å› ç´ æœ€å½±å“æµå¤±

**æ”¹è¿›æ–¹å‘**ï¼š
1. æ›´å¤šç‰¹å¾å·¥ç¨‹
2. å°è¯•ä¸åŒé‡‡æ ·æ–¹æ³•
3. è¶…å‚æ•°è°ƒä¼˜
4. æ¨¡å‹é›†æˆ

---

### æ¡ˆä¾‹3ï¼šæ¨èç³»ç»ŸåŸºç¡€ï¼ˆè¿›é˜¶é¡¹ç›®ï¼‰

**ä¸šåŠ¡èƒŒæ™¯**ï¼šæ„å»ºä¸€ä¸ªåŸºç¡€çš„æ¨èç³»ç»Ÿï¼Œä¸ºç”¨æˆ·æ¨èå•†å“ã€‚

**é—®é¢˜æŠ½è±¡**ï¼š
- è¾“å…¥ï¼šç”¨æˆ·å†å²è¡Œä¸ºã€å•†å“ç‰¹å¾
- è¾“å‡ºï¼šæ¨èå•†å“åˆ—è¡¨
- ç±»å‹ï¼šæ¨èç³»ç»Ÿï¼ˆååŒè¿‡æ»¤ã€å†…å®¹è¿‡æ»¤ï¼‰

**ç«¯åˆ°ç«¯å®ç°**ï¼š
```python
import pandas as pd
import numpy as np
from sklearn.decomposition import NMF
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
warnings.filterwarnings('ignore')

# 1. æ•°æ®åŠ è½½
ratings = pd.read_csv('ratings.csv')
items = pd.read_csv('items.csv')

# 2. ååŒè¿‡æ»¤ï¼ˆåŸºäºçŸ©é˜µåˆ†è§£ï¼‰
# åˆ›å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ
user_item_matrix = ratings.pivot_table(
    index='user_id', 
    columns='item_id', 
    values='rating'
).fillna(0)

# ä½¿ç”¨NMFè¿›è¡ŒçŸ©é˜µåˆ†è§£
nmf = NMF(n_components=50, random_state=42)
W = nmf.fit_transform(user_item_matrix)
H = nmf.components_

# é‡å»ºçŸ©é˜µ
user_item_predicted = np.dot(W, H)

# 3. ä¸ºç”¨æˆ·æ¨è
def recommend_items(user_id, n_recommendations=10):
    """ä¸ºç”¨æˆ·æ¨èå•†å“"""
    user_idx = user_item_matrix.index.get_loc(user_id)
    user_ratings = user_item_matrix.iloc[user_idx].values
    user_predictions = user_item_predicted[user_idx]
    
    # æ‰¾åˆ°æœªè¯„åˆ†çš„å•†å“
    unrated_items = np.where(user_ratings == 0)[0]
    
    # é¢„æµ‹è¯„åˆ†
    predicted_ratings = user_predictions[unrated_items]
    
    # æ’åºå¹¶è¿”å›top N
    top_items_idx = unrated_items[np.argsort(predicted_ratings)[::-1][:n_recommendations]]
    top_items = user_item_matrix.columns[top_items_idx]
    
    return top_items.tolist()

# 4. å†…å®¹è¿‡æ»¤ï¼ˆåŸºäºå•†å“ç‰¹å¾ï¼‰
# ä½¿ç”¨TF-IDFå‘é‡åŒ–å•†å“æè¿°
vectorizer = TfidfVectorizer(max_features=100)
item_features = vectorizer.fit_transform(items['description'])

# è®¡ç®—å•†å“ç›¸ä¼¼åº¦
item_similarity = cosine_similarity(item_features)

def recommend_similar_items(item_id, n_recommendations=10):
    """æ¨èç›¸ä¼¼å•†å“"""
    item_idx = items[items['item_id'] == item_id].index[0]
    similar_items_idx = np.argsort(item_similarity[item_idx])[::-1][1:n_recommendations+1]
    similar_items = items.iloc[similar_items_idx]['item_id'].tolist()
    return similar_items

# 5. æ··åˆæ¨è
def hybrid_recommend(user_id, item_id, n_recommendations=10):
    """æ··åˆæ¨èï¼ˆååŒè¿‡æ»¤ + å†…å®¹è¿‡æ»¤ï¼‰"""
    # ååŒè¿‡æ»¤æ¨è
    cf_items = recommend_items(user_id, n_recommendations)
    
    # å†…å®¹è¿‡æ»¤æ¨è
    cf_content_items = recommend_similar_items(item_id, n_recommendations)
    
    # åˆå¹¶å’Œå»é‡
    hybrid_items = list(set(cf_items + cf_content_items))[:n_recommendations]
    
    return hybrid_items

# ä½¿ç”¨ç¤ºä¾‹
user_id = 1
recommended = recommend_items(user_id, n_recommendations=10)
print(f"ä¸ºç”¨æˆ· {user_id} æ¨èçš„å•†å“: {recommended}")
```

**ç»“æœè§£è¯»**ï¼š
- ååŒè¿‡æ»¤åŸºäºç”¨æˆ·è¡Œä¸ºæ¨è
- å†…å®¹è¿‡æ»¤åŸºäºå•†å“ç‰¹å¾æ¨è
- æ··åˆæ¨èç»“åˆä¸¤ç§æ–¹æ³•

**æ”¹è¿›æ–¹å‘**ï¼š
1. ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹
2. æ·»åŠ æ—¶é—´å› ç´ 
3. å¤„ç†å†·å¯åŠ¨é—®é¢˜
4. å®æ—¶æ¨è

---

## 7. è‡ªæˆ‘è¯„ä¼°

### æ¦‚å¿µé¢˜

#### é€‰æ‹©é¢˜ï¼ˆ10-15é“ï¼‰

1. æ•°æ®ç§‘å­¦é¡¹ç›®çš„ç¬¬ä¸€ä¸ªæ­¥éª¤é€šå¸¸æ˜¯ï¼Ÿ
   A. æ¨¡å‹è®­ç»ƒ  B. é—®é¢˜å®šä¹‰  C. æ•°æ®æ”¶é›†  D. æ¨¡å‹è¯„ä¼°
   **ç­”æ¡ˆ**ï¼šB

2. ç‰¹å¾å·¥ç¨‹çš„ä¸»è¦ç›®çš„æ˜¯ï¼Ÿ
   A. å¢åŠ æ•°æ®é‡  B. æé«˜æ¨¡å‹æ€§èƒ½  C. å‡å°‘è®¡ç®—é‡  D. å¯è§†åŒ–æ•°æ®
   **ç­”æ¡ˆ**ï¼šB

3. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜çš„æ–¹æ³•ä¸åŒ…æ‹¬ï¼Ÿ
   A. è¿‡é‡‡æ ·  B. æ¬ é‡‡æ ·  C. ç±»åˆ«æƒé‡  D. åˆ é™¤å°‘æ•°ç±»
   **ç­”æ¡ˆ**ï¼šD

4. æ¨¡å‹éƒ¨ç½²åéœ€è¦ï¼Ÿ
   A. æŒç»­ç›‘æ§  B. å®šæœŸæ›´æ–°  C. æ€§èƒ½è¯„ä¼°  D. ä»¥ä¸Šéƒ½æ˜¯
   **ç­”æ¡ˆ**ï¼šD

5. é¡¹ç›®æŠ¥å‘Šåº”è¯¥åŒ…æ‹¬ï¼Ÿ
   A. é—®é¢˜å®šä¹‰  B. æ–¹æ³•è¯´æ˜  C. ç»“æœåˆ†æ  D. ä»¥ä¸Šéƒ½æ˜¯
   **ç­”æ¡ˆ**ï¼šD

#### ç®€ç­”é¢˜ï¼ˆ5-8é“ï¼‰

1. è¯´æ˜æ•°æ®ç§‘å­¦é¡¹ç›®çš„å®Œæ•´å·¥ä½œæµç¨‹ã€‚
   **å‚è€ƒç­”æ¡ˆ**ï¼šé—®é¢˜å®šä¹‰ â†’ æ•°æ®æ”¶é›† â†’ æ•°æ®æ¢ç´¢ â†’ æ•°æ®é¢„å¤„ç† â†’ ç‰¹å¾å·¥ç¨‹ â†’ æ¨¡å‹è®­ç»ƒ â†’ æ¨¡å‹è¯„ä¼° â†’ æ¨¡å‹éƒ¨ç½² â†’ ç›‘æ§ç»´æŠ¤

2. è§£é‡Šå¦‚ä½•å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚
   **å‚è€ƒç­”æ¡ˆ**ï¼šå¯ä»¥ä½¿ç”¨è¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰ã€æ¬ é‡‡æ ·ã€è°ƒæ•´ç±»åˆ«æƒé‡ã€ä½¿ç”¨ä¸å¹³è¡¡å­¦ä¹ çš„ç®—æ³•ç­‰æ–¹æ³•ã€‚

3. è¯´æ˜ç‰¹å¾å·¥ç¨‹çš„é‡è¦æ€§ã€‚
   **å‚è€ƒç­”æ¡ˆ**ï¼šç‰¹å¾å·¥ç¨‹å¯ä»¥åˆ›å»ºæ›´æœ‰æ„ä¹‰çš„ç‰¹å¾ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ï¼Œå‡å°‘è¿‡æ‹Ÿåˆï¼Œæ”¹å–„æ¨¡å‹è§£é‡Šæ€§ã€‚

---

### ç¼–ç¨‹å®è·µé¢˜ï¼ˆ2-3é“ï¼‰

#### é¢˜ç›®1ï¼šå®Œæˆä¸€ä¸ªå®Œæ•´çš„åˆ†ç±»é¡¹ç›®
**è¦æ±‚**ï¼š
1. æ•°æ®åŠ è½½å’Œæ¢ç´¢
2. æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
3. æ¨¡å‹è®­ç»ƒå’Œè°ƒä¼˜
4. æ¨¡å‹è¯„ä¼°
5. ç»“æœå¯è§†åŒ–
6. æ’°å†™é¡¹ç›®æŠ¥å‘Š

**è¯„åˆ†æ ‡å‡†**ï¼š
- å·¥ä½œæµç¨‹å®Œæ•´ï¼ˆ30åˆ†ï¼‰
- ä»£ç è´¨é‡ï¼ˆ20åˆ†ï¼‰
- æ¨¡å‹æ€§èƒ½ï¼ˆ20åˆ†ï¼‰
- æŠ¥å‘Šè´¨é‡ï¼ˆ20åˆ†ï¼‰
- åˆ›æ–°æ€§ï¼ˆ10åˆ†ï¼‰

---

### ç»¼åˆåº”ç”¨é¢˜ï¼ˆ1-2é“ï¼‰

#### é¢˜ç›®1ï¼šæ„å»ºå®Œæ•´çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿ
**è¦æ±‚**ï¼š
1. å®Œæ•´çš„æ•°æ®ç§‘å­¦å·¥ä½œæµç¨‹
2. è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹
3. æ¨¡å‹é€‰æ‹©å’Œè°ƒä¼˜
4. æ¨¡å‹è§£é‡Š
5. APIå¼€å‘
6. æ¨¡å‹ç›‘æ§
7. å®Œæ•´çš„é¡¹ç›®æ–‡æ¡£

**è¯„åˆ†æ ‡å‡†**ï¼š
- ç³»ç»Ÿå®Œæ•´æ€§ï¼ˆ30åˆ†ï¼‰
- ä»£ç è´¨é‡ï¼ˆ20åˆ†ï¼‰
- æ¨¡å‹æ€§èƒ½ï¼ˆ20åˆ†ï¼‰
- æ–‡æ¡£è´¨é‡ï¼ˆ20åˆ†ï¼‰
- åˆ›æ–°æ€§ï¼ˆ10åˆ†ï¼‰

---

## 8. æ‹“å±•å­¦ä¹ 

### è®ºæ–‡æ¨è

1. **Provost, F., & Fawcett, T. (2013). "Data Science for Business."** O'Reilly Media
   - æ•°æ®ç§‘å­¦å®è·µæŒ‡å—

2. **Kuhn, M., & Johnson, K. (2013). "Applied Predictive Modeling."** Springer
   - é¢„æµ‹å»ºæ¨¡å®è·µ

### ä¹¦ç±æ¨è

1. **ã€Šæ•°æ®ç§‘å­¦æ‰‹å†Œã€‹- Jake VanderPlas**
   - æ•°æ®ç§‘å­¦å®è·µæŒ‡å—

2. **ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹- Peter Harrington**
   - æœºå™¨å­¦ä¹ é¡¹ç›®å®è·µ

### ç›¸å…³å·¥å…·ä¸åº“

1. **scikit-learn**
   - æœºå™¨å­¦ä¹ å·¥å…·
   - æ–‡æ¡£ï¼šhttps://scikit-learn.org/

2. **pandas**
   - æ•°æ®å¤„ç†
   - æ–‡æ¡£ï¼šhttps://pandas.pydata.org/

3. **Flask/FastAPI**
   - APIå¼€å‘
   - æ–‡æ¡£ï¼šhttps://flask.palletsprojects.com/

### è¿›é˜¶è¯é¢˜æŒ‡å¼•

1. **MLOps**
   - æ¨¡å‹éƒ¨ç½²
   - æ¨¡å‹ç›‘æ§
   - æŒç»­é›†æˆ

2. **æ¨¡å‹è§£é‡Šæ€§**
   - SHAPå€¼
   - LIME
   - ç‰¹å¾é‡è¦æ€§

3. **è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ **
   - AutoML
   - è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹
   - è‡ªåŠ¨æ¨¡å‹é€‰æ‹©

### ä¸‹èŠ‚è¯¾é¢„å‘Šä¸å­¦ä¹ å»ºè®®

**ä¸‹èŠ‚è¯¾**ï¼š`05_æ·±åº¦å­¦ä¹ åŸºç¡€`

**å­¦ä¹ å»ºè®®**ï¼š
1. å®Œæˆæ‰€æœ‰å®æˆ˜é¡¹ç›®
2. æ€»ç»“é¡¹ç›®ç»éªŒ
3. å»ºç«‹é¡¹ç›®ä½œå“é›†
4. å‡†å¤‡å­¦ä¹ æ·±åº¦å­¦ä¹ 

**å‰ç½®å‡†å¤‡**ï¼š
- å¤ä¹ çº¿æ€§ä»£æ•°å’Œå¾®ç§¯åˆ†
- äº†è§£ç¥ç»ç½‘ç»œåŸºç¡€
- å‡†å¤‡GPUç¯å¢ƒï¼ˆå¯é€‰ï¼‰

---

**å®Œæˆæœ¬è¯¾ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š**
- âœ… ç‹¬ç«‹å®Œæˆç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ é¡¹ç›®
- âœ… å¤„ç†çœŸå®ä¸–ç•Œçš„æ•°æ®é—®é¢˜
- âœ… é€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œè¯„ä¼°æ–¹æ³•
- âœ… ä¼˜åŒ–æ¨¡å‹å¹¶è§£é‡Šç»“æœ
- âœ… æ’°å†™ä¸“ä¸šçš„é¡¹ç›®æŠ¥å‘Š

**ç»§ç»­å­¦ä¹ ï¼Œæˆä¸ºAIå¤§å¸ˆï¼** ğŸš€
