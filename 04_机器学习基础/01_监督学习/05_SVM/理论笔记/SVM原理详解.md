# 支持向量机（SVM）原理详解

## 1. SVM的基本思想

### 1.1 什么是支持向量机？

**支持向量机（SVM）**：一种二分类模型，目标是找到一个最优的超平面来分离两类数据。

**核心思想**：不仅要正确分类，还要使分类间隔最大化。

**类比**：就像在两类数据之间画一条"最宽的道路"，这条道路的边界就是最优分类超平面。

**为什么需要最大化间隔？**⚠️【知其所以然】

1. **泛化能力**：间隔越大，对新样本的分类越可靠
2. **鲁棒性**：对数据的小扰动不敏感
3. **唯一性**：最大间隔超平面是唯一的
4. **统计学习理论**：间隔与泛化误差上界相关

### 1.2 支持向量

**支持向量**：距离超平面最近的那些样本点。

**为什么叫"支持向量"？**⚠️【知其所以然】

- 这些向量"支撑"着超平面
- 如果移除支持向量，超平面会改变
- 只有支持向量对超平面有影响，其他样本点不影响

---

## 2. 线性可分情况（硬间隔）

### 2.1 超平面方程

**超平面方程**：
$$w^T x + b = 0$$

其中：
- $w$ 是法向量（权重向量）
- $b$ 是偏置项
- $x$ 是样本点

**分类规则**：
- $w^T x + b \geq 1$：正类（$y = +1$）
- $w^T x + b \leq -1$：负类（$y = -1$）

### 2.2 间隔最大化

**函数间隔**：
$$\hat{\gamma}_i = y_i(w^T x_i + b)$$

**几何间隔**：
$$\gamma_i = \frac{y_i(w^T x_i + b)}{||w||} = \frac{\hat{\gamma}_i}{||w||}$$

**为什么需要归一化？**⚠️【知其所以然】

- 函数间隔依赖于$w$的尺度
- 几何间隔是标准化的，不依赖于尺度
- 几何间隔有明确的几何意义：点到超平面的距离

**最大间隔优化问题**：
$$\max_{w,b} \frac{2}{||w||}$$

等价于：
$$\min_{w,b} \frac{1}{2}||w||^2$$

约束条件：
$$y_i(w^T x_i + b) \geq 1, \quad i = 1, 2, ..., n$$

**为什么最小化$||w||^2$而不是$||w||$？**⚠️【知其所以然】

1. **数学便利**：$||w||^2$是凸函数，便于优化
2. **等价性**：最小化$||w||^2$等价于最小化$||w||$
3. **计算效率**：二次函数更容易求导和优化

---

## 3. 线性不可分情况（软间隔）

### 3.1 为什么需要软间隔？

**问题**：数据可能不是线性可分的，或者存在噪声。

**解决方案**：允许一些样本违反间隔约束，但需要惩罚。

**为什么允许违反约束？**⚠️【知其所以然】

- **现实性**：真实数据很少完全线性可分
- **鲁棒性**：对噪声和异常值更鲁棒
- **灵活性**：在可分性和间隔之间权衡

### 3.2 软间隔SVM

**引入松弛变量**$\xi_i \geq 0$：
$$y_i(w^T x_i + b) \geq 1 - \xi_i$$

**优化问题**：
$$\min_{w,b,\xi} \frac{1}{2}||w||^2 + C\sum_{i=1}^{n}\xi_i$$

约束条件：
$$y_i(w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i = 1, 2, ..., n$$

**参数$C$的作用**：
- **$C$很大**：几乎不允许违反约束（接近硬间隔）
- **$C$很小**：允许更多违反约束（间隔更宽但可能误分类更多）

**为什么$C$控制权衡？**⚠️【知其所以然】

- **第一项**：$\frac{1}{2}||w||^2$ 控制间隔大小（最大化间隔）
- **第二项**：$C\sum\xi_i$ 控制误分类惩罚（最小化误分类）
- **$C$**：平衡这两项的权重

---

## 4. 对偶问题

### 4.1 拉格朗日乘数法

**原始问题**（Primal Problem）：
$$\min_{w,b} \frac{1}{2}||w||^2 + C\sum_{i=1}^{n}\xi_i$$

**拉格朗日函数**：
$$L(w, b, \xi, \alpha, \mu) = \frac{1}{2}||w||^2 + C\sum_{i=1}^{n}\xi_i - \sum_{i=1}^{n}\alpha_i[y_i(w^T x_i + b) - 1 + \xi_i] - \sum_{i=1}^{n}\mu_i\xi_i$$

其中$\alpha_i \geq 0, \mu_i \geq 0$是拉格朗日乘数。

### 4.2 对偶问题

**对偶问题**（Dual Problem）：
$$\max_{\alpha} \sum_{i=1}^{n}\alpha_i - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_j y_i y_j x_i^T x_j$$

约束条件：
$$0 \leq \alpha_i \leq C, \quad \sum_{i=1}^{n}\alpha_i y_i = 0$$

**为什么需要转换为对偶问题？**⚠️【知其所以然】

1. **计算优势**：对偶问题只涉及$\alpha$，维度更低
2. **核技巧**：对偶形式可以自然地引入核函数
3. **支持向量**：$\alpha_i > 0$对应支持向量，更直观
4. **优化效率**：某些情况下对偶问题更容易求解

### 4.3 KKT条件

**KKT条件**（Karush-Kuhn-Tucker）：
1. **原始可行性**：$y_i(w^T x_i + b) - 1 + \xi_i \geq 0$
2. **对偶可行性**：$\alpha_i \geq 0, \mu_i \geq 0$
3. **互补松弛性**：$\alpha_i[y_i(w^T x_i + b) - 1 + \xi_i] = 0$
4. **梯度条件**：$\frac{\partial L}{\partial w} = 0, \frac{\partial L}{\partial b} = 0, \frac{\partial L}{\partial \xi_i} = 0$

**从KKT条件得到**：
$$w = \sum_{i=1}^{n}\alpha_i y_i x_i$$

**为什么$w$是支持向量的线性组合？**⚠️【知其所以然】

- 根据互补松弛性，只有$\alpha_i > 0$的样本（支持向量）对$w$有贡献
- 非支持向量的$\alpha_i = 0$，不影响$w$
- 这解释了为什么SVM只依赖支持向量

---

## 5. 核技巧（Kernel Trick）

### 5.1 为什么需要核技巧？

**问题**：数据可能不是线性可分的，需要非线性分类。

**解决方案**：将数据映射到高维空间，在高维空间中线性可分。

**为什么高维空间更容易线性可分？**⚠️【知其所以然】

- **维度诅咒的逆用**：虽然高维空间稀疏，但分类边界更简单
- **Cover定理**：高维空间中数据更容易线性可分
- **非线性映射**：通过映射将非线性问题转化为线性问题

### 5.2 核函数

**核函数定义**：
$$K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$$

其中$\phi$是映射函数。

**为什么需要核函数？**⚠️【知其所以然】

- **避免显式映射**：不需要计算$\phi(x)$，只需计算$K(x_i, x_j)$
- **计算效率**：核函数计算通常比高维内积更高效
- **维度无关**：核函数计算复杂度与原始维度相关，而不是映射后的维度

### 5.3 常用核函数

**线性核**：
$$K(x_i, x_j) = x_i^T x_j$$

**多项式核**：
$$K(x_i, x_j) = (x_i^T x_j + 1)^d$$

**RBF核（高斯核）**：
$$K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)$$

**为什么RBF核最常用？**⚠️【知其所以然】

- **通用性**：理论上可以逼近任何连续函数
- **局部性**：$\gamma$控制局部影响范围
- **灵活性**：通过调整$\gamma$适应不同数据分布

---

## 6. SVM的优缺点

### 6.1 优点

1. **理论基础扎实**：基于统计学习理论
2. **全局最优**：凸优化问题，有全局最优解
3. **稀疏性**：只依赖支持向量
4. **核技巧**：可以处理非线性问题
5. **泛化能力强**：最大间隔原则

### 6.2 缺点

1. **计算复杂度**：大规模数据计算慢
2. **参数敏感**：$C$和$\gamma$需要仔细调优
3. **内存占用**：需要存储支持向量
4. **概率输出**：不直接提供概率，需要额外处理

---

## 7. 多分类SVM

### 7.1 一对多（One-vs-Rest）

**方法**：为每个类别训练一个二分类SVM。

**问题**：可能产生不平衡的分类区域。

### 7.2 一对一（One-vs-One）

**方法**：为每对类别训练一个二分类SVM。

**优点**：更平衡，但需要$C(C-1)/2$个分类器。

---

## 8. 总结

SVM的核心：
1. **最大间隔**：寻找最优分类超平面
2. **支持向量**：只依赖支持向量
3. **对偶问题**：转换为对偶问题求解
4. **核技巧**：通过核函数处理非线性问题

**继续学习**：
- SVR（支持向量回归）
- 核方法
- 其他核函数

