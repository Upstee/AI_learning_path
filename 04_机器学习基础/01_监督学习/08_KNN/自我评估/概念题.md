# KNN - 概念题

## 一、选择题（每题2分，共20分）

### 1. KNN是什么类型的学习方法？
A. 急切学习  
B. 懒惰学习  
C. 在线学习  
D. 强化学习  

**答案**：B

**解释**：⚠️【详细解释】KNN是"懒惰学习"，训练时不做任何计算，只存储数据，所有计算都在预测时进行。

---

### 2. KNN中K值的含义是？
A. 特征数  
B. 最近的K个邻居  
C. 类别数  
D. 样本数  

**答案**：B

**解释**：⚠️【详细解释】K值表示选择最近的K个训练样本来做预测。

---

### 3. 欧氏距离的公式是？
A. sum(|x_i - y_i|)  
B. sqrt(sum((x_i - y_i)^2))  
C. max(|x_i - y_i|)  
D. sum(x_i * y_i)  

**答案**：B

**解释**：⚠️【详细解释】欧氏距离是两点之间的直线距离，公式是各维度差值的平方和的平方根。

---

### 4. K值太小会导致？
A. 欠拟合  
B. 过拟合  
C. 计算慢  
D. 内存占用大  

**答案**：B

**解释**：⚠️【详细解释】K值太小（如K=1）对局部模式过于敏感，容易过拟合，对噪声敏感。

---

### 5. K值太大会导致？
A. 过拟合  
B. 欠拟合  
C. 计算快  
D. 准确率高  

**答案**：B

**解释**：⚠️【详细解释】K值太大（如K=N）会忽略局部模式，可能导致欠拟合。

---

### 6. KNN对特征尺度？
A. 不敏感  
B. 敏感  
C. 无关  
D. 取决于数据  

**答案**：B

**解释**：⚠️【详细解释】KNN基于距离，如果特征尺度不同，大尺度特征会主导距离计算，因此需要标准化。

---

### 7. 加权KNN中，距离越近的邻居？
A. 权重越小  
B. 权重越大  
C. 权重相同  
D. 不参与投票  

**答案**：B

**解释**：⚠️【详细解释】加权KNN给距离更近的邻居更高的权重，因为距离越近越相似。

---

### 8. KNN的主要缺点不包括？
A. 计算复杂度高  
B. 对维度敏感  
C. 对不平衡数据敏感  
D. 训练速度快  

**答案**：D

**解释**：⚠️【详细解释】KNN训练时不做计算，但预测时需要计算与所有训练样本的距离，计算复杂度高。

---

### 9. KD树的主要作用是？
A. 提高准确率  
B. 优化最近邻搜索  
C. 减少内存占用  
D. 简化算法  

**答案**：B

**解释**：⚠️【详细解释】KD树是一种数据结构，可以快速找到最近邻居，将时间复杂度从O(N)降低到O(log N)。

---

### 10. KNN回归时，预测值是？
A. K个邻居的众数  
B. K个邻居的平均值  
C. K个邻居的中位数  
D. K个邻居的最大值  

**答案**：B

**解释**：⚠️【详细解释】KNN回归时，预测值是K个最近邻居的平均值（或加权平均）。

---

## 二、简答题（每题10分，共40分）

### 1. 解释KNN的"懒惰学习"特点。

**参考答案**：
- **定义**：KNN在训练时不做任何计算，只存储训练数据
- **预测时计算**：所有计算都在预测时进行
- **与急切学习对比**：急切学习（如决策树、SVM）在训练时就建立模型
- **优缺点**：
  - 优点：训练快，适应新数据容易
  - 缺点：预测慢，需要存储所有训练数据

---

### 2. 说明K值选择的影响和方法。

**参考答案**：
- **K值太小**：过拟合，对噪声敏感，但能捕捉局部模式
- **K值太大**：欠拟合，忽略局部模式，但更稳定
- **选择方法**：
  - 交叉验证：测试不同的K值，选择性能最好的
  - 经验法则：K = √N（N是样本数）是一个起点
  - 通常选择奇数，避免平票
- **最优K值**：在偏差和方差之间找到平衡

---

### 3. 解释为什么KNN需要特征标准化。

**参考答案**：
- **距离度量依赖尺度**：KNN基于距离，如果特征尺度不同，大尺度特征会主导距离计算
- **示例**：如果特征1的范围是0-100，特征2的范围是0-1，特征1会主导距离
- **解决方案**：
  - 标准化：(x - μ) / σ
  - 归一化：(x - min) / (max - min)
- **效果**：标准化后，所有特征对距离的贡献相等

---

### 4. 说明KNN的优缺点。

**参考答案**：
- **优点**：
  1. 简单易懂，容易实现
  2. 非参数方法，不假设数据分布
  3. 对局部模式敏感
  4. 适用于多分类
  5. 可解释性强
  
- **缺点**：
  1. 计算复杂度高：O(N×D)
  2. 对维度敏感：高维空间中距离失效
  3. 对不平衡数据敏感
  4. 需要选择合适的距离度量
  5. 需要存储所有训练数据

---

## 三、计算题（每题10分，共20分）

### 1. 给定以下数据，使用KNN（K=3）预测新样本的类别：

训练数据：
- 样本1：(1, 2)，类别A
- 样本2：(2, 3)，类别A
- 样本3：(3, 1)，类别A
- 样本4：(6, 5)，类别B
- 样本5：(7, 6)，类别B

新样本：(4, 3)

使用欧氏距离，均匀权重。

**参考答案**：
- 计算距离：
  - d(新, 样本1) = sqrt((4-1)² + (3-2)²) = sqrt(9+1) = √10 ≈ 3.16
  - d(新, 样本2) = sqrt((4-2)² + (3-3)²) = sqrt(4+0) = 2.00
  - d(新, 样本3) = sqrt((4-3)² + (3-1)²) = sqrt(1+4) = √5 ≈ 2.24
  - d(新, 样本4) = sqrt((4-6)² + (3-5)²) = sqrt(4+4) = √8 ≈ 2.83
  - d(新, 样本5) = sqrt((4-7)² + (3-6)²) = sqrt(9+9) = √18 ≈ 4.24
  
- 最近的3个：样本2（2.00）、样本3（2.24）、样本4（2.83）
- 类别：A、A、B
- 多数投票：类别A（2票）> 类别B（1票）
- 预测：类别A

---

### 2. 给定K=5，计算加权KNN的预测（使用反距离权重）：

距离和类别：
- 距离1.0，类别A
- 距离2.0，类别A
- 距离1.5，类别B
- 距离3.0，类别B
- 距离2.5，类别A

**参考答案**：
- 计算权重（1/距离）：
  - 类别A：1/1.0 + 1/2.0 + 1/2.5 = 1.0 + 0.5 + 0.4 = 1.9
  - 类别B：1/1.5 + 1/3.0 = 0.67 + 0.33 = 1.0
  
- 总权重：1.9 + 1.0 = 2.9
- 类别A概率：1.9 / 2.9 ≈ 0.66
- 类别B概率：1.0 / 2.9 ≈ 0.34
- 预测：类别A（权重更大）

---

## 评分标准

- **选择题**：每题2分，共20分
- **简答题**：每题10分，共40分
- **计算题**：每题10分，共20分
- **总分**：80分
- **及格线**：64分（80%）

---

**完成后请对照答案检查，理解每道题的原理！**

