# 线性回归 - 概念题

## 一、选择题（每题2分，共20分）

### 1. 线性回归的目标是？
A. 分类  
B. 回归  
C. 聚类  
D. 降维  

**答案**：B

---

### 2. 线性回归的假设函数是？
A. $h(x) = wx + b$  
B. $h(x) = w^2x + b$  
C. $h(x) = \sin(wx)$  
D. $h(x) = e^{wx}$  

**答案**：A

---

### 3. 最小二乘法的目标是？
A. 最小化预测值  
B. 最小化误差平方和  
C. 最大化准确率  
D. 最小化参数  

**答案**：B

**解析**：最小二乘法通过最小化误差平方和来找到最佳拟合直线。

---

### 4. 梯度下降的作用是？
A. 求导数  
B. 优化参数  
C. 计算梯度  
D. 绘制图形  

**答案**：B

**解析**：梯度下降是一种优化算法，用于找到使损失函数最小的参数值。

---

### 5. 正则化的作用是？
A. 加速训练  
B. 防止过拟合  
C. 提高准确率  
D. 减少数据  

**答案**：B

**解析**：正则化通过在损失函数中添加惩罚项来限制模型复杂度，从而防止过拟合。

---

### 6. Ridge回归使用什么正则化？
A. L1  
B. L2  
C. L0  
D. 无  

**答案**：B

**解析**：Ridge回归使用L2正则化，即对权重的平方和进行惩罚。

---

### 7. Lasso回归的主要特点是？
A. 所有参数都缩小  
B. 部分参数变为0  
C. 参数不变  
D. 参数增大  

**答案**：B

**解析**：Lasso回归使用L1正则化，可以将部分参数的系数压缩为0，实现特征选择。

---

### 8. R²分数的取值范围是？
A. [0, 1]  
B. (-∞, 1]  
C. [0, ∞)  
D. (-∞, ∞)  

**答案**：B

**解析**：R²分数可以小于0（当模型预测比简单平均值还差时），但最大值为1。

---

### 9. 线性回归对特征缩放的要求是？
A. 必须标准化  
B. 不需要标准化  
C. 只对正则化模型需要  
D. 只对梯度下降需要  

**答案**：C

**解析**：对于使用正则化的模型（Ridge、Lasso），特征缩放很重要；对于普通线性回归，不是必须的，但建议进行。

---

### 10. 线性回归的缺点不包括？
A. 只能处理线性关系  
B. 对异常值敏感  
C. 特征相关时有问题  
D. 计算复杂度高  

**答案**：D

**解析**：线性回归的计算复杂度相对较低，这是它的优点之一。

---

## 二、简答题（每题10分，共40分）

### 1. 解释线性回归的基本原理。

**参考答案**：

线性回归的基本原理是：
1. **假设**：特征和目标变量之间存在线性关系
2. **模型**：$h(x) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$
3. **目标**：找到参数 $w$ 和 $b$，使得预测值与真实值的误差最小
4. **方法**：
   - 最小二乘法：直接求解解析解
   - 梯度下降：迭代优化参数
5. **评估**：使用MSE、RMSE、R²等指标评估模型性能

---

### 2. 说明最小二乘法和梯度下降的区别。

**参考答案**：

| 方面 | 最小二乘法 | 梯度下降 |
|------|-----------|---------|
| **方法** | 解析解，直接计算 | 数值解，迭代优化 |
| **公式** | $w = (X^TX)^{-1}X^Ty$ | $w := w - \alpha \nabla J$ |
| **优点** | 精确，一步到位 | 适合大规模数据，可扩展 |
| **缺点** | 需要矩阵求逆，计算复杂度高 | 需要调学习率，可能不收敛 |
| **适用** | 数据量小，特征少 | 数据量大，特征多 |

---

### 3. 解释过拟合和正则化的关系。

**参考答案**：

**过拟合**：
- 模型过于复杂，在训练集上表现很好，但在测试集上表现差
- 模型"记住"了训练数据的细节，而不是学习一般规律

**正则化**：
- 通过在损失函数中添加惩罚项来限制模型复杂度
- **Ridge（L2）**：$J = MSE + \lambda ||w||^2$，所有参数都缩小
- **Lasso（L1）**：$J = MSE + \lambda ||w||_1$，部分参数变为0

**关系**：
- 正则化通过限制参数大小来防止模型过于复杂
- $\lambda$ 越大，正则化越强，模型越简单，越不容易过拟合
- 但 $\lambda$ 太大可能导致欠拟合

---

### 4. 说明线性回归的优缺点。

**参考答案**：

**优点**：
1. **简单易懂**：原理简单，易于理解和实现
2. **计算快速**：训练和预测都很快
3. **可解释性强**：参数有明确的物理意义，可以解释特征的影响
4. **稳定**：不容易过拟合（有正则化时）
5. **理论基础强**：有坚实的数学基础

**缺点**：
1. **线性假设**：只能处理线性关系，非线性关系需要特征工程
2. **对异常值敏感**：异常值会显著影响结果
3. **特征相关**：多重共线性会导致参数估计不稳定
4. **需要特征工程**：非线性关系需要手动创建特征

---

## 三、计算题（每题10分，共20分）

### 1. 给定数据点 (1, 2), (2, 3), (3, 5)，使用最小二乘法求线性回归的参数。

**参考答案**：

**步骤1：构建矩阵**

$$X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{bmatrix}, \quad y = \begin{bmatrix} 2 \\ 3 \\ 5 \end{bmatrix}$$

**步骤2：计算**

$$X^TX = \begin{bmatrix} 3 & 6 \\ 6 & 14 \end{bmatrix}$$

$$X^Ty = \begin{bmatrix} 10 \\ 23 \end{bmatrix}$$

**步骤3：求逆**

$$(X^TX)^{-1} = \frac{1}{6} \begin{bmatrix} 14 & -6 \\ -6 & 3 \end{bmatrix}$$

**步骤4：求解**

$$\theta = (X^TX)^{-1}X^Ty = \begin{bmatrix} \frac{1}{3} \\ \frac{3}{2} \end{bmatrix}$$

所以：$b = \frac{1}{3}$，$w = \frac{3}{2}$

**验证**：$h(x) = \frac{1}{3} + \frac{3}{2}x$

---

### 2. 给定损失函数 $J(w) = \frac{1}{2}(wx + b - y)^2$，计算梯度。

**参考答案**：

**对 $w$ 求导**：

$$\frac{\partial J}{\partial w} = \frac{1}{2} \cdot 2(wx + b - y) \cdot x = (wx + b - y)x$$

**对 $b$ 求导**：

$$\frac{\partial J}{\partial b} = \frac{1}{2} \cdot 2(wx + b - y) \cdot 1 = (wx + b - y)$$

**梯度**：

$$\nabla J = \begin{bmatrix} \frac{\partial J}{\partial w} \\ \frac{\partial J}{\partial b} \end{bmatrix} = \begin{bmatrix} (wx + b - y)x \\ (wx + b - y) \end{bmatrix}$$

---

## 评分标准

- **选择题**：每题2分，共20分
- **简答题**：每题10分，共40分
- **计算题**：每题10分，共20分
- **总分**：80分
- **及格线**：64分（80%）

---

**完成后请对照答案检查，理解每道题的原理！**

