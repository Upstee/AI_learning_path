# 最小二乘法推导

## 1. 问题设定

给定训练集 $\{(\mathbf{x}^{(1)}, y^{(1)}), (\mathbf{x}^{(2)}, y^{(2)}), ..., (\mathbf{x}^{(m)}, y^{(m)})\}$，其中：
- $\mathbf{x}^{(i)} \in \mathbb{R}^n$ 是第 $i$ 个样本的特征向量
- $y^{(i)} \in \mathbb{R}$ 是第 $i$ 个样本的目标值
- $m$ 是样本数量
- $n$ 是特征数量

我们的目标是找到参数 $\mathbf{w} \in \mathbb{R}^n$ 和 $b \in \mathbb{R}$，使得：
$$h(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b$$

预测值与真实值的误差最小。

## 2. 矩阵表示

### 2.1 特征矩阵

将所有样本的特征向量堆叠成矩阵：
$$\mathbf{X} = \begin{bmatrix}
(\mathbf{x}^{(1)})^T \\
(\mathbf{x}^{(2)})^T \\
\vdots \\
(\mathbf{x}^{(m)})^T
\end{bmatrix} = \begin{bmatrix}
x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
\vdots & \vdots & \ddots & \vdots \\
x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
\end{bmatrix}$$

这是一个 $m \times n$ 的矩阵。

### 2.2 目标向量

将所有目标值堆叠成向量：
$$\mathbf{y} = \begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix}$$

这是一个 $m \times 1$ 的向量。

### 2.3 参数向量

将偏置 $b$ 合并到权重向量中：
$$\boldsymbol{\theta} = \begin{bmatrix}
b \\
w_1 \\
w_2 \\
\vdots \\
w_n
\end{bmatrix}$$

对应的特征矩阵需要添加一列1：
$$\mathbf{X}' = \begin{bmatrix}
1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
1 & x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
\end{bmatrix}$$

这样，预测可以写成：
$$\mathbf{h} = \mathbf{X}' \boldsymbol{\theta}$$

## 3. 损失函数

### 3.1 均方误差

损失函数定义为：
$$J(\boldsymbol{\theta}) = \frac{1}{2m} \sum_{i=1}^{m} (h(\mathbf{x}^{(i)}) - y^{(i)})^2$$

使用矩阵表示：
$$J(\boldsymbol{\theta}) = \frac{1}{2m} ||\mathbf{X}' \boldsymbol{\theta} - \mathbf{y}||^2$$

其中 $||\cdot||$ 表示向量的L2范数。

### 3.2 展开形式

$$J(\boldsymbol{\theta}) = \frac{1}{2m} (\mathbf{X}' \boldsymbol{\theta} - \mathbf{y})^T (\mathbf{X}' \boldsymbol{\theta} - \mathbf{y})$$

展开：
$$J(\boldsymbol{\theta}) = \frac{1}{2m} [\boldsymbol{\theta}^T (\mathbf{X}')^T \mathbf{X}' \boldsymbol{\theta} - 2\boldsymbol{\theta}^T (\mathbf{X}')^T \mathbf{y} + \mathbf{y}^T \mathbf{y}]$$

## 4. 梯度计算

### 4.1 对参数求导

对 $J(\boldsymbol{\theta})$ 关于 $\boldsymbol{\theta}$ 求导：

$$\frac{\partial J}{\partial \boldsymbol{\theta}} = \frac{1}{2m} \frac{\partial}{\partial \boldsymbol{\theta}} [\boldsymbol{\theta}^T (\mathbf{X}')^T \mathbf{X}' \boldsymbol{\theta} - 2\boldsymbol{\theta}^T (\mathbf{X}')^T \mathbf{y} + \mathbf{y}^T \mathbf{y}]$$

### 4.2 逐项求导

**第一项**：$\frac{\partial}{\partial \boldsymbol{\theta}} [\boldsymbol{\theta}^T (\mathbf{X}')^T \mathbf{X}' \boldsymbol{\theta}]$

使用矩阵求导公式：
$$\frac{\partial}{\partial \mathbf{x}} \mathbf{x}^T \mathbf{A} \mathbf{x} = (\mathbf{A} + \mathbf{A}^T) \mathbf{x}$$

由于 $(\mathbf{X}')^T \mathbf{X}'$ 是对称矩阵：
$$\frac{\partial}{\partial \boldsymbol{\theta}} [\boldsymbol{\theta}^T (\mathbf{X}')^T \mathbf{X}' \boldsymbol{\theta}] = 2(\mathbf{X}')^T \mathbf{X}' \boldsymbol{\theta}$$

**第二项**：$\frac{\partial}{\partial \boldsymbol{\theta}} [2\boldsymbol{\theta}^T (\mathbf{X}')^T \mathbf{y}]$

$$\frac{\partial}{\partial \boldsymbol{\theta}} [2\boldsymbol{\theta}^T (\mathbf{X}')^T \mathbf{y}] = 2(\mathbf{X}')^T \mathbf{y}$$

**第三项**：$\frac{\partial}{\partial \boldsymbol{\theta}} [\mathbf{y}^T \mathbf{y}] = 0$

### 4.3 最终梯度

$$\frac{\partial J}{\partial \boldsymbol{\theta}} = \frac{1}{m} [(\mathbf{X}')^T \mathbf{X}' \boldsymbol{\theta} - (\mathbf{X}')^T \mathbf{y}]$$

## 5. 最优解

### 5.1 令梯度为零

令 $\frac{\partial J}{\partial \boldsymbol{\theta}} = 0$：

$$(\mathbf{X}')^T \mathbf{X}' \boldsymbol{\theta} - (\mathbf{X}')^T \mathbf{y} = 0$$

$$(\mathbf{X}')^T \mathbf{X}' \boldsymbol{\theta} = (\mathbf{X}')^T \mathbf{y}$$

### 5.2 求解

如果 $(\mathbf{X}')^T \mathbf{X}'$ 可逆（即 $\mathbf{X}'$ 列满秩），则：

$$\boldsymbol{\theta} = [(\mathbf{X}')^T \mathbf{X}']^{-1} (\mathbf{X}')^T \mathbf{y}$$

这就是**最小二乘法的解析解**。

### 5.3 不可逆情况

如果 $(\mathbf{X}')^T \mathbf{X}'$ 不可逆（即存在多重共线性），可以使用：
- **伪逆**：$\boldsymbol{\theta} = (\mathbf{X}')^+ \mathbf{y}$，其中 $(\mathbf{X}')^+$ 是伪逆
- **正则化**：添加正则项使矩阵可逆
- **特征选择**：删除相关特征

## 6. 几何解释

### 6.1 投影视角

最小二乘法可以理解为：在 $m$ 维空间中，找到 $\mathbf{X}'$ 的列空间中的一个向量，使得它与目标向量 $\mathbf{y}$ 的距离最小。

这个向量就是 $\mathbf{y}$ 在 $\mathbf{X}'$ 的列空间上的**投影**。

### 6.2 正交性

最优解满足：
$$(\mathbf{X}')^T (\mathbf{X}' \boldsymbol{\theta} - \mathbf{y}) = 0$$

这意味着残差 $\mathbf{X}' \boldsymbol{\theta} - \mathbf{y}$ 与 $\mathbf{X}'$ 的列空间**正交**。

## 7. 计算复杂度

- **矩阵乘法**：$O(mn^2)$
- **矩阵求逆**：$O(n^3)$
- **总复杂度**：$O(mn^2 + n^3)$

当 $m$ 很大时，计算成本较高，此时梯度下降可能更合适。

## 8. 数值稳定性

### 8.1 条件数

矩阵 $(\mathbf{X}')^T \mathbf{X}'$ 的条件数可能很大，导致数值不稳定。

### 8.2 改进方法

- **特征缩放**：标准化特征
- **QR分解**：使用QR分解求解
- **SVD分解**：使用奇异值分解

---

**下一节**：梯度下降算法详解

