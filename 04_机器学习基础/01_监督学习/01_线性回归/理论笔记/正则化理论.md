# 正则化理论

## 1. 过拟合问题

### 1.1 什么是过拟合

**过拟合（Overfitting）**：模型在训练集上表现很好，但在测试集上表现差。

**原因**：
- 模型过于复杂
- 训练数据太少
- 模型"记住"了训练数据的噪声

**表现**：
- 训练集误差小，测试集误差大
- 模型对训练数据过度拟合

### 1.2 欠拟合

**欠拟合（Underfitting）**：模型在训练集和测试集上都表现差。

**原因**：
- 模型过于简单
- 特征不足
- 模型无法捕捉数据中的模式

## 2. 偏差-方差分解

### 2.1 泛化误差分解

泛化误差可以分解为：
$$\text{泛化误差} = \text{偏差}^2 + \text{方差} + \text{噪声}$$

**偏差（Bias）**：
- 模型的拟合能力
- 高偏差：模型过于简单，无法拟合数据
- 对应欠拟合

**方差（Variance）**：
- 模型对数据变化的敏感度
- 高方差：模型过于复杂，对数据变化敏感
- 对应过拟合

#### ⚠️【知其所以然】偏差-方差分解的数学证明

**设定**：
- 真实函数：$f(\mathbf{x})$
- 训练集：$\mathcal{D} = \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^{m}$
- 学习到的模型：$\hat{f}_{\mathcal{D}}(\mathbf{x})$
- 期望模型：$\bar{f}(\mathbf{x}) = E_{\mathcal{D}}[\hat{f}_{\mathcal{D}}(\mathbf{x})]$

**证明**：
对于测试点 $(\mathbf{x}, y)$，其中 $y = f(\mathbf{x}) + \epsilon$，$\epsilon$ 是噪声（$E[\epsilon] = 0$，$\text{Var}(\epsilon) = \sigma^2$）。

期望平方误差：
$$E[(y - \hat{f}_{\mathcal{D}}(\mathbf{x}))^2] = E[(f(\mathbf{x}) + \epsilon - \hat{f}_{\mathcal{D}}(\mathbf{x}))^2]$$

展开：
$$= E[(f(\mathbf{x}) - \hat{f}_{\mathcal{D}}(\mathbf{x}))^2] + 2E[\epsilon(f(\mathbf{x}) - \hat{f}_{\mathcal{D}}(\mathbf{x}))] + E[\epsilon^2]$$

由于 $\epsilon$ 与模型独立，$E[\epsilon(f(\mathbf{x}) - \hat{f}_{\mathcal{D}}(\mathbf{x}))] = 0$，所以：
$$= E[(f(\mathbf{x}) - \hat{f}_{\mathcal{D}}(\mathbf{x}))^2] + \sigma^2$$

进一步分解 $E[(f(\mathbf{x}) - \hat{f}_{\mathcal{D}}(\mathbf{x}))^2]$：
$$= E[(f(\mathbf{x}) - \bar{f}(\mathbf{x}) + \bar{f}(\mathbf{x}) - \hat{f}_{\mathcal{D}}(\mathbf{x}))^2]$$
$$= E[(f(\mathbf{x}) - \bar{f}(\mathbf{x}))^2] + E[(\bar{f}(\mathbf{x}) - \hat{f}_{\mathcal{D}}(\mathbf{x}))^2] + 2E[(f(\mathbf{x}) - \bar{f}(\mathbf{x}))(\bar{f}(\mathbf{x}) - \hat{f}_{\mathcal{D}}(\mathbf{x}))]$$

由于 $E[\hat{f}_{\mathcal{D}}(\mathbf{x})] = \bar{f}(\mathbf{x})$，交叉项为0：
$$= (f(\mathbf{x}) - \bar{f}(\mathbf{x}))^2 + E[(\bar{f}(\mathbf{x}) - \hat{f}_{\mathcal{D}}(\mathbf{x}))^2]$$

因此：
$$\text{泛化误差} = \underbrace{(f(\mathbf{x}) - \bar{f}(\mathbf{x}))^2}_{\text{偏差}^2} + \underbrace{E[(\bar{f}(\mathbf{x}) - \hat{f}_{\mathcal{D}}(\mathbf{x}))^2]}_{\text{方差}} + \underbrace{\sigma^2}_{\text{噪声}}$$

**结论**：
- **偏差**：模型平均预测与真实值的差异（拟合能力）
- **方差**：模型预测的波动（稳定性）
- **噪声**：数据本身的随机性（不可减少）

**正则化的作用**：
- 增加偏差（模型变简单）
- 减少方差（模型更稳定）
- 通过权衡偏差和方差，最小化泛化误差

### 2.2 偏差-方差权衡

- **简单模型**：高偏差，低方差（欠拟合）
- **复杂模型**：低偏差，高方差（过拟合）
- **理想模型**：低偏差，低方差

正则化通过限制模型复杂度来平衡偏差和方差。

## 3. Ridge回归（L2正则化）

### 3.1 损失函数

Ridge回归的损失函数：
$$J(w) = \frac{1}{2m} ||Xw - y||^2 + \lambda ||w||^2$$

其中：
- 第一项是均方误差
- 第二项是L2正则化项
- $\lambda$ 是正则化参数

### 3.2 L2范数

$$||w||^2 = \sum_{i=1}^{n} w_i^2 = w^T w$$

### 3.3 解析解

对损失函数求导并令其为零：
$$\frac{\partial J}{\partial w} = \frac{1}{m} X^T(Xw - y) + 2\lambda w = 0$$

$$X^T X w + m\lambda w = X^T y$$

$$(X^T X + m\lambda I) w = X^T y$$

如果 $(X^T X + m\lambda I)$ 可逆：
$$w = (X^T X + m\lambda I)^{-1} X^T y$$

### 3.4 特点

**优点**：
- 所有参数都缩小，但不为0
- 适合特征很多的情况
- 数值稳定（矩阵总是可逆）

**缺点**：
- 不能进行特征选择
- 所有特征都保留

### 3.5 参数选择

**$\lambda$ 的影响**：
- $\lambda = 0$：退化为普通线性回归
- $\lambda$ 小：轻微正则化，参数略缩小
- $\lambda$ 大：强正则化，参数大幅缩小，可能欠拟合

**选择方法**：
- 交叉验证
- 网格搜索
- 观察验证集性能

## 4. Lasso回归（L1正则化）

### 4.1 损失函数

Lasso回归的损失函数：
$$J(w) = \frac{1}{2m} ||Xw - y||^2 + \lambda ||w||_1$$

其中 $||w||_1 = \sum_{i=1}^{n} |w_i|$ 是L1范数。

### 4.2 特点

**关键特性**：
- **特征选择**：部分参数的系数变为0
- **稀疏性**：产生稀疏模型
- **可解释性**：只保留重要特征

### 4.3 为什么能产生稀疏性

#### ⚠️【知其所以然】Lasso为什么能产生稀疏性？几何和优化理论解释

**1. 几何直观：等高线的交点**

考虑优化问题：
$$\min_{\mathbf{w}} \frac{1}{2}||\mathbf{X}\mathbf{w} - \mathbf{y}||^2 + \lambda ||\mathbf{w}||_1$$

**L1范数的等高线**：$||\mathbf{w}||_1 = c$ 在2D空间中是**菱形**（$|w_1| + |w_2| = c$）

**损失函数的等高线**：$\frac{1}{2}||\mathbf{X}\mathbf{w} - \mathbf{y}||^2 = c$ 是**椭圆**

**关键观察**：
- 当 $\lambda$ 增大时，可行域（菱形）缩小
- 最优解在椭圆的等高线与菱形的**顶点**相交
- 菱形的顶点在坐标轴上，即 $w_i = 0$ 的点

**可视化**（2D情况）：
```
        w2
        |
        |    /\
        |   /  \  (L1等高线：菱形)
        |  /    \
        | /      \
        |/________\____ w1
       /|          \
      / |            \
     /  |              \
    /   |                \
```

当椭圆与菱形相切时，切点通常在顶点（坐标轴上），使得 $w_1 = 0$ 或 $w_2 = 0$。

**2. 优化理论：次梯度（Subgradient）**

L1正则化在 $w_i = 0$ 处不可导，需要使用**次梯度**。

**次梯度定义**：对于 $f(w) = |w|$，在 $w = 0$ 处的次梯度集合为：
$$\partial f(0) = [-1, 1]$$

**KKT条件**（Karush-Kuhn-Tucker）：
在最优解 $\mathbf{w}^*$ 处，必须满足：
$$\frac{\partial J}{\partial w_i} + \lambda \cdot \text{sign}(w_i^*) = 0, \quad \text{如果 } w_i^* \neq 0$$
$$|\frac{\partial J}{\partial w_i}| \leq \lambda, \quad \text{如果 } w_i^* = 0$$

**稀疏性条件**：
- 如果 $|\frac{\partial J}{\partial w_i}| < \lambda$，则 $w_i^* = 0$（特征被选择掉）
- 如果 $|\frac{\partial J}{\partial w_i}| = \lambda$，则 $w_i^*$ 可能为0或非0（临界情况）

**3. 为什么L2不能产生稀疏性？**

**L2范数的等高线**：$||\mathbf{w}||_2^2 = c$ 是**圆形**（2D）或**球面**（高维）

**关键区别**：
- 圆形是**光滑的**，没有尖角
- 椭圆与圆形相切时，切点**几乎不可能**在坐标轴上
- 因此L2正则化**不会**产生稀疏性

**数学证明**：
L2正则化的梯度：
$$\frac{\partial}{\partial w_i}(\lambda ||\mathbf{w}||_2^2) = 2\lambda w_i$$

在 $w_i = 0$ 处，梯度为0，但这是**最小值点**，不是不可导点。KKT条件要求：
$$\frac{\partial J}{\partial w_i} + 2\lambda w_i = 0$$

除非 $\frac{\partial J}{\partial w_i} = 0$（恰好），否则 $w_i \neq 0$。

**4. 稀疏性的优势**

**特征选择**：
- 自动识别重要特征
- 减少模型复杂度
- 提高可解释性

**计算优势**：
- 稀疏模型存储和计算更高效
- 预测时只需要非零特征

**统计优势**：
- 减少过拟合风险
- 提高泛化能力

### 4.4 求解方法

Lasso没有解析解，需要使用：
- **坐标下降法**
- **最小角回归（LARS）**
- **近端梯度法**

### 4.5 参数选择

**$\lambda$ 的影响**：
- $\lambda = 0$：退化为普通线性回归
- $\lambda$ 小：少量特征被选择
- $\lambda$ 大：更多特征被选择为0，可能欠拟合

## 5. Elastic Net

### 5.1 损失函数

结合L1和L2正则化：
$$J(w) = \frac{1}{2m} ||Xw - y||^2 + \lambda_1 ||w||_1 + \lambda_2 ||w||^2$$

或写成：
$$J(w) = \frac{1}{2m} ||Xw - y||^2 + \lambda (\rho ||w||_1 + \frac{1-\rho}{2} ||w||^2)$$

其中 $\rho$ 是L1比例。

### 5.2 特点

**优点**：
- 结合L1和L2的优点
- 在特征相关时更稳定
- 可以进行特征选择

**适用场景**：
- 特征数量 > 样本数量
- 特征高度相关
- 需要特征选择

## 6. 正则化参数选择

### 6.1 交叉验证

**K折交叉验证**：
1. 将数据分成K折
2. 对每个 $\lambda$ 值：
   - 用K-1折训练
   - 用剩余1折验证
   - 计算平均性能
3. 选择性能最好的 $\lambda$

### 6.2 网格搜索

```python
from sklearn.model_selection import GridSearchCV

param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}
ridge = Ridge()
grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)
best_alpha = grid_search.best_params_['alpha']
```

### 6.3 学习曲线

绘制不同 $\lambda$ 值的验证集性能曲线，选择性能最好的点。

## 7. 正则化效果可视化

### 7.1 系数路径

绘制不同 $\lambda$ 值下各特征系数的变化：
- Lasso：系数逐渐变为0
- Ridge：系数逐渐缩小但不为0

### 7.2 偏差-方差曲线

- 训练集误差：随 $\lambda$ 增大而增大
- 验证集误差：先减小后增大（U形曲线）
- 最优 $\lambda$：验证集误差最小的点

## 8. 实际应用建议

### 8.1 何时使用正则化

**使用Ridge**：
- 特征很多
- 所有特征都可能有用
- 特征相关

**使用Lasso**：
- 需要特征选择
- 特征很多，但只有部分重要
- 需要可解释性

**使用Elastic Net**：
- 特征数量 > 样本数量
- 特征高度相关
- 需要特征选择但也要稳定性

### 8.2 实践步骤

1. **数据预处理**：标准化特征（对正则化很重要）
2. **尝试不同模型**：线性回归、Ridge、Lasso
3. **参数调优**：使用交叉验证选择最佳 $\lambda$
4. **评估**：在测试集上评估最终模型
5. **分析**：查看特征重要性

## 9. 总结

正则化是防止过拟合的重要技术：

1. **Ridge（L2）**：缩小所有参数，适合特征很多
2. **Lasso（L1）**：特征选择，产生稀疏模型
3. **Elastic Net**：结合两者优点
4. **参数选择**：使用交叉验证
5. **特征缩放**：对正则化很重要

---

**完成理论笔记学习！**

