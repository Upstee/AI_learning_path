# 梯度下降算法详解

## 1. 基本概念

### 1.1 什么是梯度下降

梯度下降（Gradient Descent）是一种用于优化函数的迭代算法。它通过沿着函数梯度的反方向移动来找到函数的最小值。

**核心思想**：
- 从随机点开始
- 计算梯度（函数在该点的斜率）
- 沿着梯度反方向移动（因为我们要找最小值）
- 重复直到收敛

### 1.2 在线性回归中的应用

对于线性回归，损失函数是：
$$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2$$

梯度下降的目标是找到使 $J(w, b)$ 最小的 $w$ 和 $b$。

## 2. 梯度计算

### 2.1 对权重w求导

$$\frac{\partial J}{\partial w} = \frac{1}{m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) \cdot x^{(i)}$$

使用矩阵表示：
$$\frac{\partial J}{\partial w} = \frac{1}{m} X^T (Xw + b - y)$$

### 2.2 对偏置b求导

$$\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})$$

使用矩阵表示：
$$\frac{\partial J}{\partial b} = \frac{1}{m} \mathbf{1}^T (Xw + b - y)$$

其中 $\mathbf{1}$ 是全1向量。

## 3. 更新规则

### 3.1 基本更新规则

$$w := w - \alpha \frac{\partial J}{\partial w}$$

$$b := b - \alpha \frac{\partial J}{\partial b}$$

其中 $\alpha$ 是学习率（步长）。

### 3.2 同时更新

**重要**：必须同时更新 $w$ 和 $b$，不能先更新一个再用新值更新另一个。

**错误做法**：
```python
w = w - alpha * dw
b = b - alpha * db  # 这里使用了新的w，错误！
```

**正确做法**：
```python
w_new = w - alpha * dw
b_new = b - alpha * db
w = w_new
b = b_new
```

## 4. 学习率选择

### 4.1 学习率的影响

**学习率太小**：
- 收敛慢
- 需要很多次迭代
- 可能陷入局部最优

**学习率太大**：
- 可能发散
- 损失函数值可能增大
- 无法收敛

**学习率合适**：
- 快速收敛
- 稳定下降
- 找到最优解

#### ⚠️【知其所以然】为什么梯度下降能收敛？学习率如何影响收敛？

**1. 收敛性证明（凸函数情况）**

对于线性回归，损失函数 $J(\mathbf{w}) = \frac{1}{2m}||\mathbf{X}\mathbf{w} - \mathbf{y}||^2$ 是**凸函数**。

**关键性质**：凸函数的梯度满足：
$$J(\mathbf{w}') \geq J(\mathbf{w}) + \nabla J(\mathbf{w})^T(\mathbf{w}' - \mathbf{w})$$

**更新规则**：$\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \alpha \nabla J(\mathbf{w}^{(t)})$

**收敛条件**：如果学习率 $\alpha$ 满足 $0 < \alpha < \frac{2}{L}$，其中 $L$ 是损失函数的Lipschitz常数，则梯度下降收敛。

对于线性回归，$L = \frac{1}{m}\lambda_{max}(\mathbf{X}^T\mathbf{X})$，其中 $\lambda_{max}$ 是最大特征值。

**证明思路**：
1. 使用凸函数的性质
2. 证明每次迭代损失函数值下降
3. 证明损失函数有下界（$\geq 0$）
4. 由单调有界定理，序列收敛

**2. 学习率的影响（数学分析）**

**学习率太小**：
- 每次更新步长小：$||\mathbf{w}^{(t+1)} - \mathbf{w}^{(t)}|| = \alpha ||\nabla J||$
- 需要很多次迭代才能接近最优解
- 收敛速度：$O(1/t)$（次线性收敛）

**学习率太大**：
- 可能"跳过"最优解
- 损失函数值可能增大：$J(\mathbf{w}^{(t+1)}) > J(\mathbf{w}^{(t)})$
- 甚至发散：$||\mathbf{w}^{(t)}|| \to \infty$

**最优学习率**：
- 对于二次函数，最优学习率为：$\alpha^* = \frac{1}{L}$
- 此时收敛速度最快（线性收敛）

**3. 为什么线性回归的损失函数是凸的？**

**证明**：损失函数 $J(\mathbf{w}) = \frac{1}{2m}||\mathbf{X}\mathbf{w} - \mathbf{y}||^2$ 的Hessian矩阵为：
$$\mathbf{H} = \frac{1}{m}\mathbf{X}^T\mathbf{X}$$

对于任意向量 $\mathbf{v}$：
$$\mathbf{v}^T\mathbf{H}\mathbf{v} = \frac{1}{m}\mathbf{v}^T\mathbf{X}^T\mathbf{X}\mathbf{v} = \frac{1}{m}||\mathbf{X}\mathbf{v}||^2 \geq 0$$

因此 $\mathbf{H}$ 是半正定矩阵，损失函数是**凸函数**。

**凸函数的重要性**：
- **全局最优**：凸函数只有一个全局最小值，没有局部最小值
- **收敛保证**：梯度下降保证收敛到全局最优
- **稳定性**：不会陷入局部最优

**4. 几何直观：为什么沿着梯度反方向下降？**

梯度 $\nabla J(\mathbf{w})$ 指向**损失函数增长最快的方向**。

**证明**：方向导数：
$$D_{\mathbf{v}}J = \lim_{h \to 0} \frac{J(\mathbf{w} + h\mathbf{v}) - J(\mathbf{w})}{h} = \nabla J(\mathbf{w})^T\mathbf{v}$$

由柯西-施瓦茨不等式：
$$|\nabla J(\mathbf{w})^T\mathbf{v}| \leq ||\nabla J(\mathbf{w})|| \cdot ||\mathbf{v}||$$

当 $\mathbf{v} = -\frac{\nabla J(\mathbf{w})}{||\nabla J(\mathbf{w})||}$（负梯度方向）时，方向导数最小（最负），即**下降最快**。

**结论**：沿着负梯度方向更新，损失函数下降最快。

### 4.2 选择策略

1. **从小的开始**：通常从 0.01 或 0.001 开始
2. **逐步增大**：如果收敛太慢，可以增大
3. **观察损失**：如果损失震荡或增大，减小学习率
4. **自适应学习率**：使用Adam、RMSprop等自适应算法

## 5. 收敛判断

### 5.1 收敛条件

**方法1：损失变化**
$$|J^{(t)} - J^{(t-1)}| < \epsilon$$

**方法2：参数变化**
$$||w^{(t)} - w^{(t-1)}|| < \epsilon$$

**方法3：最大迭代次数**
达到预设的最大迭代次数后停止。

### 5.2 实现示例

```python
for i in range(max_iter):
    # 计算损失和梯度
    cost = compute_cost(X, y, w, b)
    dw, db = compute_gradient(X, y, w, b)
    
    # 更新参数
    w = w - alpha * dw
    b = b - alpha * db
    
    # 检查收敛
    if i > 0 and abs(cost_history[-1] - cost) < tol:
        break
    
    cost_history.append(cost)
```

## 6. 批量梯度下降 vs 随机梯度下降

### 6.1 批量梯度下降（Batch Gradient Descent）

**特点**：
- 每次使用所有训练样本计算梯度
- 更新稳定，但计算量大
- 适合小到中等规模数据

**更新规则**：
$$w := w - \alpha \frac{1}{m} \sum_{i=1}^{m} \nabla J_i(w)$$

### 6.2 随机梯度下降（Stochastic Gradient Descent, SGD）

**特点**：
- 每次只使用一个样本计算梯度
- 更新快，但可能震荡
- 适合大规模数据

**更新规则**：
$$w := w - \alpha \nabla J_i(w)$$

其中 $i$ 是随机选择的样本索引。

### 6.3 小批量梯度下降（Mini-batch Gradient Descent）

**特点**：
- 每次使用一小批样本（如32、64个）
- 平衡了速度和稳定性
- **实际应用中最常用**

**更新规则**：
$$w := w - \alpha \frac{1}{k} \sum_{i \in batch} \nabla J_i(w)$$

其中 $k$ 是批次大小。

## 7. 优化技巧

### 7.1 特征缩放

**为什么需要**：
- 不同特征的量纲不同
- 梯度下降的收敛速度受特征尺度影响
- 未缩放的特征可能导致收敛慢

**方法**：
- **标准化**：$x' = \frac{x - \mu}{\sigma}$
- **归一化**：$x' = \frac{x - x_{min}}{x_{max} - x_{min}}$

### 7.2 动量（Momentum）

**思想**：利用历史梯度的移动平均来加速收敛。

**更新规则**：
$$v_t = \beta v_{t-1} + (1-\beta) \nabla J(w_t)$$
$$w_{t+1} = w_t - \alpha v_t$$

其中 $\beta$ 是动量系数（通常0.9）。

### 7.3 自适应学习率

**Adam算法**：
- 结合动量和自适应学习率
- 对每个参数维护学习率的估计
- 通常比标准梯度下降更快收敛

## 8. 实现示例

### 8.1 批量梯度下降

```python
def batch_gradient_descent(X, y, alpha=0.01, max_iter=1000, tol=1e-6):
    m, n = X.shape
    w = np.zeros(n)
    b = 0
    cost_history = []
    
    for i in range(max_iter):
        # 预测
        y_pred = X @ w + b
        
        # 计算损失
        cost = (1 / (2 * m)) * np.sum((y_pred - y) ** 2)
        cost_history.append(cost)
        
        # 计算梯度
        dw = (1 / m) * X.T @ (y_pred - y)
        db = (1 / m) * np.sum(y_pred - y)
        
        # 更新参数
        w = w - alpha * dw
        b = b - alpha * db
        
        # 检查收敛
        if i > 0 and abs(cost_history[-2] - cost_history[-1]) < tol:
            break
    
    return w, b, cost_history
```

### 8.2 随机梯度下降

```python
def stochastic_gradient_descent(X, y, alpha=0.01, max_epochs=100):
    m, n = X.shape
    w = np.zeros(n)
    b = 0
    cost_history = []
    
    for epoch in range(max_epochs):
        # 打乱数据
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        epoch_cost = 0
        
        for i in range(m):
            # 单个样本
            x_i = X_shuffled[i]
            y_i = y_shuffled[i]
            
            # 预测
            y_pred = x_i @ w + b
            
            # 计算梯度
            dw = (y_pred - y_i) * x_i
            db = (y_pred - y_i)
            
            # 更新参数
            w = w - alpha * dw
            b = b - alpha * db
            
            # 累积损失
            epoch_cost += (y_pred - y_i) ** 2
        
        cost = epoch_cost / (2 * m)
        cost_history.append(cost)
    
    return w, b, cost_history
```

## 9. 常见问题

### 9.1 损失不下降

**可能原因**：
- 学习率太大
- 代码有bug
- 特征未缩放

**解决方法**：
- 减小学习率
- 检查代码
- 标准化特征

### 9.2 损失震荡

**可能原因**：
- 学习率太大
- 使用SGD（正常现象）

**解决方法**：
- 减小学习率
- 使用批量梯度下降或增加批次大小

### 9.3 收敛慢

**可能原因**：
- 学习率太小
- 特征未缩放
- 初始值不好

**解决方法**：
- 增大学习率
- 标准化特征
- 使用更好的初始化方法

## 10. 总结

梯度下降是优化线性回归参数的重要方法：

1. **原理**：沿着梯度反方向更新参数
2. **关键**：学习率选择和特征缩放
3. **变体**：批量、随机、小批量
4. **优化**：动量、自适应学习率

---

**下一节**：正则化理论

