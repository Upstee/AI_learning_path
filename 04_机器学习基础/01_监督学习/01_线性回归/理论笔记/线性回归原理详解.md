# 线性回归原理详解

## 1. 基本概念

### 1.1 什么是线性回归

线性回归（Linear Regression）是一种用于预测连续数值的监督学习算法。它假设特征和目标变量之间存在线性关系。

**数学表达式**：
对于单个特征：
$$h(x) = w_0 + w_1 x_1$$

对于多个特征：
$$h(x) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n$$

其中：
- $h(x)$ 是预测值
- $w_0$ 是偏置（截距）
- $w_1, w_2, ..., w_n$ 是权重（系数）
- $x_1, x_2, ..., x_n$ 是特征值

**向量形式**：
$$h(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b$$

其中 $\mathbf{w} = [w_1, w_2, ..., w_n]^T$，$\mathbf{x} = [x_1, x_2, ..., x_n]^T$，$b = w_0$。

### 1.2 线性回归的目标

线性回归的目标是找到一组参数 $\mathbf{w}$ 和 $b$，使得预测值与真实值之间的误差最小。

**损失函数（均方误差）**：
$$J(\mathbf{w}, b) = \frac{1}{2m} \sum_{i=1}^{m} (h(\mathbf{x}^{(i)}) - y^{(i)})^2$$

其中：
- $m$ 是训练样本数量
- $h(\mathbf{x}^{(i)})$ 是第 $i$ 个样本的预测值
- $y^{(i)}$ 是第 $i$ 个样本的真实值

#### ⚠️【知其所以然】为什么使用均方误差（MSE）？

**1. 统计意义：最大似然估计**

假设误差 $\epsilon^{(i)} = y^{(i)} - h(\mathbf{x}^{(i)})$ 服从正态分布：
$$\epsilon^{(i)} \sim \mathcal{N}(0, \sigma^2)$$

那么 $y^{(i)} | \mathbf{x}^{(i)} \sim \mathcal{N}(h(\mathbf{x}^{(i)}), \sigma^2)$，似然函数为：
$$L(\mathbf{w}, b) = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y^{(i)} - h(\mathbf{x}^{(i)}))^2}{2\sigma^2}\right)$$

取对数似然：
$$\log L(\mathbf{w}, b) = -\frac{m}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{m}(y^{(i)} - h(\mathbf{x}^{(i)}))^2$$

最大化对数似然等价于最小化 $\sum_{i=1}^{m}(y^{(i)} - h(\mathbf{x}^{(i)}))^2$，这就是MSE！

**结论**：MSE对应最大似然估计，假设误差服从正态分布。

**2. 数学性质：可导性和凸性**

- **可导性**：平方函数处处可导，便于使用梯度下降
- **凸性**：MSE是凸函数，保证全局最优解
- **平滑性**：梯度连续，优化稳定

**对比其他损失函数**：
- **MAE（平均绝对误差）**：$|y - h(x)|$，在0点不可导，优化困难
- **Huber损失**：结合MSE和MAE，但更复杂

**3. 几何意义：欧氏距离**

MSE实际上是预测向量 $\mathbf{h} = [h(\mathbf{x}^{(1)}), ..., h(\mathbf{x}^{(m)})]^T$ 和真实向量 $\mathbf{y} = [y^{(1)}, ..., y^{(m)}]^T$ 之间的**欧氏距离的平方**：
$$J = \frac{1}{2m} ||\mathbf{h} - \mathbf{y}||^2$$

这符合我们对"距离"的直觉理解。

**4. 为什么除以2？**

$$\frac{1}{2m}$$ 中的 $\frac{1}{2}$ 是为了求导时消去系数2，使梯度更简洁：
$$\frac{\partial J}{\partial w} = \frac{1}{m}\sum_{i=1}^{m}(h(\mathbf{x}^{(i)}) - y^{(i)})x^{(i)}$$

如果没有 $\frac{1}{2}$，梯度会有额外的系数2。

## 2. 最小二乘法（解析解）

### 2.1 矩阵形式

将问题写成矩阵形式：
$$\mathbf{y} = \mathbf{X}\mathbf{w} + \mathbf{b}$$

其中：
- $\mathbf{X}$ 是 $m \times n$ 的特征矩阵
- $\mathbf{y}$ 是 $m \times 1$ 的目标向量
- $\mathbf{w}$ 是 $n \times 1$ 的权重向量

### 2.2 推导过程

**步骤1：定义损失函数**

$$J(\mathbf{w}) = \frac{1}{2m} ||\mathbf{X}\mathbf{w} - \mathbf{y}||^2$$

**步骤2：对权重求导**

$$\frac{\partial J}{\partial \mathbf{w}} = \frac{1}{m} \mathbf{X}^T(\mathbf{X}\mathbf{w} - \mathbf{y})$$

**步骤3：令导数为零**

$$\mathbf{X}^T(\mathbf{X}\mathbf{w} - \mathbf{y}) = 0$$

$$\mathbf{X}^T\mathbf{X}\mathbf{w} = \mathbf{X}^T\mathbf{y}$$

**步骤4：求解**

如果 $\mathbf{X}^T\mathbf{X}$ 可逆：
$$\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$

这就是**最小二乘法的解析解**。

### 2.3 几何解释

最小二乘法在几何上可以理解为：在 $n$ 维空间中，找到一条直线（或超平面），使得所有数据点到这条直线的距离平方和最小。

#### ⚠️【知其所以然】为什么最小二乘法的解是最优的？

**1. 投影理论（Projection Theorem）**

在 $m$ 维空间中，$\mathbf{y}$ 是真实值向量，$\mathbf{X}\mathbf{w}$ 是预测值向量（在 $\mathbf{X}$ 的列空间中）。

**关键洞察**：最优解 $\mathbf{w}^*$ 使得残差 $\mathbf{e} = \mathbf{y} - \mathbf{X}\mathbf{w}^*$ **与列空间正交**。

**证明**：
- 如果 $\mathbf{e}$ 不与列空间正交，那么存在 $\mathbf{X}\mathbf{v}$ 使得 $\mathbf{e}^T\mathbf{X}\mathbf{v} \neq 0$
- 这意味着可以调整 $\mathbf{w}$ 使误差更小，矛盾！
- 因此必须有：$\mathbf{X}^T\mathbf{e} = 0$，即 $\mathbf{X}^T(\mathbf{y} - \mathbf{X}\mathbf{w}) = 0$

这就是最小二乘法的法方程！

**几何直观**：
```
        y (真实值)
        |
        | e (残差，与列空间正交)
        |
        |/
    Xw* (最优预测，y在列空间上的投影)
```

**2. 最优性证明（严格数学证明）**

**定理**：如果 $\mathbf{X}^T\mathbf{X}$ 可逆，则 $\mathbf{w}^* = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$ 是唯一的最优解。

**证明**：
设任意 $\mathbf{w}$，损失为：
$$J(\mathbf{w}) = ||\mathbf{X}\mathbf{w} - \mathbf{y}||^2$$

展开：
$$J(\mathbf{w}) = (\mathbf{X}\mathbf{w} - \mathbf{y})^T(\mathbf{X}\mathbf{w} - \mathbf{y})$$
$$= \mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w} - 2\mathbf{y}^T\mathbf{X}\mathbf{w} + \mathbf{y}^T\mathbf{y}$$

对 $\mathbf{w}$ 求导并令其为零：
$$\frac{\partial J}{\partial \mathbf{w}} = 2\mathbf{X}^T\mathbf{X}\mathbf{w} - 2\mathbf{X}^T\mathbf{y} = 0$$

得到：$\mathbf{X}^T\mathbf{X}\mathbf{w} = \mathbf{X}^T\mathbf{y}$

如果 $\mathbf{X}^T\mathbf{X}$ 可逆，则：
$$\mathbf{w}^* = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$

**验证这是最小值**：
二阶导数（Hessian矩阵）：
$$\frac{\partial^2 J}{\partial \mathbf{w}^2} = 2\mathbf{X}^T\mathbf{X}$$

由于 $\mathbf{X}^T\mathbf{X}$ 是半正定矩阵（对所有 $\mathbf{v}$，$\mathbf{v}^T\mathbf{X}^T\mathbf{X}\mathbf{v} = ||\mathbf{X}\mathbf{v}||^2 \geq 0$），所以这是**全局最小值**。

**3. 统计意义：BLUE（最佳线性无偏估计）**

在高斯-马尔可夫定理下，最小二乘估计是**最佳线性无偏估计（BLUE）**：
- **线性**：估计量是观测值的线性函数
- **无偏**：$E[\mathbf{w}^*] = \mathbf{w}_{true}$
- **最佳**：在所有线性无偏估计中，方差最小

**4. 为什么需要 $\mathbf{X}^T\mathbf{X}$ 可逆？**

- **可逆条件**：$\mathbf{X}$ 列满秩（特征不线性相关）
- **如果不可逆**：存在多重共线性，解不唯一
- **解决方法**：使用伪逆 $(\mathbf{X}^T\mathbf{X})^+$ 或正则化

## 3. 梯度下降（数值解）

### 3.1 梯度下降原理

当 $\mathbf{X}^T\mathbf{X}$ 不可逆或数据量很大时，可以使用梯度下降法迭代求解。

**梯度下降更新规则**：
$$\mathbf{w} := \mathbf{w} - \alpha \nabla J(\mathbf{w})$$

其中：
- $\alpha$ 是学习率（步长）
- $\nabla J(\mathbf{w})$ 是损失函数的梯度

### 3.2 梯度计算

对于线性回归，梯度为：
$$\nabla J(\mathbf{w}) = \frac{1}{m} \mathbf{X}^T(\mathbf{X}\mathbf{w} - \mathbf{y})$$

**更新规则**：
$$\mathbf{w} := \mathbf{w} - \alpha \frac{1}{m} \mathbf{X}^T(\mathbf{X}\mathbf{w} - \mathbf{y})$$

### 3.3 学习率选择

学习率 $\alpha$ 的选择很重要：
- **太小**：收敛慢，需要很多次迭代
- **太大**：可能发散，无法收敛
- **合适**：快速收敛到最优解

通常从 0.01 或 0.001 开始尝试。

### 3.4 批量梯度下降 vs 随机梯度下降

**批量梯度下降（Batch Gradient Descent）**：
- 使用所有训练样本计算梯度
- 每次更新都使用完整数据集
- 收敛稳定，但计算量大

**随机梯度下降（Stochastic Gradient Descent）**：
- 每次只使用一个样本计算梯度
- 更新速度快，但可能震荡
- 适合大规模数据

**小批量梯度下降（Mini-batch Gradient Descent）**：
- 每次使用一小批样本（如32、64个）
- 平衡了速度和稳定性
- 实际应用中最常用

## 4. 正则化

### 4.1 过拟合问题

当模型过于复杂时，可能在训练集上表现很好，但在测试集上表现差，这就是**过拟合**。

### 4.2 Ridge回归（L2正则化）

**损失函数**：
$$J(\mathbf{w}) = \frac{1}{2m} ||\mathbf{X}\mathbf{w} - \mathbf{y}||^2 + \lambda ||\mathbf{w}||^2$$

其中 $\lambda$ 是正则化参数。

**解析解**：
$$\mathbf{w} = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}$$

**特点**：
- 所有参数都缩小，但不为0
- 适合特征很多的情况
- 防止过拟合

### 4.3 Lasso回归（L1正则化）

**损失函数**：
$$J(\mathbf{w}) = \frac{1}{2m} ||\mathbf{X}\mathbf{w} - \mathbf{y}||^2 + \lambda ||\mathbf{w}||_1$$

其中 $||\mathbf{w}||_1 = \sum_{i=1}^{n} |w_i|$。

**特点**：
- 部分参数变为0（特征选择）
- 适合特征选择
- 可以产生稀疏模型

### 4.4 Elastic Net

结合L1和L2正则化：
$$J(\mathbf{w}) = \frac{1}{2m} ||\mathbf{X}\mathbf{w} - \mathbf{y}||^2 + \lambda_1 ||\mathbf{w}||_1 + \lambda_2 ||\mathbf{w}||^2$$

## 5. 评估指标

### 5.1 均方误差（MSE）

$$MSE = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2$$

### 5.2 均方根误差（RMSE）

$$RMSE = \sqrt{MSE} = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2}$$

### 5.3 平均绝对误差（MAE）

$$MAE = \frac{1}{m} \sum_{i=1}^{m} |y^{(i)} - \hat{y}^{(i)}|$$

### 5.4 R²分数（决定系数）

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^{m}(y^{(i)} - \bar{y})^2}$$

其中：
- $SS_{res}$ 是残差平方和
- $SS_{tot}$ 是总平方和
- $\bar{y}$ 是真实值的均值

$R^2$ 的取值范围是 $(-\infty, 1]$，越接近1越好。

## 6. 假设条件

线性回归有以下假设：
1. **线性关系**：特征和目标变量之间存在线性关系
2. **独立性**：样本之间相互独立
3. **同方差性**：误差的方差是常数
4. **正态性**：误差服从正态分布
5. **无多重共线性**：特征之间不高度相关

## 7. 优缺点总结

### 优点
- **简单易懂**：原理简单，易于理解和实现
- **计算快速**：训练和预测都很快
- **可解释性强**：参数有明确的物理意义
- **稳定**：不容易过拟合（有正则化时）

### 缺点
- **线性假设**：只能处理线性关系
- **对异常值敏感**：异常值会显著影响结果
- **特征相关**：多重共线性会导致问题
- **需要特征工程**：非线性关系需要特征变换

## 8. 应用场景

线性回归适用于：
- 特征与目标呈线性关系
- 需要快速预测
- 需要可解释性
- 数据量不是特别大

常见应用：
- 房价预测
- 销售预测
- 股票价格预测
- 温度预测

---

**下一节**：最小二乘法推导（详细数学证明）

