# 最大似然估计推导

## 1. 问题设定

给定训练集 $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})\}$，其中：
- $x^{(i)} \in \mathbb{R}^{n+1}$ 是特征向量
- $y^{(i)} \in \{0, 1\}$ 是标签

我们的目标是找到参数 $\theta$，使得模型能够最好地拟合数据。

---

## 2. 概率模型

### 2.1 伯努利分布

对于二分类问题，标签 $y$ 服从伯努利分布：
$$P(y|x; \theta) = h_\theta(x)^y (1 - h_\theta(x))^{1-y}$$

其中 $h_\theta(x) = \sigma(\theta^T x) = \frac{1}{1 + e^{-\theta^T x}}$。

**验证**：
- 当 $y=1$ 时：$P(y=1|x; \theta) = h_\theta(x)$
- 当 $y=0$ 时：$P(y=0|x; \theta) = 1 - h_\theta(x)$

---

## 3. 似然函数

### 3.1 单个样本的似然

对于样本 $(x^{(i)}, y^{(i)})$：
$$L^{(i)}(\theta) = P(y^{(i)}|x^{(i)}; \theta) = h_\theta(x^{(i)})^{y^{(i)}} (1 - h_\theta(x^{(i)}))^{1-y^{(i)}}$$

### 3.2 所有样本的似然

假设样本独立，联合概率为：
$$L(\theta) = \prod_{i=1}^{m} P(y^{(i)}|x^{(i)}; \theta) = \prod_{i=1}^{m} h_\theta(x^{(i)})^{y^{(i)}} (1 - h_\theta(x^{(i)}))^{1-y^{(i)}}$$

---

## 4. 对数似然函数

### 4.1 为什么取对数？

1. **数值稳定性**：避免连乘导致数值下溢
2. **计算简化**：将连乘转化为求和
3. **优化方便**：对数函数单调递增，不改变最优解

### 4.2 对数似然函数

$$\ell(\theta) = \log L(\theta) = \sum_{i=1}^{m} \log P(y^{(i)}|x^{(i)}; \theta)$$

展开：
$$\ell(\theta) = \sum_{i=1}^{m} [y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)}))]$$

---

## 5. 最大似然估计

### 5.1 目标函数

最大化对数似然：
$$\hat{\theta}_{MLE} = \arg\max_\theta \ell(\theta) = \arg\max_\theta \sum_{i=1}^{m} [y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)}))]$$

### 5.2 等价于最小化负对数似然

由于最大化 $\ell(\theta)$ 等价于最小化 $-\ell(\theta)$：
$$\hat{\theta}_{MLE} = \arg\min_\theta -\sum_{i=1}^{m} [y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)}))]$$

**这就是交叉熵损失函数！**

---

## 6. 梯度计算

### 6.1 对单个样本的梯度

$$\frac{\partial \ell^{(i)}}{\partial \theta_j} = \frac{\partial}{\partial \theta_j} [y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)}))]$$

使用链式法则：
$$\frac{\partial \ell^{(i)}}{\partial \theta_j} = \frac{y^{(i)}}{h_\theta(x^{(i)})} \cdot \frac{\partial h_\theta(x^{(i)})}{\partial \theta_j} - \frac{1-y^{(i)}}{1-h_\theta(x^{(i)})} \cdot \frac{\partial h_\theta(x^{(i)})}{\partial \theta_j}$$

由于 $h_\theta(x) = \sigma(\theta^T x)$，且 $\sigma'(z) = \sigma(z)(1-\sigma(z))$：
$$\frac{\partial h_\theta(x^{(i)})}{\partial \theta_j} = h_\theta(x^{(i)})(1-h_\theta(x^{(i)})) \cdot x_j^{(i)}$$

代入：
$$\frac{\partial \ell^{(i)}}{\partial \theta_j} = \left[\frac{y^{(i)}}{h_\theta(x^{(i)})} - \frac{1-y^{(i)}}{1-h_\theta(x^{(i)})}\right] \cdot h_\theta(x^{(i)})(1-h_\theta(x^{(i)})) \cdot x_j^{(i)}$$

化简：
$$\frac{\partial \ell^{(i)}}{\partial \theta_j} = [y^{(i)}(1-h_\theta(x^{(i)})) - (1-y^{(i)})h_\theta(x^{(i)})] \cdot x_j^{(i)}$$

进一步化简：
$$\frac{\partial \ell^{(i)}}{\partial \theta_j} = (y^{(i)} - h_\theta(x^{(i)})) \cdot x_j^{(i)}$$

### 6.2 对所有样本的梯度

$$\frac{\partial \ell}{\partial \theta_j} = \sum_{i=1}^{m} (y^{(i)} - h_\theta(x^{(i)})) \cdot x_j^{(i)}$$

**向量形式**：
$$\nabla_\theta \ell = X^T (y - h_\theta(X))$$

其中：
- $X$ 是 $m \times (n+1)$ 的特征矩阵
- $y$ 是 $m \times 1$ 的标签向量
- $h_\theta(X)$ 是 $m \times 1$ 的预测向量

---

## 7. 优化

### 7.1 梯度上升

最大化对数似然，使用梯度上升：
$$\theta_j := \theta_j + \alpha \frac{\partial \ell}{\partial \theta_j}$$

其中 $\alpha$ 是学习率。

### 7.2 等价于梯度下降

如果定义损失函数为负对数似然：
$$J(\theta) = -\ell(\theta) = -\sum_{i=1}^{m} [y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)}))]$$

则梯度为：
$$\frac{\partial J}{\partial \theta_j} = -\frac{\partial \ell}{\partial \theta_j} = -\sum_{i=1}^{m} (y^{(i)} - h_\theta(x^{(i)})) \cdot x_j^{(i)}$$

使用梯度下降：
$$\theta_j := \theta_j - \alpha \frac{\partial J}{\partial \theta_j} = \theta_j + \alpha \sum_{i=1}^{m} (y^{(i)} - h_\theta(x^{(i)})) \cdot x_j^{(i)}$$

这与梯度上升等价！

---

## 8. 总结

最大似然估计的完整流程：

1. **建立概率模型**：$P(y|x; \theta) = h_\theta(x)^y (1 - h_\theta(x))^{1-y}$
2. **构建似然函数**：$L(\theta) = \prod_{i=1}^{m} P(y^{(i)}|x^{(i)}; \theta)$
3. **取对数**：$\ell(\theta) = \log L(\theta)$
4. **最大化对数似然**：$\hat{\theta} = \arg\max_\theta \ell(\theta)$
5. **计算梯度**：$\nabla_\theta \ell = X^T (y - h_\theta(X))$
6. **优化求解**：使用梯度上升或梯度下降

**关键洞察**：
- 最大似然估计自然导出交叉熵损失
- 梯度形式简单：$(y - h_\theta(x)) \cdot x$
- 这解释了为什么逻辑回归使用这种损失函数

