{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 从零实现逻辑回归\n",
        "\n",
        "## 学习目标\n",
        "\n",
        "通过本notebook，你将学会：\n",
        "- 理解逻辑回归的基本原理和Sigmoid函数\n",
        "- 从零实现逻辑回归算法（包括梯度下降优化）\n",
        "- 理解交叉熵损失函数和最大似然估计\n",
        "- 掌握模型评估方法（准确率、ROC曲线）\n",
        "- 可视化决策边界和训练过程\n",
        "\n",
        "## 课程概述\n",
        "\n",
        "本notebook将带你从零开始实现逻辑回归算法，包括：\n",
        "1. **Sigmoid函数**：将线性输出转换为概率\n",
        "2. **交叉熵损失函数**：用于二分类问题的损失函数\n",
        "3. **梯度下降优化**：迭代优化参数\n",
        "4. **正则化**：L1和L2正则化防止过拟合\n",
        "\n",
        "我们将通过可视化和实验来理解逻辑回归的工作原理。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 环境准备\n",
        "\n",
        "### 版本要求\n",
        "- Python >= 3.7\n",
        "- NumPy >= 1.19.0\n",
        "- Matplotlib >= 3.3.0\n",
        "- scikit-learn >= 0.24.0\n",
        "\n",
        "### 导入库\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "# 设置中文字体（如果需要显示中文）\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 设置随机种子，确保结果可复现\n",
        "np.random.seed(42)\n",
        "\n",
        "# 设置matplotlib在notebook中内联显示\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"环境准备完成！\")\n",
        "print(f\"NumPy版本: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 理论回顾\n",
        "\n",
        "### 逻辑回归的基本原理\n",
        "\n",
        "逻辑回归是用于**二分类**问题的线性模型，它通过Sigmoid函数将线性输出转换为概率。\n",
        "\n",
        "**数学形式**：\n",
        "$$h_\\theta(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}$$\n",
        "\n",
        "其中：\n",
        "- $\\sigma(z)$ 是Sigmoid函数\n",
        "- $\\theta$ 是模型参数（权重和偏置）\n",
        "- $x$ 是输入特征\n",
        "\n",
        "### 损失函数：交叉熵\n",
        "\n",
        "逻辑回归使用**交叉熵损失函数**（Cross-Entropy Loss）：\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} [y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))]$$\n",
        "\n",
        "这个损失函数来自**最大似然估计**（MLE），目标是最大化数据的似然概率。\n",
        "\n",
        "### 梯度下降\n",
        "\n",
        "梯度更新公式：\n",
        "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}$$\n",
        "\n",
        "其中：\n",
        "$$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 实现LogisticRegression类\n",
        "\n",
        "现在让我们从零实现逻辑回归算法。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"逻辑回归类（从零实现）\"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6, \n",
        "                 regularization=None, lambda_reg=0.1):\n",
        "        \"\"\"\n",
        "        初始化逻辑回归模型\n",
        "        \n",
        "        参数:\n",
        "        - learning_rate: 学习率（默认0.01）\n",
        "        - max_iter: 最大迭代次数（默认1000）\n",
        "        - tol: 收敛容差（默认1e-6）\n",
        "        - regularization: 正则化类型，'l1'、'l2'或None（默认None）\n",
        "        - lambda_reg: 正则化系数（默认0.1）\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.regularization = regularization\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.theta = None  # 模型参数（权重和偏置）\n",
        "        self.cost_history = []  # 记录每次迭代的损失值\n",
        "    \n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Sigmoid函数：将任意实数映射到(0, 1)区间\n",
        "        \n",
        "        参数:\n",
        "        - z: 输入值（可以是标量、向量或矩阵）\n",
        "        \n",
        "        返回:\n",
        "        - Sigmoid函数值\n",
        "        \"\"\"\n",
        "        # 防止数值溢出：将z限制在合理范围内\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    def compute_cost(self, X, y):\n",
        "        \"\"\"\n",
        "        计算损失函数（交叉熵）\n",
        "        \n",
        "        参数:\n",
        "        - X: 特征矩阵，形状为(m, n+1)，已包含偏置项\n",
        "        - y: 标签向量，形状为(m,)\n",
        "        \n",
        "        返回:\n",
        "        - 损失值（标量）\n",
        "        \"\"\"\n",
        "        m = X.shape[0]  # 样本数量\n",
        "        h = self.sigmoid(X @ self.theta)  # 预测概率\n",
        "        \n",
        "        # 交叉熵损失\n",
        "        # 添加小常数1e-15避免log(0)\n",
        "        cost = -(1/m) * np.sum(y * np.log(h + 1e-15) + (1-y) * np.log(1-h + 1e-15))\n",
        "        \n",
        "        # 正则化项\n",
        "        if self.regularization == 'l2':\n",
        "            # L2正则化：惩罚参数平方和\n",
        "            cost += (self.lambda_reg / (2*m)) * np.sum(self.theta[1:]**2)\n",
        "        elif self.regularization == 'l1':\n",
        "            # L1正则化：惩罚参数绝对值之和\n",
        "            cost += (self.lambda_reg / m) * np.sum(np.abs(self.theta[1:]))\n",
        "        \n",
        "        return cost\n",
        "    \n",
        "    def compute_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        计算梯度\n",
        "        \n",
        "        参数:\n",
        "        - X: 特征矩阵，形状为(m, n+1)\n",
        "        - y: 标签向量，形状为(m,)\n",
        "        \n",
        "        返回:\n",
        "        - 梯度向量，形状为(n+1,)\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        h = self.sigmoid(X @ self.theta)  # 预测概率\n",
        "        \n",
        "        # 基本梯度：交叉熵损失对参数的导数\n",
        "        gradient = (1/m) * X.T @ (h - y)\n",
        "        \n",
        "        # 正则化项的梯度\n",
        "        if self.regularization == 'l2':\n",
        "            # L2正则化的梯度：lambda * theta\n",
        "            gradient[1:] += (self.lambda_reg / m) * self.theta[1:]\n",
        "        elif self.regularization == 'l1':\n",
        "            # L1正则化的次梯度：lambda * sign(theta)\n",
        "            gradient[1:] += (self.lambda_reg / m) * np.sign(self.theta[1:])\n",
        "        \n",
        "        return gradient\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        训练模型：使用梯度下降优化参数\n",
        "        \n",
        "        参数:\n",
        "        - X: 特征矩阵，形状为(m, n)\n",
        "        - y: 标签向量，形状为(m,)\n",
        "        \n",
        "        返回:\n",
        "        - self: 返回自身，支持链式调用\n",
        "        \"\"\"\n",
        "        # 添加偏置项：在特征矩阵前加一列1\n",
        "        m, n = X.shape\n",
        "        X_with_bias = np.hstack([np.ones((m, 1)), X])\n",
        "        \n",
        "        # 初始化参数：全零初始化\n",
        "        self.theta = np.zeros(n + 1)\n",
        "        \n",
        "        # 梯度下降迭代\n",
        "        for i in range(self.max_iter):\n",
        "            # 计算当前损失\n",
        "            cost = self.compute_cost(X_with_bias, y)\n",
        "            self.cost_history.append(cost)\n",
        "            \n",
        "            # 计算梯度\n",
        "            gradient = self.compute_gradient(X_with_bias, y)\n",
        "            \n",
        "            # 更新参数：沿着梯度反方向更新\n",
        "            self.theta -= self.learning_rate * gradient\n",
        "            \n",
        "            # 检查收敛：如果损失变化小于容差，提前停止\n",
        "            if i > 0 and abs(self.cost_history[-2] - self.cost_history[-1]) < self.tol:\n",
        "                print(f\"在第 {i+1} 次迭代后收敛\")\n",
        "                break\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        预测概率：返回每个样本属于正类的概率\n",
        "        \n",
        "        参数:\n",
        "        - X: 特征矩阵，形状为(m, n)\n",
        "        \n",
        "        返回:\n",
        "        - 概率向量，形状为(m,)，每个值在[0, 1]之间\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        X_with_bias = np.hstack([np.ones((m, 1)), X])\n",
        "        return self.sigmoid(X_with_bias @ self.theta)\n",
        "    \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        预测类别：根据概率和阈值预测类别\n",
        "        \n",
        "        参数:\n",
        "        - X: 特征矩阵，形状为(m, n)\n",
        "        - threshold: 决策阈值（默认0.5）\n",
        "        \n",
        "        返回:\n",
        "        - 预测标签，形状为(m,)，值为0或1\n",
        "        \"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return (probabilities >= threshold).astype(int)\n",
        "    \n",
        "    def plot_cost_history(self):\n",
        "        \"\"\"绘制损失函数收敛曲线\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(self.cost_history)\n",
        "        plt.xlabel('迭代次数')\n",
        "        plt.ylabel('损失值')\n",
        "        plt.title('损失函数收敛曲线')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "print(\"LogisticRegression类定义完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 生成二分类数据\n",
        "# n_samples: 样本数量\n",
        "# n_features: 特征数量\n",
        "# n_redundant: 冗余特征数量（与信息特征线性相关）\n",
        "# n_informative: 信息特征数量（对分类有用的特征）\n",
        "# n_clusters_per_class: 每个类别的聚类数量\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
        "                           n_informative=2, n_clusters_per_class=1, \n",
        "                           random_state=42)\n",
        "\n",
        "# 划分训练集和测试集\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"训练集大小: {X_train.shape[0]}\")\n",
        "print(f\"测试集大小: {X_test.shape[0]}\")\n",
        "print(f\"特征维度: {X_train.shape[1]}\")\n",
        "print(f\"\\n类别分布:\")\n",
        "print(f\"  类别0: {np.sum(y_train == 0)} 个样本\")\n",
        "print(f\"  类别1: {np.sum(y_train == 1)} 个样本\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 模型训练\n",
        "\n",
        "使用梯度下降训练逻辑回归模型。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建模型实例\n",
        "# learning_rate: 学习率，控制参数更新步长\n",
        "# max_iter: 最大迭代次数\n",
        "# regularization: 正则化类型，'l2'表示L2正则化\n",
        "# lambda_reg: 正则化系数，控制正则化强度\n",
        "model = LogisticRegression(learning_rate=0.1, max_iter=1000, \n",
        "                          regularization='l2', lambda_reg=0.1)\n",
        "\n",
        "# 训练模型\n",
        "print(\"开始训练模型...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"模型训练完成！\")\n",
        "\n",
        "# 查看训练后的参数\n",
        "print(f\"\\n模型参数:\")\n",
        "print(f\"  偏置项 (theta[0]): {model.theta[0]:.4f}\")\n",
        "print(f\"  权重 (theta[1:]): {model.theta[1:]}\")\n",
        "print(f\"  最终损失: {model.cost_history[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 模型评估\n",
        "\n",
        "评估模型在测试集上的性能。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 预测测试集\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# 计算准确率\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"测试集准确率: {accuracy:.4f}\")\n",
        "\n",
        "# 计算ROC曲线和AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# 显示前10个样本的预测结果\n",
        "print(\"\\n前10个样本的预测结果:\")\n",
        "print(\"真实标签 | 预测标签 | 预测概率\")\n",
        "print(\"-\" * 35)\n",
        "for i in range(min(10, len(y_test))):\n",
        "    print(f\"   {y_test[i]}     |    {y_pred[i]}     |  {y_proba[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 可视化结果\n",
        "\n",
        "可视化损失函数收敛曲线、决策边界和ROC曲线。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建图形\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 1. 损失函数收敛曲线\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(model.cost_history)\n",
        "plt.xlabel('迭代次数')\n",
        "plt.ylabel('损失值')\n",
        "plt.title('损失函数收敛曲线')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. 决策边界\n",
        "plt.subplot(1, 3, 2)\n",
        "# 创建网格点用于绘制决策边界\n",
        "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# 预测网格点的概率\n",
        "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# 绘制决策边界和概率等高线\n",
        "plt.contourf(xx, yy, Z, levels=50, alpha=0.5, cmap='RdYlBu')\n",
        "plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2, linestyles='--')\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', \n",
        "           edgecolors='black', s=30, alpha=0.7)\n",
        "plt.xlabel('特征1')\n",
        "plt.ylabel('特征2')\n",
        "plt.title('决策边界（训练集）')\n",
        "plt.colorbar(label='预测概率')\n",
        "\n",
        "# 3. ROC曲线\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(fpr, tpr, label=f'ROC曲线 (AUC = {roc_auc:.2f})', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='随机分类器')\n",
        "plt.xlabel('假正例率 (FPR)')\n",
        "plt.ylabel('真正例率 (TPR)')\n",
        "plt.title('ROC曲线')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 参数实验\n",
        "\n",
        "尝试不同的学习率和正则化参数，观察对模型性能的影响。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 实验不同的学习率\n",
        "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model_exp = LogisticRegression(learning_rate=lr, max_iter=1000, \n",
        "                                   regularization='l2', lambda_reg=0.1)\n",
        "    model_exp.fit(X_train, y_train)\n",
        "    y_pred_exp = model_exp.predict(X_test)\n",
        "    accuracy_exp = accuracy_score(y_test, y_pred_exp)\n",
        "    results.append({\n",
        "        'learning_rate': lr,\n",
        "        'accuracy': accuracy_exp,\n",
        "        'final_cost': model_exp.cost_history[-1],\n",
        "        'iterations': len(model_exp.cost_history)\n",
        "    })\n",
        "    print(f\"学习率 {lr:6.3f}: 准确率={accuracy_exp:.4f}, \"\n",
        "          f\"最终损失={model_exp.cost_history[-1]:.4f}, \"\n",
        "          f\"迭代次数={len(model_exp.cost_history)}\")\n",
        "\n",
        "# 可视化结果\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# 准确率对比\n",
        "axes[0].bar(range(len(results)), [r['accuracy'] for r in results])\n",
        "axes[0].set_xticks(range(len(results)))\n",
        "axes[0].set_xticklabels([f\"{r['learning_rate']}\" for r in results])\n",
        "axes[0].set_xlabel('学习率')\n",
        "axes[0].set_ylabel('准确率')\n",
        "axes[0].set_title('不同学习率对准确率的影响')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 最终损失对比\n",
        "axes[1].bar(range(len(results)), [r['final_cost'] for r in results])\n",
        "axes[1].set_xticks(range(len(results)))\n",
        "axes[1].set_xticklabels([f\"{r['learning_rate']}\" for r in results])\n",
        "axes[1].set_xlabel('学习率')\n",
        "axes[1].set_ylabel('最终损失')\n",
        "axes[1].set_title('不同学习率对最终损失的影响')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 总结与思考\n",
        "\n",
        "### 关键知识点总结\n",
        "\n",
        "1. **Sigmoid函数**：将线性输出转换为概率，值域为(0, 1)\n",
        "2. **交叉熵损失**：用于二分类问题的损失函数，来自最大似然估计\n",
        "3. **梯度下降**：迭代优化参数，需要合适的学习率\n",
        "4. **正则化**：L1和L2正则化可以防止过拟合\n",
        "\n",
        "### 思考问题\n",
        "\n",
        "1. **为什么逻辑回归使用交叉熵而不是均方误差？**\n",
        "   - 提示：考虑概率解释和梯度性质\n",
        "\n",
        "2. **学习率对训练有什么影响？**\n",
        "   - 提示：观察不同学习率下的收敛曲线\n",
        "\n",
        "3. **决策边界是如何形成的？**\n",
        "   - 提示：思考Sigmoid函数和阈值的关系\n",
        "\n",
        "4. **正则化如何影响模型？**\n",
        "   - 提示：尝试不同的lambda_reg值，观察参数和性能变化\n",
        "\n",
        "### 下一步学习\n",
        "\n",
        "- 学习多分类逻辑回归（Softmax）\n",
        "- 学习使用scikit-learn实现逻辑回归\n",
        "- 学习处理类别不平衡问题\n",
        "- 学习特征工程和特征选择\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
