# 基础练习1：从零实现逻辑回归

## 练习目标

不使用任何机器学习库（如scikit-learn），从零实现逻辑回归算法，包括：
1. Sigmoid函数
2. 交叉熵损失函数
3. 梯度计算
4. 梯度下降优化
5. 预测功能

## 练习要求

### 1. 实现LogisticRegression类

创建一个`LogisticRegression`类，包含以下方法：

- `__init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6)`
  - 初始化学习率、最大迭代次数、收敛容差
  
- `sigmoid(self, z)`
  - 实现Sigmoid函数：$\sigma(z) = \frac{1}{1 + e^{-z}}$
  - 注意处理数值溢出问题
  
- `compute_cost(self, X, y)`
  - 计算交叉熵损失函数
  - 公式：$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m} [y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)}))]$
  
- `compute_gradient(self, X, y)`
  - 计算梯度
  - 公式：$\frac{\partial J}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$
  
- `fit(self, X, y)`
  - 训练模型，使用梯度下降优化参数
  - 记录每次迭代的损失值
  - 当损失变化小于容差时停止
  
- `predict_proba(self, X)`
  - 预测概率：返回 $P(y=1|x)$
  
- `predict(self, X, threshold=0.5)`
  - 预测类别：如果概率 >= threshold，预测为1，否则为0

### 2. 测试要求

1. 使用`sklearn.datasets.make_classification`生成二分类数据
2. 划分训练集和测试集（80/20）
3. 训练模型
4. 在测试集上评估准确率
5. 绘制损失函数收敛曲线
6. 可视化决策边界

### 3. 代码要求

- 代码清晰，有注释
- 处理数值稳定性问题（如Sigmoid函数溢出）
- 正确实现梯度下降
- 可视化结果美观

## 提示

1. **Sigmoid函数溢出**：
   ```python
   # 防止溢出
   z = np.clip(z, -500, 500)
   ```

2. **对数计算**：
   ```python
   # 避免log(0)
   np.log(h + 1e-15)
   ```

3. **添加偏置项**：
   ```python
   X_with_bias = np.hstack([np.ones((m, 1)), X])
   ```

4. **决策边界可视化**：
   - 创建网格点
   - 预测每个网格点的概率
   - 使用contourf绘制概率分布
   - 使用contour绘制决策边界（概率=0.5）

## 评估标准

- **正确性**（40分）：算法实现正确，能正确分类
- **代码质量**（20分）：代码清晰，有注释，结构合理
- **数值稳定性**（20分）：处理溢出和除零问题
- **可视化**（20分）：损失曲线和决策边界清晰美观

## 预期结果

- 测试集准确率应该 > 0.85（对于简单的二分类数据）
- 损失函数应该单调递减并收敛
- 决策边界应该是一条直线（线性决策边界）

---

**完成后，请将代码保存为 `练习1_答案.py`，并放在 `答案/` 文件夹中。**

