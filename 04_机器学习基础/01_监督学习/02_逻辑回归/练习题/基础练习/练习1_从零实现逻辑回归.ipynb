{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 基础练习1：从零实现逻辑回归\n",
        "\n",
        "## 练习目标\n",
        "\n",
        "不使用任何机器学习库（如scikit-learn），从零实现逻辑回归算法，包括：\n",
        "1. Sigmoid函数\n",
        "2. 交叉熵损失函数\n",
        "3. 梯度计算\n",
        "4. 梯度下降优化\n",
        "5. 预测功能\n",
        "\n",
        "## 练习要求\n",
        "\n",
        "### 1. 实现LogisticRegression类\n",
        "\n",
        "创建一个`LogisticRegression`类，包含以下方法：\n",
        "- `__init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6)`\n",
        "- `sigmoid(self, z)` - 实现Sigmoid函数\n",
        "- `compute_cost(self, X, y)` - 计算交叉熵损失\n",
        "- `compute_gradient(self, X, y)` - 计算梯度\n",
        "- `fit(self, X, y)` - 训练模型，使用梯度下降优化参数\n",
        "- `predict_proba(self, X)` - 预测概率\n",
        "- `predict(self, X, threshold=0.5)` - 预测类别\n",
        "\n",
        "### 2. 核心算法\n",
        "\n",
        "**Sigmoid函数**：$\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
        "\n",
        "**交叉熵损失**：$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} [y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))]$\n",
        "\n",
        "**梯度**：$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$\n",
        "\n",
        "**参数更新**：$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第一步：导入库\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 设置随机种子\n",
        "np.random.seed(42)\n",
        "\n",
        "# 设置matplotlib在notebook中内联显示\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"环境准备完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第二步：实现LogisticRegression类\n",
        "\n",
        "**你的任务**：实现LogisticRegression类，使用梯度下降优化参数。\n",
        "\n",
        "**提示**：\n",
        "1. Sigmoid函数需要处理数值溢出（使用np.clip限制z的范围）\n",
        "2. 计算损失时避免log(0)，添加小常数（如1e-15）\n",
        "3. 在fit方法中实现梯度下降迭代\n",
        "4. 记录损失函数历史，用于可视化\n",
        "5. 添加偏置项：在特征矩阵前加一列1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"从零实现的逻辑回归类\"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6):\n",
        "        \"\"\"\n",
        "        初始化模型\n",
        "        \n",
        "        参数:\n",
        "        - learning_rate: 学习率，控制每次更新的步长\n",
        "        - max_iter: 最大迭代次数\n",
        "        - tol: 收敛容差，当损失变化小于此值时停止\n",
        "        \"\"\"\n",
        "        # TODO: 初始化参数\n",
        "        # 提示：保存learning_rate、max_iter、tol\n",
        "        # 初始化theta（参数）、cost_history（损失历史）\n",
        "        \n",
        "        # 你的代码：\n",
        "        pass\n",
        "    \n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Sigmoid函数：将任意实数映射到(0, 1)区间\n",
        "        \n",
        "        参数:\n",
        "        - z: 输入值\n",
        "        \n",
        "        返回:\n",
        "        - Sigmoid函数值\n",
        "        \"\"\"\n",
        "        # TODO: 实现Sigmoid函数\n",
        "        # 提示：使用np.clip防止数值溢出（限制z在-500到500之间）\n",
        "        \n",
        "        # 你的代码：\n",
        "        pass\n",
        "    \n",
        "    def compute_cost(self, X, y):\n",
        "        \"\"\"\n",
        "        计算交叉熵损失函数\n",
        "        \n",
        "        参数:\n",
        "        - X: 特征矩阵，形状为(m, n+1)，已包含偏置项\n",
        "        - y: 标签向量，形状为(m,)\n",
        "        \n",
        "        返回:\n",
        "        - 损失值（标量）\n",
        "        \"\"\"\n",
        "        # TODO: 计算交叉熵损失\n",
        "        # 提示：\n",
        "        # 1. 计算预测概率 h = sigmoid(X @ theta)\n",
        "        # 2. 计算损失：-(1/m) * sum(y*log(h) + (1-y)*log(1-h))\n",
        "        # 3. 添加小常数（1e-15）避免log(0)\n",
        "        \n",
        "        # 你的代码：\n",
        "        pass\n",
        "    \n",
        "    def compute_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        计算梯度\n",
        "        \n",
        "        参数:\n",
        "        - X: 特征矩阵，形状为(m, n+1)\n",
        "        - y: 标签向量，形状为(m,)\n",
        "        \n",
        "        返回:\n",
        "        - 梯度向量，形状为(n+1,)\n",
        "        \"\"\"\n",
        "        # TODO: 计算梯度\n",
        "        # 提示：gradient = (1/m) * X.T @ (h - y)\n",
        "        \n",
        "        # 你的代码：\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        训练模型，使用梯度下降优化参数\n",
        "        \n",
        "        参数:\n",
        "        - X: 特征矩阵，形状为(m, n)\n",
        "        - y: 标签向量，形状为(m,)\n",
        "        \n",
        "        返回:\n",
        "        - self: 返回自身，支持链式调用\n",
        "        \"\"\"\n",
        "        # TODO: 实现训练过程\n",
        "        # 提示：\n",
        "        # 1. 添加偏置项：X_with_bias = np.hstack([np.ones((m, 1)), X])\n",
        "        # 2. 初始化参数：theta = np.zeros(n + 1)\n",
        "        # 3. 梯度下降迭代：\n",
        "        #    - 计算损失和梯度\n",
        "        #    - 更新参数：theta -= learning_rate * gradient\n",
        "        #    - 检查收敛\n",
        "        \n",
        "        # 你的代码：\n",
        "        pass\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        预测概率：返回每个样本属于正类的概率\n",
        "        \n",
        "        参数:\n",
        "        - X: 特征矩阵，形状为(m, n)\n",
        "        \n",
        "        返回:\n",
        "        - 概率向量，形状为(m,)\n",
        "        \"\"\"\n",
        "        # TODO: 实现概率预测\n",
        "        # 提示：\n",
        "        # 1. 添加偏置项\n",
        "        # 2. 计算 sigmoid(X @ theta)\n",
        "        \n",
        "        # 你的代码：\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        预测类别：根据概率和阈值预测类别\n",
        "        \n",
        "        参数:\n",
        "        - X: 特征矩阵，形状为(m, n)\n",
        "        - threshold: 决策阈值（默认0.5）\n",
        "        \n",
        "        返回:\n",
        "        - 预测标签，形状为(m,)，值为0或1\n",
        "        \"\"\"\n",
        "        # TODO: 实现类别预测\n",
        "        # 提示：如果概率 >= threshold，预测为1，否则为0\n",
        "        \n",
        "        # 你的代码：\n",
        "        pass\n",
        "\n",
        "print(\"LogisticRegression类框架已创建，请填写TODO部分！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第三步：测试你的实现\n",
        "\n",
        "使用以下代码测试你的实现。如果实现正确，应该能够：\n",
        "1. 成功训练模型\n",
        "2. 在测试集上获得较高的准确率（>0.8）\n",
        "3. 看到损失函数收敛曲线\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 生成测试数据\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
        "                           n_informative=2, n_clusters_per_class=1, \n",
        "                           random_state=42)\n",
        "\n",
        "# 划分训练集和测试集\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"训练集大小: {X_train.shape[0]}\")\n",
        "print(f\"测试集大小: {X_test.shape[0]}\")\n",
        "\n",
        "# 创建和训练模型\n",
        "model = LogisticRegression(learning_rate=0.1, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 预测\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 评估\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\n测试集准确率: {accuracy:.4f}\")\n",
        "\n",
        "# 绘制损失函数收敛曲线\n",
        "if len(model.cost_history) > 0:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(model.cost_history)\n",
        "    plt.xlabel('迭代次数')\n",
        "    plt.ylabel('损失值')\n",
        "    plt.title('损失函数收敛曲线')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"警告：cost_history为空，请检查fit方法的实现！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第四步：可视化决策边界（可选）\n",
        "\n",
        "如果你完成了前面的步骤，可以尝试可视化决策边界。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: 可视化决策边界\n",
        "# 提示：\n",
        "# 1. 创建网格点\n",
        "# 2. 预测网格点的概率\n",
        "# 3. 使用contourf绘制决策边界\n",
        "# 4. 使用scatter绘制数据点\n",
        "\n",
        "# 你的代码：\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "完成本练习后，你应该：\n",
        "- ✅ 理解Sigmoid函数的作用和实现\n",
        "- ✅ 理解交叉熵损失函数的计算\n",
        "- ✅ 掌握梯度下降优化过程\n",
        "- ✅ 能够从零实现逻辑回归算法\n",
        "\n",
        "### 思考问题\n",
        "\n",
        "1. 为什么Sigmoid函数需要处理数值溢出？\n",
        "2. 交叉熵损失函数与均方误差损失有什么区别？\n",
        "3. 学习率对训练过程有什么影响？尝试不同的学习率观察效果。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
