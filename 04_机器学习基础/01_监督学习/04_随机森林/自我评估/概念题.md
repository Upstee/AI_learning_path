# 随机森林 - 概念题

## 一、选择题（每题2分，共20分）

### 1. 随机森林使用什么集成方法？
A. Boosting  
B. Bagging  
C. Stacking  
D. Blending  

**答案**：B

---

### 2. 随机森林中每棵树使用什么数据？
A. 全部数据  
B. 随机采样的数据（Bootstrap）  
C. 固定数据  
D. 验证集  

**答案**：B

---

### 3. 随机森林中每棵树使用什么特征？
A. 全部特征  
B. 随机选择的特征  
C. 固定特征  
D. 重要特征  

**答案**：B

---

### 4. 随机森林的最终预测如何得到？
A. 第一棵树的预测  
B. 所有树的平均（回归）或投票（分类）  
C. 投票  
D. B和C  

**答案**：D

---

### 5. 随机森林相比单棵决策树的优势不包括？
A. 更准确  
B. 更稳定  
C. 更简单  
D. 更不容易过拟合  

**答案**：C

---

### 6. Bootstrap采样中，OOB样本的比例约为？
A. 25%  
B. 36.8%  
C. 50%  
D. 63.2%  

**答案**：B

---

### 7. 随机森林中，分类任务通常使用多少个特征？
A. p（全部特征）  
B. √p  
C. p/3  
D. log₂(p)  

**答案**：B

---

### 8. OOB误差的作用？
A. 评估模型性能，无需验证集  
B. 选择特征  
C. 剪枝  
D. 以上都不对  

**答案**：A

---

### 9. 随机森林的主要缺点？
A. 计算成本高  
B. 内存占用大  
C. 可解释性不如单棵决策树  
D. 以上都是  

**答案**：D

---

### 10. 随机森林中，增加树的数量通常？
A. 总是提高性能  
B. 可能提高性能，但计算成本增加  
C. 降低性能  
D. 没有影响  

**答案**：B

---

## 二、简答题（每题10分，共40分）

### 1. 解释随机森林为什么比单棵决策树更好。

**参考答案**：
- **减少过拟合**：通过集成和随机性减少过拟合
- **提高准确率**：多个模型的集体决策更准确
- **更稳定**：对数据变化不敏感
- **降低方差**：多个模型的平均减少预测方差

---

### 2. 说明Bootstrap采样和特征随机选择的作用。

**参考答案**：
- **Bootstrap采样**：增加数据多样性，每个树使用不同的训练数据
- **特征随机选择**：增加特征多样性，降低树之间的相关性
- **共同作用**：通过随机性增加模型的多样性，提高泛化能力

---

### 3. 解释OOB误差的原理和优势。

**参考答案**：
- **原理**：每个树使用Bootstrap采样，约36.8%的样本未被使用，这些OOB样本可以用于验证
- **优势**：无需单独划分验证集，充分利用数据，评估相对无偏

---

### 4. 说明随机森林的特征重要性如何计算。

**参考答案**：
- **基于不纯度减少**：计算每个特征在所有树中减少的不纯度，归一化后得到重要性
- **基于排列重要性**：随机打乱特征值，观察模型性能下降，性能下降越大特征越重要

---

## 三、计算题（每题10分，共20分）

### 1. 给定数据集有100个样本，计算Bootstrap采样中：
- 每个样本被选中的概率
- OOB样本的期望比例

**参考答案**：
- 每个样本被选中的概率：1 - (1-1/100)^100 ≈ 1 - 1/e ≈ 0.632
- OOB样本的期望比例：(1-1/100)^100 ≈ 1/e ≈ 0.368

---

### 2. 给定数据集有16个特征，计算：
- 分类任务中每次分割考虑的特征数（使用√p）
- 回归任务中每次分割考虑的特征数（使用p/3）

**参考答案**：
- 分类任务：√16 = 4个特征
- 回归任务：16/3 ≈ 5个特征（取整）

---

## 评分标准

- **选择题**：每题2分，共20分
- **简答题**：每题10分，共40分
- **计算题**：每题10分，共20分
- **总分**：80分
- **及格线**：64分（80%）

---

**完成后请对照答案检查，理解每道题的原理！**

