# 集成学习 - 概念题

## 一、选择题（每题2分，共20分）

### 1. 集成学习的三种主要方法是？
A. Bagging, Boosting, Stacking  
B. 决策树, 随机森林, SVM  
C. 线性, 非线性, 核方法  
D. 分类, 回归, 聚类  

**答案**：A

---

### 2. Bagging的主要思想是？
A. 顺序训练模型  
B. 并行训练模型  
C. 堆叠模型  
D. 融合模型  

**答案**：B

---

### 3. Boosting的主要思想是？
A. 并行训练  
B. 顺序训练，关注错误样本  
C. 随机采样  
D. 投票  

**答案**：B

---

### 4. 梯度提升树使用什么优化方法？
A. 最小二乘法  
B. 梯度下降  
C. 最大似然  
D. 信息增益  

**答案**：B

---

### 5. XGBoost相比GBDT的主要改进不包括？
A. 正则化  
B. 并行计算  
C. 缺失值处理  
D. 更简单的算法  

**答案**：D

---

### 6. Stacking的第二层是？
A. 基学习器  
B. 元学习器  
C. 特征选择器  
D. 数据采样器  

**答案**：B

---

### 7. Bagging主要减少？
A. 偏差  
B. 方差  
C. 噪声  
D. 过拟合  

**答案**：B

---

### 8. Boosting主要减少？
A. 偏差  
B. 方差  
C. 噪声  
D. 过拟合  

**答案**：A

---

### 9. 集成学习需要基学习器具有？
A. 高准确率  
B. 多样性  
C. 相同算法  
D. 相同参数  

**答案**：B

---

### 10. LightGBM相比XGBoost的主要优势？
A. 更准确  
B. 训练更快  
C. 更简单  
D. 更稳定  

**答案**：B

---

## 二、简答题（每题10分，共40分）

### 1. 解释集成学习为什么能提升性能。

**参考答案**：
- **减少方差**：多个模型的平均减少预测方差
- **减少偏差**：不同模型可能捕获不同的模式
- **错误抵消**：单个模型的错误可以被其他模型纠正
- **提高鲁棒性**：对数据的小扰动不敏感

---

### 2. 说明Bagging和Boosting的区别。

**参考答案**：
- **训练方式**：Bagging并行训练，Boosting顺序训练
- **关注点**：Bagging关注减少方差，Boosting关注减少偏差
- **样本权重**：Bagging样本权重相等，Boosting关注错误样本
- **适用场景**：Bagging适合高方差模型，Boosting适合高偏差模型

---

### 3. 解释Stacking的原理和优势。

**参考答案**：
- **原理**：第一层训练多个基学习器，第二层使用基学习器的预测作为特征训练元学习器
- **优势**：可以学习如何最好地组合基学习器，利用不同学习器的互补性
- **关键**：使用交叉验证生成元特征，避免过拟合

---

### 4. 说明为什么集成学习需要多样性。

**参考答案**：
- **错误不相关**：如果所有学习器都犯相同的错误，集成无法纠正
- **互补性**：不同的学习器可以互补，提高整体性能
- **鲁棒性**：多样性提高模型的鲁棒性
- **方差减少**：独立模型的平均，方差减少为$1/n$

---

## 三、计算题（每题10分，共20分）

### 1. 给定5个基学习器，每个准确率为60%，如果它们独立，多数投票的准确率是多少？

**参考答案**：
- 需要至少3个学习器预测正确
- 使用二项分布：P(X≥3) = C(5,3)×0.6³×0.4² + C(5,4)×0.6⁴×0.4¹ + C(5,5)×0.6⁵
- ≈ 0.3456 + 0.2592 + 0.0778 ≈ 0.6826
- 准确率约为68.26%

---

### 2. 给定10个基学习器，每个的方差为σ²，如果它们独立，集成的方差是多少？

**参考答案**：
- 如果基学习器独立，集成的方差 = σ²/n
- 其中n是学习器数量
- 集成的方差 = σ²/10 = 0.1σ²
- 方差减少了90%

---

## 评分标准

- **选择题**：每题2分，共20分
- **简答题**：每题10分，共40分
- **计算题**：每题10分，共20分
- **总分**：80分
- **及格线**：64分（80%）

---

**完成后请对照答案检查，理解每道题的原理！**

