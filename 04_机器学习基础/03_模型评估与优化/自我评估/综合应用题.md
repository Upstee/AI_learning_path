# 模型评估与优化 - 综合应用题

## 题目1：构建完整的模型评估与优化系统（50分）

### 场景描述

你是一家数据科学公司的机器学习工程师，需要为一个客户构建一个完整的模型评估与优化系统。客户提供的数据集是一个二分类问题，包含10000个样本和50个特征。

### 任务要求

1. **数据分析和预处理**（10分）
   - 分析数据特征（缺失值、分布、相关性等）
   - 进行必要的数据预处理
   - 划分训练集、验证集和测试集

2. **模型训练和评估**（15分）
   - 训练至少3个不同的分类模型
   - 使用多种评估指标评估模型性能
   - 进行交叉验证
   - 可视化评估结果

3. **模型优化**（15分）
   - 使用网格搜索或随机搜索进行超参数调优
   - 尝试特征选择或特征工程
   - 比较优化前后的模型性能

4. **系统设计和文档**（10分）
   - 设计清晰的代码结构
   - 编写完整的文档说明
   - 提供使用示例

### 评分标准

- **功能完整性**（30分）：所有功能正确实现
- **代码质量**（10分）：代码规范、结构清晰
- **优化效果**（5分）：模型性能有明显提升
- **文档完整性**（5分）：文档清晰、示例完整

### 参考答案思路

1. **数据预处理**
   ```python
   # 数据加载和分析
   # 处理缺失值
   # 特征标准化/归一化
   # 数据划分（60%训练，20%验证，20%测试）
   ```

2. **模型训练和评估**
   ```python
   # 训练多个模型（逻辑回归、随机森林、SVM等）
   # 计算多种评估指标
   # 5折交叉验证
   # 可视化结果
   ```

3. **模型优化**
   ```python
   # 网格搜索超参数
   # 特征重要性分析
   # 特征选择
   # 重新训练和评估
   ```

4. **系统设计**
   ```python
   # 模块化设计
   # 配置管理
   # 结果保存和报告生成
   ```

---

## 题目2：处理类别不平衡问题（30分）

### 场景描述

你遇到一个严重的类别不平衡问题：正类样本只有5%，负类样本占95%。需要设计一个评估和优化方案来处理这个问题。

### 任务要求

1. **问题分析**（5分）
   - 分析类别不平衡对评估指标的影响
   - 说明为什么准确率不适合这种情况

2. **评估指标选择**（10分）
   - 选择合适的评估指标（精确率、召回率、F1、ROC-AUC等）
   - 解释为什么选择这些指标
   - 实现这些指标的计算

3. **模型优化策略**（10分）
   - 使用类别权重平衡
   - 尝试过采样或欠采样
   - 使用SMOTE等高级技术（可选）

4. **结果分析**（5分）
   - 比较不同策略的效果
   - 分析各指标的权衡
   - 给出最佳方案建议

### 评分标准

- **问题理解**（10分）：正确理解类别不平衡问题
- **指标选择**（10分）：选择合适的评估指标并正确实现
- **优化策略**（8分）：尝试多种优化策略
- **结果分析**（2分）：分析深入、建议合理

### 参考答案思路

1. **问题分析**
   - 准确率会误导（即使全部预测为负类，准确率也有95%）
   - 需要关注少数类的识别能力

2. **评估指标**
   - 使用精确率、召回率、F1分数
   - 使用ROC-AUC（不受类别分布影响）
   - 使用PR曲线（精确率-召回率曲线）

3. **优化策略**
   ```python
   # 方法1：类别权重
   model = RandomForestClassifier(class_weight='balanced')
   
   # 方法2：过采样（SMOTE）
   from imblearn.over_sampling import SMOTE
   smote = SMOTE()
   X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
   
   # 方法3：欠采样
   from imblearn.under_sampling import RandomUnderSampler
   undersample = RandomUnderSampler()
   X_resampled, y_resampled = undersample.fit_resample(X_train, y_train)
   ```

4. **结果分析**
   - 比较不同方法的精确率、召回率、F1分数
   - 分析ROC-AUC和PR-AUC
   - 根据业务需求选择最佳方案

---

## 题目3：大规模数据的模型评估（20分）

### 场景描述

你需要评估一个在大规模数据集（100万样本）上训练的模型，但计算资源有限，无法进行完整的交叉验证。

### 任务要求

1. **评估策略设计**（10分）
   - 设计适合大规模数据的评估策略
   - 实现增量评估或采样评估
   - 说明策略的优缺点

2. **性能优化**（7分）
   - 优化评估过程的计算效率
   - 使用并行计算（可选）
   - 减少内存占用

3. **结果验证**（3分）
   - 验证评估结果的可靠性
   - 分析采样误差

### 评分标准

- **策略设计**（10分）：策略合理、可行
- **性能优化**（7分）：有明显性能提升
- **结果验证**（3分）：验证方法合理

### 参考答案思路

1. **评估策略**
   ```python
   # 方法1：分层采样
   from sklearn.model_selection import train_test_split
   X_sample, _, y_sample, _ = train_test_split(
       X, y, test_size=0.99, stratify=y, random_state=42
   )
   
   # 方法2：增量交叉验证
   # 每次只加载一部分数据进行验证
   
   # 方法3：时间序列分割（如果是时间序列数据）
   from sklearn.model_selection import TimeSeriesSplit
   ```

2. **性能优化**
   ```python
   # 使用n_jobs参数并行计算
   cross_val_score(model, X, y, cv=5, n_jobs=-1)
   
   # 使用更快的评估指标
   # 减少数据复制
   ```

3. **结果验证**
   ```python
   # 多次采样，计算置信区间
   # 比较不同采样大小的结果
   # 分析结果的稳定性
   ```

---

## 总评分标准

- **≥80分**：优秀，完全掌握综合应用
- **60-79分**：良好，基本掌握，需要加强
- **<60分**：需要重新学习相关知识点

---

## 提交要求

1. 完整的代码实现
2. 详细的分析报告
3. 可视化结果
4. 使用说明文档

