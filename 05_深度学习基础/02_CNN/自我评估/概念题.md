# CNN概念题

## 📋 说明

本评估包含15道选择题和8道简答题，用于检验你对CNN核心概念的理解。

**建议**：在完成理论学习和代码实践后，进行自我评估。

---

## 一、选择题（15题）

### 1. 卷积操作的主要优势是什么？

A. 计算速度快  
B. 参数共享，减少参数数量  
C. 可以处理任意大小的输入  
D. 以上都是  

**答案**：D

**解析**：卷积操作通过参数共享大幅减少参数数量，同时保持了计算效率和灵活性。

---

### 2. 对于输入尺寸为224×224×3的图像，使用3×3卷积核，输出通道数为64，步长为1，填充为1，输出尺寸是多少？

A. 224×224×64  
B. 222×222×64  
C. 112×112×64  
D. 56×56×64  

**答案**：A

**解析**：使用公式 $H_{out} = \lfloor \frac{H_{in} + 2P - K}{S} \rfloor + 1$，填充为1时，输出尺寸与输入相同。

---

### 3. 最大池化的主要作用是什么？

A. 增加特征图尺寸  
B. 降维、减少计算量、增加感受野  
C. 增加参数数量  
D. 提高模型精度  

**答案**：B

**解析**：最大池化通过降维减少计算量，同时增加感受野，有助于提取更高级的特征。

---

### 4. 为什么VGG使用多个3×3卷积核替代一个5×5卷积核？

A. 3×3卷积核计算更快  
B. 参数更少，且增加非线性  
C. 3×3卷积核效果更好  
D. 以上都是  

**答案**：B

**解析**：两个3×3卷积核的感受野等于一个5×5卷积核，但参数更少（18 vs 25），且增加了非线性。

---

### 5. ResNet的核心创新是什么？

A. 使用更深的网络  
B. 残差连接，解决梯度消失问题  
C. 使用更多的卷积层  
D. 使用更大的卷积核  

**答案**：B

**解析**：ResNet通过残差连接（$y = F(x) + x$）使梯度可以直接传播，解决了深层网络的梯度消失问题。

---

### 6. 卷积层中，如果输入是32×32×3，使用64个3×3卷积核，步长为1，填充为1，参数数量是多少？

A. 576  
B. 1,728  
C. 64  
D. 1,792  

**答案**：D

**解析**：参数数量 = 输出通道数 × 输入通道数 × 卷积核大小 × 卷积核大小 + 输出通道数 = 64 × 3 × 3 × 3 + 64 = 1,792

---

### 7. 1×1卷积的主要作用是什么？

A. 降维  
B. 增加非线性  
C. 跨通道信息融合  
D. 以上都是  

**答案**：D

**解析**：1×1卷积可以降维、增加非线性，同时实现跨通道信息融合。

---

### 8. 批量归一化（Batch Normalization）通常放在哪里？

A. 卷积层之前  
B. 卷积层之后，激活函数之前  
C. 激活函数之后  
D. 池化层之后  

**答案**：B

**解析**：批量归一化通常放在卷积层之后、激活函数之前，可以加速训练并提高稳定性。

---

### 9. Dropout在CNN中的作用是什么？

A. 增加模型容量  
B. 防止过拟合  
C. 加速训练  
D. 提高精度  

**答案**：B

**解析**：Dropout通过随机丢弃神经元，防止模型过拟合。

---

### 10. 感受野（Receptive Field）的定义是什么？

A. 卷积核的大小  
B. 输出特征图的尺寸  
C. 输入图像中影响某个输出值的区域大小  
D. 卷积层的数量  

**答案**：C

**解析**：感受野是指输入图像中影响某个输出值的区域大小。

---

### 11. 深度可分离卷积（Depthwise Separable Convolution）的优势是什么？

A. 参数更少，计算更快  
B. 精度更高  
C. 更容易训练  
D. 以上都是  

**答案**：A

**解析**：深度可分离卷积将标准卷积分解为深度卷积和点卷积，大幅减少参数和计算量。

---

### 12. 全局平均池化（Global Average Pooling）的主要作用是什么？

A. 增加特征图尺寸  
B. 将特征图转换为固定长度的向量  
C. 增加参数数量  
D. 提高计算复杂度  

**答案**：B

**解析**：全局平均池化将整个特征图转换为固定长度的向量，常用于分类网络的最后层。

---

### 13. 转置卷积（Transposed Convolution）的主要用途是什么？

A. 图像分类  
B. 图像上采样、语义分割  
C. 特征提取  
D. 降维  

**答案**：B

**解析**：转置卷积可以实现上采样，常用于语义分割、图像生成等任务。

---

### 14. 在CNN中，浅层通常学习什么特征？

A. 高级语义特征  
B. 低级特征（边缘、纹理）  
C. 全局特征  
D. 抽象特征  

**答案**：B

**解析**：CNN的浅层通常学习低级特征（边缘、纹理），深层学习高级语义特征。

---

### 15. 为什么CNN对图像平移具有不变性？

A. 因为使用了全连接层  
B. 因为卷积操作在图像上滑动，相同特征在不同位置被相同卷积核检测  
C. 因为使用了池化操作  
D. 因为使用了激活函数  

**答案**：B

**解析**：卷积操作在图像上滑动，相同的特征在不同位置会被相同的卷积核检测到，因此具有平移不变性。

---

## 二、简答题（8题）

### 1. 解释卷积操作和全连接操作的区别，为什么CNN使用卷积而不是全连接？

**参考答案**：
- **参数数量**：全连接层参数多（如224×224×3的图像，第一层全连接需要约1500万参数），卷积层参数少（如3×3卷积核，参数只有几十到几百）
- **空间结构**：卷积操作保持空间结构，全连接操作丢失空间信息
- **参数共享**：卷积操作参数共享，全连接操作每个连接都有独立参数
- **局部连接**：卷积操作只连接局部区域，全连接操作连接所有输入

---

### 2. 解释填充（Padding）的作用，什么情况下使用"same"填充？

**参考答案**：
- **作用**：
  1. 保持输出尺寸与输入尺寸相同
  2. 避免边界信息丢失
  3. 控制感受野大小
- **Same填充**：当需要保持输出尺寸与输入尺寸相同时使用，填充大小 = (卷积核大小 - 1) / 2

---

### 3. 解释最大池化和平均池化的区别，各适用于什么场景？

**参考答案**：
- **最大池化**：
  - 取窗口内的最大值
  - 保留最显著的特征
  - 对噪声鲁棒
  - 适用于需要突出重要特征的场景
- **平均池化**：
  - 取窗口内的平均值
  - 保留整体信息
  - 更平滑
  - 适用于需要保留整体信息的场景

---

### 4. 解释ResNet的残差连接如何解决梯度消失问题？

**参考答案**：
- **残差连接**：$y = F(x) + x$
- **梯度传播**：$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot (1 + \frac{\partial F}{\partial x})$
- **关键点**：即使$\frac{\partial F}{\partial x}$很小，梯度也可以通过恒等映射（$+1$）直接传播
- **结果**：梯度可以几乎无损地传播，解决了深层网络的梯度消失问题

---

### 5. 解释批量归一化（Batch Normalization）的原理和作用。

**参考答案**：
- **原理**：
  1. 对每个批次的数据进行归一化：$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$
  2. 可学习的缩放和平移：$y_i = \gamma \hat{x}_i + \beta$
- **作用**：
  1. 加速训练
  2. 允许更大的学习率
  3. 减少对初始化的依赖
  4. 有正则化效果

---

### 6. 解释CNN中层次化特征学习的原理。

**参考答案**：
- **浅层**：学习低级特征（边缘、纹理、颜色）
- **中层**：学习中级特征（形状、模式）
- **深层**：学习高级特征（物体、语义）
- **原理**：通过堆叠多个卷积层，每一层在前一层特征的基础上学习更抽象的特征
- **类比**：就像人类视觉系统，从简单的线条到复杂的物体

---

### 7. 解释1×1卷积的作用，为什么它在CNN中很重要？

**参考答案**：
- **降维**：减少通道数，降低计算量
- **增加非线性**：在保持空间尺寸的同时增加非线性
- **跨通道信息融合**：实现不同通道之间的信息交互
- **应用**：GoogLeNet的Inception模块、ResNet的瓶颈结构都大量使用1×1卷积

---

### 8. 解释CNN的局限性，以及如何改进？

**参考答案**：
- **局限性**：
  1. 固定感受野，难以处理不同尺度的物体
  2. 缺乏旋转不变性
  3. 需要大量标注数据
  4. 计算量大
  5. 可解释性差
- **改进方向**：
  1. 注意力机制（关注重要区域）
  2. 多尺度特征（FPN、PANet）
  3. 轻量级网络（MobileNet、ShuffleNet）
  4. 可解释性方法（可视化、注意力图）

---

## 📊 评分标准

- **选择题**：每题1分，共15分
- **简答题**：每题5分，共40分
- **总分**：55分

**评估标准**：
- 45分以上：优秀，已掌握CNN核心概念
- 35-44分：良好，基本掌握，需要加强某些方面
- 25-34分：中等，需要进一步学习
- 25分以下：需要重新学习基础概念

---

**完成了评估？检查你的答案，确保真正理解了CNN！** ✅

