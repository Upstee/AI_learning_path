# 卷积神经网络（CNN）

> **🎯 快速开始**：如果你是第一次学习CNN，建议先完成[快速上手](./00_快速上手.md)（30分钟），快速体验效果，建立学习信心！

---

## 📚 学习资源导航

- **[快速上手](./00_快速上手.md)** - 30分钟快速体验，建立学习信心
- **[学习检查点](./学习检查点.md)** - 自我评估，确保真正掌握
- **[常见问题FAQ](./常见问题FAQ.md)** - 快速解决学习中的问题
- **[神经网络可视化工具](../01_神经网络基础/神经网络可视化工具.md)** - 可视化CNN结构
- **[训练过程监控指南](../01_神经网络基础/训练过程监控指南.md)** - 监控训练过程
- **[调试技巧和常见错误](../01_神经网络基础/调试技巧和常见错误.md)** - 调试技巧

---

## 1. 课程概述

### 课程目标

1. 理解卷积操作和池化操作
2. 掌握CNN的架构设计原理
3. 理解经典CNN架构（LeNet、AlexNet、VGG、ResNet等）
4. 能够使用PyTorch/TensorFlow实现CNN
5. 能够应用CNN解决图像分类等问题

### 预计学习时间

- **理论学习**：8-10小时
- **代码实践**：12-15小时
- **练习巩固**：6-8小时
- **总计**：26-33小时（约1-2周）

### 难度等级

- **中等偏上** - 需要理解卷积操作和特征提取

### 课程定位

- **前置课程**：01_神经网络基础、04_PyTorch_TensorFlow
- **后续课程**：06_计算机视觉（CV应用）
- **在体系中的位置**：图像处理的基础架构，使用框架实现

### 学完能做什么

- ✅ 理解CNN的工作原理
- ✅ 能够使用框架实现CNN
- ✅ 能够设计CNN架构
- ✅ 能够应用CNN解决图像问题
- ✅ 理解经典CNN架构的设计思想

---

## 2. 前置知识检查

### 必备前置概念清单

- **神经网络基础**：前向传播、反向传播
- **深度学习框架**：PyTorch或TensorFlow
- **图像处理基础**：理解图像数据格式
- **线性代数**：矩阵运算、卷积运算

### 回顾链接/跳转

- 如果不熟悉神经网络：[01_神经网络基础](../01_神经网络基础/)
- 如果不熟悉框架：[04_PyTorch_TensorFlow](../04_PyTorch_TensorFlow/)

### 2.5 知识关联

#### 前置知识依赖链

**直接前置**：
- [前馈神经网络](../01_神经网络基础/02_前馈神经网络/) - 理解网络结构、前向传播、反向传播
- [PyTorch基础](../04_PyTorch_TensorFlow/01_PyTorch基础/) 或 [TensorFlow基础](../04_PyTorch_TensorFlow/02_TensorFlow基础/) - 使用框架实现

**间接前置**：
- [线性代数](../../02_数学基础/01_线性代数/) - 卷积运算、矩阵运算
- [感知器与神经元](../01_神经网络基础/01_感知器与神经元/) - 理解基本单元

#### 相关概念交叉引用

**本模块核心概念**：
- **卷积操作**：本模块首次详细讲解，是CNN的核心
- **池化操作**：本模块首次详细讲解，是CNN的重要组成部分
- **CNN架构**：基于[前馈神经网络](../01_神经网络基础/02_前馈神经网络/)的扩展

**相关概念**：
- **前向传播**：[前馈神经网络/前向传播](../01_神经网络基础/02_前馈神经网络/#前向传播算法) - CNN使用相同原理
- **反向传播**：[前馈神经网络/反向传播](../01_神经网络基础/02_前馈神经网络/#反向传播算法) - CNN使用相同原理
- **BatchNorm**：[逐层归一化/批量归一化](../05_网络优化与正则化/04_逐层归一化/01_批量归一化/) - CNN中常用

#### 后续应用场景

**直接后续**：
- [计算机视觉](../../06_计算机视觉/) - CNN在CV中的应用
- [Transformer](../../06_注意力机制与外部记忆/02_Transformer架构/) - Vision Transformer结合CNN和Transformer

**优化应用**：
- [网络优化](../05_网络优化与正则化/01_网络优化/) - 优化CNN训练
- [数据增强](../05_网络优化与正则化/06_网络正则化/05_数据增强/) - CV中常用数据增强

---

## 3. 核心知识点详解

### 3.1 卷积操作

#### 3.1.1 卷积的概念

**卷积（Convolution）**：通过滑动窗口提取局部特征。

**数学表示**：
$$(f * g)(x) = \int f(\tau) g(x - \tau) d\tau$$

**离散形式**（2D图像）：
$$(I * K)(i, j) = \sum_{m} \sum_{n} I(i+m, j+n) K(m, n)$$

#### 3.1.2 卷积层的作用

- **特征提取**：自动学习有用的特征
- **参数共享**：减少参数数量
- **局部连接**：只连接局部区域

### 3.2 池化操作

#### 3.2.1 最大池化（Max Pooling）

**作用**：降维、减少计算量、增加感受野

**操作**：在窗口内取最大值

#### 3.2.2 平均池化（Average Pooling）

**操作**：在窗口内取平均值

### 3.3 经典CNN架构

- **LeNet**（1998）：最早的CNN
- **AlexNet**（2012）：深度学习复兴
- **VGG**（2014）：深度网络
- **ResNet**（2015）：残差连接
- **EfficientNet**（2019）：效率优化

---

## 4. Python代码实践

详细代码请参考：`代码示例/` 文件夹

---

## 5. 动手练习（分层次）

### 基础练习（3-5题）

#### 练习1：理解卷积操作
**难度**：⭐⭐

#### 练习2：实现简单CNN
**难度**：⭐⭐⭐

#### 练习3：实现经典架构
**难度**：⭐⭐⭐⭐

### 进阶练习（2-3题）

#### 练习1：设计自定义CNN架构
**难度**：⭐⭐⭐⭐

### 挑战练习（1-2题）

#### 练习1：实现ResNet
**难度**：⭐⭐⭐⭐⭐

---

## 6. 实际案例

详细内容请参考：`实战案例/` 文件夹

---

## 7. 自我评估

详细评估题目请参考：`自我评估/` 文件夹

---

## 8. 拓展学习

### 论文推荐

1. **LeCun, Y., et al. (1998). "Gradient-based learning applied to document recognition."**
   - LeNet的原始论文
   - 难度：⭐⭐⭐⭐

2. **Krizhevsky, A., et al. (2012). "ImageNet classification with deep convolutional neural networks."**
   - AlexNet的原始论文
   - 难度：⭐⭐⭐⭐

### 下节课预告

**下节课**：`03_RNN_LSTM`

**内容预告**：
- 循环神经网络
- LSTM和GRU
- 序列建模

---

**继续学习，成为深度学习专家！** 🚀

