# 循环神经网络（RNN/LSTM）

> **🎯 快速开始**：如果你是第一次学习RNN/LSTM，建议先完成[快速上手](./00_快速上手.md)（30分钟），快速体验效果，建立学习信心！

---

## 📚 学习资源导航

- **[快速上手](./00_快速上手.md)** - 30分钟快速体验，建立学习信心
- **[学习检查点](./学习检查点.md)** - 自我评估，确保真正掌握
- **[常见问题FAQ](./常见问题FAQ.md)** - 快速解决学习中的问题
- **[神经网络可视化工具](../01_神经网络基础/神经网络可视化工具.md)** - 可视化RNN结构
- **[训练过程监控指南](../01_神经网络基础/训练过程监控指南.md)** - 监控训练过程
- **[调试技巧和常见错误](../01_神经网络基础/调试技巧和常见错误.md)** - 调试技巧

---

## 1. 课程概述

### 课程目标

1. 理解循环神经网络（RNN）的原理
2. 掌握LSTM和GRU的结构和工作原理
3. 理解序列建模的概念
4. 能够使用框架实现RNN/LSTM
5. 能够应用RNN解决序列问题

### 预计学习时间

- **理论学习**：8-10小时
- **代码实践**：12-15小时
- **练习巩固**：6-8小时
- **总计**：26-33小时（约1-2周）

### 难度等级

- **中等偏上** - 需要理解序列建模和记忆机制

### 课程定位

- **前置课程**：01_神经网络基础、04_PyTorch_TensorFlow
- **后续课程**：06_注意力机制与外部记忆、07_自然语言处理
- **在体系中的位置**：序列建模的基础架构，使用框架实现

### 学完能做什么

- ✅ 理解RNN和LSTM的工作原理
- ✅ 能够使用框架实现RNN/LSTM
- ✅ 能够应用RNN解决序列问题
- ✅ 理解梯度消失问题及LSTM的解决方案
- ✅ 为学习Transformer打下基础

---

## 2. 前置知识检查

### 必备前置概念清单

- **神经网络基础**：前向传播、反向传播
- **深度学习框架**：PyTorch或TensorFlow
- **序列数据**：理解时间序列、文本序列
- **梯度问题**：理解梯度消失

### 回顾链接/跳转

- 如果不熟悉神经网络：[01_神经网络基础](../01_神经网络基础/)
- 如果不熟悉框架：[04_PyTorch_TensorFlow](../04_PyTorch_TensorFlow/)

### 2.5 知识关联

#### 前置知识依赖链

**直接前置**：
- [前馈神经网络](../01_神经网络基础/02_前馈神经网络/) - 理解网络结构、前向传播、反向传播
- [PyTorch基础](../04_PyTorch_TensorFlow/01_PyTorch基础/) 或 [TensorFlow基础](../04_PyTorch_TensorFlow/02_TensorFlow基础/) - 使用框架实现

**间接前置**：
- [感知器与神经元](../01_神经网络基础/01_感知器与神经元/) - 理解基本单元
- [自动梯度与优化](../01_神经网络基础/03_自动梯度与优化/) - 理解梯度传播

#### 相关概念交叉引用

**本模块核心概念**：
- **RNN**：本模块首次详细讲解，是序列建模的基础
- **LSTM**：本模块首次详细讲解，解决RNN的梯度消失问题
- **GRU**：本模块首次详细讲解，LSTM的简化版本

**相关概念**：
- **梯度消失**：[前馈神经网络/梯度消失](../01_神经网络基础/02_前馈神经网络/#梯度消失和梯度爆炸) - RNN也存在此问题
- **激活函数**：[感知器与神经元/激活函数](../01_神经网络基础/01_感知器与神经元/#激活函数) - RNN使用Tanh/Sigmoid
- **LayerNorm**：[逐层归一化/层归一化](../05_网络优化与正则化/04_逐层归一化/02_层归一化/) - RNN中常用

#### 后续应用场景

**直接后续**：
- [自然语言处理](../../07_自然语言处理/) - RNN/LSTM在NLP中的应用
- [Transformer](../../06_注意力机制与外部记忆/02_Transformer架构/) - Transformer替代RNN用于序列建模
- [时间序列](../../10_时间序列/) - RNN/LSTM在时间序列中的应用

**优化应用**：
- [网络优化](../05_网络优化与正则化/01_网络优化/) - 优化RNN训练
- [梯度裁剪](../05_网络优化与正则化/01_网络优化/) - RNN中常用防止梯度爆炸

---

## 3. 核心知识点详解

### 3.1 循环神经网络（RNN）

#### 3.1.1 RNN的结构

**核心思想**：使用隐藏状态$h_t$保存历史信息。

**更新公式**：
$$h_t = f(W_h h_{t-1} + W_x x_t + b)$$

**输出**：
$$y_t = g(W_y h_t + b_y)$$

#### 3.1.2 RNN的问题

- **梯度消失**：长序列中梯度难以传播
- **梯度爆炸**：梯度可能指数增长
- **长期依赖**：难以学习长期依赖关系

### 3.2 LSTM（Long Short-Term Memory）

#### 3.2.1 LSTM的结构

**核心组件**：
- **遗忘门（Forget Gate）**：决定丢弃什么信息
- **输入门（Input Gate）**：决定存储什么信息
- **输出门（Output Gate）**：决定输出什么信息
- **细胞状态（Cell State）**：长期记忆

#### 3.2.2 LSTM的优势

- ✅ 解决梯度消失问题
- ✅ 能够学习长期依赖
- ✅ 记忆能力强

### 3.3 GRU（Gated Recurrent Unit）

**GRU**是LSTM的简化版本，只有两个门：
- **更新门（Update Gate）**
- **重置门（Reset Gate）**

**特点**：参数更少，计算更快，效果接近LSTM。

---

## 4. Python代码实践

详细代码请参考：`代码示例/` 文件夹

---

## 5. 动手练习（分层次）

### 基础练习（3-5题）

#### 练习1：实现简单RNN
**难度**：⭐⭐⭐

#### 练习2：实现LSTM
**难度**：⭐⭐⭐⭐

#### 练习3：对比RNN和LSTM
**难度**：⭐⭐⭐⭐

### 进阶练习（2-3题）

#### 练习1：实现GRU
**难度**：⭐⭐⭐⭐

### 挑战练习（1-2题）

#### 练习1：实现双向LSTM
**难度**：⭐⭐⭐⭐⭐

---

## 6. 实际案例

详细内容请参考：`实战案例/` 文件夹

---

## 7. 自我评估

详细评估题目请参考：`自我评估/` 文件夹

---

## 8. 拓展学习

### 论文推荐

1. **Hochreiter, S., & Schmidhuber, J. (1997). "Long short-term memory."**
   - LSTM的原始论文
   - 难度：⭐⭐⭐⭐

2. **Cho, K., et al. (2014). "Learning phrase representations using RNN encoder-decoder for statistical machine translation."**
   - GRU的原始论文
   - 难度：⭐⭐⭐⭐

### 下节课预告

**下节课**：`06_注意力机制与外部记忆`

**内容预告**：
- 注意力机制
- Transformer架构
- 外部记忆网络

---

**继续学习，成为深度学习专家！** 🚀

