# RNN/LSTM概念题

## 📋 说明

本评估包含15道选择题和8道简答题，用于检验你对RNN/LSTM核心概念的理解。

**建议**：在完成理论学习和代码实践后，进行自我评估。

---

## 一、选择题（15题）

### 1. RNN的核心特点是什么？

A. 可以处理变长序列  
B. 通过隐藏状态保存历史信息  
C. 参数共享  
D. 以上都是  

**答案**：D

**解析**：RNN通过隐藏状态保存历史信息，可以处理变长序列，且参数在所有时间步共享。

---

### 2. RNN的隐藏状态更新公式是什么？

A. $h_t = W_h h_{t-1} + W_x x_t$  
B. $h_t = \tanh(W_h h_{t-1} + W_x x_t + b)$  
C. $h_t = \text{ReLU}(W_h h_{t-1} + W_x x_t)$  
D. $h_t = \sigma(W_h h_{t-1} + W_x x_t)$  

**答案**：B

**解析**：RNN通常使用Tanh激活函数，更新公式为 $h_t = \tanh(W_h h_{t-1} + W_x x_t + b)$。

---

### 3. RNN的主要问题是什么？

A. 计算速度慢  
B. 梯度消失和梯度爆炸  
C. 参数过多  
D. 无法处理序列数据  

**答案**：B

**解析**：RNN的主要问题是梯度消失和梯度爆炸，导致难以学习长期依赖。

---

### 4. LSTM的核心创新是什么？

A. 使用更多的隐藏层  
B. 门控机制和细胞状态  
C. 使用ReLU激活函数  
D. 增加参数数量  

**答案**：B

**解析**：LSTM通过门控机制和细胞状态解决梯度消失问题，能够学习长期依赖。

---

### 5. LSTM有几个门？

A. 1个  
B. 2个  
C. 3个  
D. 4个  

**答案**：C

**解析**：LSTM有三个门：遗忘门、输入门、输出门。

---

### 6. LSTM的遗忘门的作用是什么？

A. 决定丢弃什么信息  
B. 决定存储什么信息  
C. 决定输出什么信息  
D. 决定输入什么信息  

**答案**：A

**解析**：遗忘门决定从细胞状态中丢弃什么信息。

---

### 7. LSTM的细胞状态更新公式是什么？

A. $c_t = f_t \odot c_{t-1}$  
B. $c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$  
C. $c_t = i_t \odot \tilde{c}_t$  
D. $c_t = o_t \odot c_{t-1}$  

**答案**：B

**解析**：细胞状态更新公式为 $c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$，结合遗忘和输入。

---

### 8. GRU和LSTM的主要区别是什么？

A. GRU有更多的门  
B. GRU没有细胞状态，参数更少  
C. GRU计算更慢  
D. GRU效果更好  

**答案**：B

**解析**：GRU没有细胞状态，只有两个门（更新门和重置门），参数更少，计算更快。

---

### 9. 双向RNN的主要优势是什么？

A. 计算更快  
B. 参数更少  
C. 同时利用过去和未来的信息  
D. 更容易训练  

**答案**：C

**解析**：双向RNN同时处理前向和后向序列，可以同时利用过去和未来的信息。

---

### 10. RNN的反向传播算法叫什么？

A. 反向传播（Backpropagation）  
B. 时间反向传播（BPTT）  
C. 梯度下降（Gradient Descent）  
D. 随机梯度下降（SGD）  

**答案**：B

**解析**：RNN的反向传播算法称为时间反向传播（Backpropagation Through Time, BPTT）。

---

### 11. 梯度裁剪的主要作用是什么？

A. 加速训练  
B. 防止梯度爆炸  
C. 防止梯度消失  
D. 提高精度  

**答案**：B

**解析**：梯度裁剪通过限制梯度范数，防止梯度爆炸，保持训练稳定。

---

### 12. RNN的隐藏状态通常如何初始化？

A. 随机初始化  
B. 初始化为零向量  
C. 初始化为全1向量  
D. 使用Xavier初始化  

**答案**：B

**解析**：RNN的隐藏状态通常初始化为零向量 $h_0 = \mathbf{0}$。

---

### 13. LSTM的遗忘门偏置通常如何初始化？

A. 初始化为0  
B. 初始化为较大的正值（如1.0）  
C. 初始化为负值  
D. 随机初始化  

**答案**：B

**解析**：遗忘门偏置初始化为较大的正值，使$f_t$初始值接近1，鼓励保留信息。

---

### 14. 序列到序列（Seq2Seq）任务通常使用什么架构？

A. 单层RNN  
B. 编码器-解码器架构  
C. 双向RNN  
D. 深度RNN  

**答案**：B

**解析**：序列到序列任务（如机器翻译）通常使用编码器-解码器架构。

---

### 15. RNN/LSTM的主要应用领域是什么？

A. 图像分类  
B. 自然语言处理、时间序列预测  
C. 目标检测  
D. 图像分割  

**答案**：B

**解析**：RNN/LSTM主要用于自然语言处理、时间序列预测等序列建模任务。

---

## 二、简答题（8题）

### 1. 解释RNN的梯度消失问题，为什么会出现？

**参考答案**：
- **问题**：在长序列中，梯度在反向传播时指数衰减，导致无法学习长期依赖
- **原因**：
  1. 梯度需要经过多个时间步传播：$\frac{\partial L_t}{\partial h_k} = \frac{\partial L_t}{\partial h_t} \prod_{j=k+1}^{t} \frac{\partial h_j}{\partial h_{j-1}}$
  2. 每个时间步的梯度都包含 $\tanh'(z_j) < 1$ 和 $W_h$，如果特征值$< 1$，梯度会指数衰减
  3. 当序列很长时，梯度接近0，无法更新早期时间步的参数

---

### 2. 解释LSTM如何解决梯度消失问题。

**参考答案**：
- **关键创新**：细胞状态$c_t$的更新是**加法**操作，而不是乘法
- **梯度传播**：$\frac{\partial c_t}{\partial c_{t-1}} = f_t + \text{其他项}$
- **关键点**：如果遗忘门$f_t \approx 1$，梯度可以几乎无损地通过细胞状态传播
- **结果**：即使其他路径梯度消失，通过细胞状态的梯度仍然可以传播，解决了梯度消失问题

---

### 3. 解释LSTM的三个门（遗忘门、输入门、输出门）的作用。

**参考答案**：
- **遗忘门**：决定从细胞状态中丢弃什么信息，$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
- **输入门**：决定存储什么新信息到细胞状态，$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
- **输出门**：决定从细胞状态中输出什么信息，$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
- **协同工作**：三个门协同控制信息的流动，实现精确的记忆管理

---

### 4. 解释GRU和LSTM的区别，各适用于什么场景？

**参考答案**：
- **GRU**：
  - 只有两个门：更新门和重置门
  - 没有细胞状态，只有隐藏状态
  - 参数更少（约LSTM的75%）
  - 计算更快
  - 效果与LSTM相近
- **LSTM**：
  - 有三个门：遗忘门、输入门、输出门
  - 有细胞状态和隐藏状态
  - 参数更多
  - 计算更慢
  - 记忆能力更强
- **选择建议**：数据量大用GRU，需要强记忆用LSTM

---

### 5. 解释双向RNN的工作原理和适用场景。

**参考答案**：
- **工作原理**：
  1. 前向RNN处理序列：$h_t^{forward} = f(W_h^f h_{t-1}^{forward} + W_x^f x_t + b_h^f)$
  2. 后向RNN处理序列：$h_t^{backward} = f(W_h^b h_{t+1}^{backward} + W_x^b x_t + b_h^b)$
  3. 合并：$h_t = [h_t^{forward}; h_t^{backward}]$
- **优势**：同时利用过去和未来的信息
- **适用场景**：序列标注（词性标注、命名实体识别）、文本分类等
- **限制**：不适合在线预测（需要看到完整序列）

---

### 6. 解释RNN的序列到序列（Seq2Seq）架构。

**参考答案**：
- **编码器（Encoder）**：
  1. 处理输入序列，生成上下文向量$c$
  2. $c = h_T$（最后一个隐藏状态）或所有隐藏状态的加权和
- **解码器（Decoder）**：
  1. 以$c$为初始隐藏状态，生成输出序列
  2. 每个时间步生成一个输出
- **应用**：机器翻译、文本摘要、对话系统
- **改进**：注意力机制可以改进Seq2Seq架构

---

### 7. 解释RNN的梯度爆炸问题及解决方法。

**参考答案**：
- **问题**：如果权重矩阵$W_h$的特征值$> 1$，梯度会指数增长，导致训练不稳定
- **解决方法**：
  1. **梯度裁剪**：限制梯度范数，$\mathbf{g} \leftarrow \mathbf{g} \cdot \frac{\text{max_norm}}{||\mathbf{g}||}$
  2. **权重初始化**：使用正交初始化，保证特征值模为1
  3. **权重正则化**：限制权重大小
  4. **使用LSTM/GRU**：门控机制可以更好地控制梯度流

---

### 8. 解释RNN/LSTM的局限性，以及Transformer如何改进。

**参考答案**：
- **RNN/LSTM的局限性**：
  1. 无法并行化（必须顺序处理）
  2. 长期依赖仍然有限（虽然LSTM改善了，但仍有限制）
  3. 计算效率低
- **Transformer的改进**：
  1. **自注意力机制**：直接建模任意距离的依赖关系
  2. **并行计算**：可以并行处理所有时间步
  3. **更好的长期依赖**：注意力机制可以关注任意位置
  4. **计算效率**：虽然单次计算更复杂，但可以并行，总体更快

---

## 📊 评分标准

- **选择题**：每题1分，共15分
- **简答题**：每题5分，共40分
- **总分**：55分

**评估标准**：
- 45分以上：优秀，已掌握RNN/LSTM核心概念
- 35-44分：良好，基本掌握，需要加强某些方面
- 25-34分：中等，需要进一步学习
- 25分以下：需要重新学习基础概念

---

**完成了评估？检查你的答案，确保真正理解了RNN/LSTM！** ✅

