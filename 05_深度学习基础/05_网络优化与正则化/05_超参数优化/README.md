# 超参数优化

## 1. 课程概述

### 课程目标

1. 理解超参数的概念和重要性
2. 掌握超参数搜索方法（网格搜索、随机搜索）
3. 理解贝叶斯优化等高级方法
4. 能够选择合适的超参数优化策略
5. 掌握超参数调优的实践技巧

### 预计学习时间

- **理论学习**：4-5小时
- **代码实践**：6-8小时
- **练习巩固**：3-4小时
- **总计**：13-17小时（约2-3天）

### 难度等级

- **中等** - 需要理解搜索策略和优化方法

### 课程定位

- **前置课程**：01_网络优化（理解优化算法）、01_神经网络基础
- **后续课程**：08_综合实战项目
- **在体系中的位置**：模型调优的关键步骤

### 学完能做什么

- ✅ 理解超参数的重要性
- ✅ 掌握网格搜索和随机搜索
- ✅ 理解贝叶斯优化等高级方法
- ✅ 能够进行超参数调优
- ✅ 提升模型性能

---

## 2. 前置知识检查

### 必备前置概念清单

- **优化算法**：理解学习率等超参数的作用
- **模型评估**：理解验证集、交叉验证
- **Python基础**：循环、函数

### 回顾链接/跳转

- 如果不熟悉优化算法：[01_网络优化](../01_网络优化/)
- 如果不熟悉模型评估：[04_机器学习基础/03_模型评估与优化](../../04_机器学习基础/03_模型评估与优化/)

### 2.5 知识关联

#### 前置知识依赖链

**直接前置**：
- [网络优化](../01_网络优化/) - 理解学习率等超参数的作用
- [模型评估](../../04_机器学习基础/03_模型评估与优化/) - 理解验证集、交叉验证

**间接前置**：
- [前馈神经网络](../../01_神经网络基础/02_前馈神经网络/) - 理解网络结构
- [Python基础](../../01_Python进阶/) - 循环、函数

#### 相关概念交叉引用

**本模块核心概念**：
- **超参数**：本模块首次详细讲解，是训练前设定的参数
- **网格搜索**：本模块首次详细讲解，是超参数搜索方法
- **随机搜索**：本模块首次详细讲解，是更高效的搜索方法

**相关概念**：
- **学习率**：[网络优化/学习率](../01_网络优化/#学习率) - 重要超参数
- **批量大小**：[网络优化/批量大小](../01_网络优化/#批量大小) - 重要超参数
- **正则化系数**：[网络正则化](../06_网络正则化/) - 重要超参数

#### 后续应用场景

**所有训练场景**：
- 所有神经网络的训练都需要超参数优化
- [CNN](../../02_CNN/)、[RNN](../../03_RNN_LSTM/)、[Transformer](../../06_注意力机制与外部记忆/02_Transformer架构/)等都需要调优

### 入门小测

**选择题**（每题2分，共10分）

1. 超参数是？
   A. 模型参数  B. 训练前设定的参数  C. 自动学习的参数  D. 数据参数
   **答案**：B
   **解释**：超参数是训练前需要人工设定的参数，如学习率、批量大小等。

2. 网格搜索的缺点是？
   A. 计算慢  B. 可能遗漏最优值  C. 需要大量计算资源  D. 以上都是
   **答案**：D
   **解释**：网格搜索需要遍历所有组合，计算量大，且可能遗漏最优值。

3. 随机搜索相比网格搜索的优势是？
   A. 计算更快  B. 可能找到更好的超参数  C. 更灵活  D. 以上都是
   **答案**：D
   **解释**：随机搜索不需要遍历所有组合，更高效，且可能探索到更好的区域。

**评分标准**：≥8分（80%）为通过

---

## 3. 核心知识点详解

### 3.1 超参数的概念

#### 3.1.1 什么是超参数？

**超参数（Hyperparameter）**：训练前需要人工设定的参数，不能通过训练自动学习。

**常见超参数**：
- 学习率（learning rate）
- 批量大小（batch size）
- 网络层数、每层神经元数
- 正则化系数
- 优化算法参数（如Adam的beta1、beta2）

#### 3.1.2 超参数 vs 模型参数

| 特性 | 超参数 | 模型参数 |
|------|--------|---------|
| **设定时机** | 训练前 | 训练中学习 |
| **设定方式** | 人工设定 | 自动学习 |
| **数量** | 少量 | 大量（百万级） |
| **影响** | 影响训练过程和模型性能 | 决定模型预测 |

### 3.2 超参数搜索方法

#### 3.2.1 网格搜索（Grid Search）

**原理**：遍历所有超参数组合。

**优点**：
- 系统全面
- 实现简单

**缺点**：
- 计算量大
- 可能遗漏最优值
- 维度灾难

#### 3.2.2 随机搜索（Random Search）

**原理**：随机采样超参数组合。

**优点**：
- 计算效率高
- 可能找到更好的超参数
- 适合高维空间

**缺点**：
- 可能遗漏某些区域
- 需要设定搜索范围

#### 3.2.3 贝叶斯优化（Bayesian Optimization）

**原理**：使用概率模型指导搜索，平衡探索和利用。

**优点**：
- 更高效
- 自动平衡探索和利用

**缺点**：
- 实现复杂
- 需要更多计算资源

---

## 4. Python代码实践

详细代码请参考：`代码示例/` 文件夹

---

## 5. 动手练习（分层次）

### 基础练习（3-5题）

#### 练习1：实现网格搜索
**难度**：⭐⭐

#### 练习2：实现随机搜索
**难度**：⭐⭐

#### 练习3：对比网格搜索和随机搜索
**难度**：⭐⭐⭐

### 进阶练习（2-3题）

#### 练习1：实现简单的贝叶斯优化
**难度**：⭐⭐⭐⭐

### 挑战练习（1-2题）

#### 练习1：设计自适应超参数优化系统
**难度**：⭐⭐⭐⭐⭐

---

## 6. 实际案例

详细内容请参考：`实战案例/` 文件夹

---

## 7. 自我评估

详细评估题目请参考：`自我评估/` 文件夹

---

## 8. 拓展学习

### 论文推荐

1. **Bergstra, J., & Bengio, Y. (2012). "Random search for hyper-parameter optimization."**
   - 随机搜索的经典论文
   - 难度：⭐⭐⭐

2. **Snoek, J., Larochelle, H., & Adams, R. P. (2012). "Practical Bayesian optimization of machine learning algorithms."**
   - 贝叶斯优化的经典论文
   - 难度：⭐⭐⭐⭐

### 书籍推荐

1. **《神经网络与深度学习-邱锡鹏》**
   - 第7章：网络优化与正则化（超参数优化部分）

### 下节课预告

**下节课**：`06_网络正则化`

**内容预告**：
- L1/L2正则化
- Dropout
- 数据增强
- 其他正则化技术

---

**继续学习，成为深度学习专家！** 🚀

