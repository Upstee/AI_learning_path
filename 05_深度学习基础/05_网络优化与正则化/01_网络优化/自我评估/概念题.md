# 自我评估：概念题

## 选择题（每题2分，共20分）

### 1. 批量梯度下降、随机梯度下降、小批量梯度下降的主要区别是？

A. 学习率  
B. 批量大小  
C. 优化算法  
D. 激活函数

**答案**：B

**解析**：三种方法的主要区别在于每次更新使用的样本数量（批量大小）。

---

### 2. Momentum算法的核心思想是？

A. 使用历史梯度  
B. 自适应学习率  
C. 二阶信息  
D. 正则化

**答案**：A

**解析**：Momentum使用历史梯度的移动平均来加速收敛并减少震荡。

---

### 3. Adam算法结合了哪两种方法？

A. SGD和Momentum  
B. Momentum和RMSprop  
C. SGD和RMSprop  
D. 都不对

**答案**：B

**解析**：Adam结合了Momentum（一阶矩）和RMSprop（二阶矩）的优点。

---

### 4. 学习率过大会导致？

A. 收敛慢  
B. 训练不稳定，可能发散  
C. 陷入局部最优  
D. 梯度消失

**答案**：B

**解析**：学习率过大会导致参数更新过大，训练不稳定，甚至发散。

---

### 5. 学习率过小会导致？

A. 训练不稳定  
B. 收敛慢，可能无法收敛  
C. 过拟合  
D. 梯度爆炸

**答案**：B

**解析**：学习率过小会导致参数更新太小，收敛速度慢，甚至无法在合理时间内收敛。

---

### 6. Adam优化器中的偏差修正（bias correction）的作用是？

A. 减少内存使用  
B. 改善初始阶段的估计  
C. 加速收敛  
D. 防止过拟合

**答案**：B

**解析**：偏差修正用于改善Adam在初始阶段（t较小时）的估计偏差。

---

### 7. 在大多数深度学习任务中，默认推荐的优化算法是？

A. SGD  
B. Momentum  
C. Adam  
D. AdaGrad

**答案**：C

**解析**：Adam是当前最流行的优化算法，适合大多数深度学习任务。

---

### 8. 批量大小（batch size）的选择主要考虑？

A. 学习率  
B. 内存大小和计算效率  
C. 优化算法  
D. 激活函数

**答案**：B

**解析**：批量大小的选择主要受限于GPU内存大小，同时影响计算效率和梯度估计准确性。

---

### 9. 梯度消失问题主要是由于？

A. 学习率太大  
B. 激活函数饱和，梯度很小  
C. 数据太少  
D. 权重太大

**答案**：B

**解析**：梯度消失主要是由于激活函数（如Sigmoid）在饱和区域梯度很小，导致反向传播时梯度逐渐变小。

---

### 10. 余弦退火学习率调整策略的特点是？

A. 学习率单调递减  
B. 学习率周期性变化，平滑过渡  
C. 学习率固定不变  
D. 学习率随机变化

**答案**：B

**解析**：余弦退火使学习率按照余弦函数平滑变化，在训练后期有小的学习率用于精细调优。

---

## 简答题（每题10分，共50分）

### 1. 解释SGD、Momentum和Adam三种优化算法的主要区别和适用场景。

**参考答案**：

**SGD（随机梯度下降）**：
- 更新规则：θ = θ - η * ∇L
- 特点：简单直接，但可能震荡
- 适用场景：凸优化、简单任务

**Momentum（动量法）**：
- 更新规则：使用历史梯度的移动平均
- 特点：加速收敛，减少震荡
- 适用场景：需要加速收敛的任务

**Adam（自适应矩估计）**：
- 更新规则：结合一阶矩和二阶矩，自适应调整学习率
- 特点：自适应、对超参数不敏感
- 适用场景：大多数深度学习任务（默认选择）

---

### 2. 详细说明学习率调整的重要性以及常见的调整策略。

**参考答案**：

**重要性**：
- 学习率是训练过程中最重要的超参数之一
- 合适的学习率能够加速收敛并达到更好的性能
- 学习率过大或过小都会影响训练效果

**常见策略**：
1. **固定学习率**：最简单，但通常不是最优
2. **指数衰减**：η_t = η_0 * e^(-kt)
3. **阶梯衰减**：每隔一定epoch降低学习率
4. **余弦退火**：平滑的学习率变化，适合精细调优
5. **自适应学习率**：Adam等算法自动调整

---

### 3. 解释批量大小对训练的影响，以及如何选择合适的批量大小。

**参考答案**：

**影响**：
- **小批量**：梯度估计不准确，但可能跳出局部最优，计算快
- **大批量**：梯度估计准确，但可能陷入局部最优，计算慢
- **中等批量**：平衡准确性和效率（推荐32-256）

**选择原则**：
- 根据GPU内存大小选择
- 通常使用2的幂次（32, 64, 128, 256）
- 在内存允许的情况下，选择较大的批量大小

---

### 4. 描述Adam优化算法的工作原理，包括一阶矩、二阶矩和偏差修正。

**参考答案**：

**工作原理**：
1. **一阶矩估计（m_t）**：梯度的移动平均，类似Momentum
   - m_t = β1 * m_{t-1} + (1-β1) * ∇L

2. **二阶矩估计（v_t）**：梯度平方的移动平均，类似RMSprop
   - v_t = β2 * v_{t-1} + (1-β2) * (∇L)^2

3. **偏差修正**：改善初始阶段的估计
   - m̂_t = m_t / (1 - β1^t)
   - v̂_t = v_t / (1 - β2^t)

4. **参数更新**：
   - θ_{t+1} = θ_t - η * m̂_t / (√v̂_t + ε)

**优点**：自适应学习率，对超参数不敏感，适合大多数任务。

---

### 5. 解释梯度消失和梯度爆炸问题，以及优化算法如何帮助解决这些问题。

**参考答案**：

**梯度消失**：
- 原因：深层网络中，梯度在反向传播时逐渐变小
- 影响：浅层参数更新缓慢，训练困难
- 解决：使用ReLU等激活函数、批量归一化、残差连接

**梯度爆炸**：
- 原因：梯度在反向传播时逐渐变大
- 影响：参数更新过大，训练不稳定
- 解决：梯度裁剪、合适的权重初始化

**优化算法的作用**：
- Adam等自适应优化算法通过调整学习率，可以在一定程度上缓解这些问题
- 但主要还是要通过激活函数、归一化等技术解决

---

## 评分标准

- **选择题**：每题2分，共20分
- **简答题**：每题10分，共50分
- **总分**：70分
- **通过标准**：≥56分（80%）

---

**完成评估后，检查你的理解程度，如有不足请复习相关章节！** 📚

