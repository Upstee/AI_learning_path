# 进阶练习1：实现Adam优化器

## 练习目标

从零实现Adam优化器，深入理解其工作原理。

## 任务说明

本练习要求你：
1. 理解Adam算法的数学原理
2. 实现Adam优化器
3. 在简单问题上测试
4. 对比不同超参数的影响

## 任务要求

### 任务1：理解Adam算法

阅读Adam算法的数学公式，理解：
- 一阶矩估计（m_t）的作用
- 二阶矩估计（v_t）的作用
- 偏差修正的必要性

### 任务2：实现Adam优化器

实现一个完整的Adam优化器类，包含：
- `__init__`：初始化参数（learning_rate, beta1, beta2, epsilon）
- `update`：更新参数的方法
- 正确实现偏差修正

### 任务3：测试和对比

1. 在简单优化问题上测试Adam
2. 对比不同beta1和beta2值的影响
3. 对比Adam与SGD、Momentum的性能

## 提示

- 参考 `代码示例/01_从零实现优化算法.ipynb` 中的Adam实现
- 注意偏差修正的实现：m̂_t = m_t / (1 - β1^t)
- 注意数值稳定性：在计算√v̂_t时加上epsilon

## 评分标准

- 正确实现Adam算法（40分）
- 正确实现偏差修正（20分）
- 测试用例通过（20分）
- 代码注释清晰（20分）

---

**完成本练习后，你将深入理解Adam优化器的工作原理！** 🎉


