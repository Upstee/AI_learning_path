{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 进阶练习1：实现Adam优化器\n",
        "\n",
        "## 练习目标\n",
        "\n",
        "从零实现Adam（Adaptive Moment Estimation）优化器，深入理解其工作原理和数学原理。\n",
        "\n",
        "## 任务说明\n",
        "\n",
        "本练习要求你：\n",
        "1. 深入理解Adam算法的数学原理和设计思想\n",
        "2. 从零实现完整的Adam优化器类\n",
        "3. 正确实现偏差修正（bias correction）\n",
        "4. 在优化问题上测试Adam优化器\n",
        "5. 对比不同超参数（beta1, beta2）的影响\n",
        "6. 对比Adam与SGD、Momentum的性能差异\n",
        "\n",
        "## 前置知识\n",
        "\n",
        "- 已完成基础练习2和练习3（SGD和Momentum）\n",
        "- 理解梯度下降的基本原理\n",
        "- 理解Momentum算法的思想\n",
        "- 熟悉Python类和对象\n",
        "- 了解NumPy的基本操作\n",
        "\n",
        "## 学习重点\n",
        "\n",
        "- Adam算法的数学公式和物理直觉\n",
        "- 一阶矩估计（m_t）和二阶矩估计（v_t）的作用\n",
        "- 偏差修正的必要性和实现方法\n",
        "- Adam如何结合Momentum和RMSprop的优点\n",
        "- 超参数beta1和beta2的选择\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 设置中文字体，确保图表能正确显示中文\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 设置随机种子，确保结果可复现\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"环境准备完成！\")\n",
        "print(\"Python版本要求：3.7+\")\n",
        "print(\"主要依赖：numpy, matplotlib\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 任务1：深入理解Adam算法\n",
        "\n",
        "### Adam算法原理\n",
        "\n",
        "**Adam（Adaptive Moment Estimation）**是一种结合了Momentum和RMSprop优点的自适应优化算法。\n",
        "\n",
        "### 核心思想\n",
        "\n",
        "1. **一阶矩估计（First Moment）**：类似Momentum，使用历史梯度的指数移动平均\n",
        "   - m_t = β₁ * m_{t-1} + (1 - β₁) * g_t\n",
        "   - 其中 g_t 是当前梯度\n",
        "\n",
        "2. **二阶矩估计（Second Moment）**：类似RMSprop，使用历史梯度平方的指数移动平均\n",
        "   - v_t = β₂ * v_{t-1} + (1 - β₂) * g_t²\n",
        "\n",
        "3. **偏差修正（Bias Correction）**：修正初始化的偏差\n",
        "   - m̂_t = m_t / (1 - β₁^t)\n",
        "   - v̂_t = v_t / (1 - β₂^t)\n",
        "\n",
        "4. **参数更新**：\n",
        "   - θ_{t+1} = θ_t - η * m̂_t / (√v̂_t + ε)\n",
        "   - 其中 η 是学习率，ε 是防止除零的小常数\n",
        "\n",
        "### 算法优势\n",
        "\n",
        "- ✅ **自适应学习率**：每个参数有自己的学习率\n",
        "- ✅ **结合Momentum**：利用历史梯度信息，加速收敛\n",
        "- ✅ **结合RMSprop**：适应不同参数的梯度尺度\n",
        "- ✅ **偏差修正**：解决初始化偏差问题\n",
        "\n",
        "### 超参数说明\n",
        "\n",
        "- **learning_rate (η)**：学习率，通常0.001\n",
        "- **beta1 (β₁)**：一阶矩衰减率，通常0.9\n",
        "- **beta2 (β₂)**：二阶矩衰减率，通常0.999\n",
        "- **epsilon (ε)**：防止除零的小常数，通常1e-8\n",
        "\n",
        "### 任务要求\n",
        "\n",
        "TODO: 阅读并理解上述Adam算法原理，思考以下问题：\n",
        "1. 为什么需要偏差修正？如果不修正会有什么问题？\n",
        "2. 一阶矩估计和二阶矩估计分别解决了什么问题？\n",
        "3. Adam如何结合Momentum和RMSprop的优点？\n",
        "4. 为什么Adam被称为\"自适应\"优化算法？\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 任务2：实现Adam优化器类\n",
        "\n",
        "### 实现要求\n",
        "\n",
        "TODO: 实现一个完整的Adam优化器类，包含以下功能：\n",
        "\n",
        "1. **初始化方法（__init__）**：\n",
        "   - 接收学习率、beta1、beta2、epsilon参数\n",
        "   - 初始化一阶矩估计（m）和二阶矩估计（v）\n",
        "   - 初始化时间步（t）\n",
        "\n",
        "2. **更新方法（update）**：\n",
        "   - 接收当前参数和梯度\n",
        "   - 更新一阶矩估计和二阶矩估计\n",
        "   - 实现偏差修正\n",
        "   - 更新参数\n",
        "   - 返回更新后的参数\n",
        "\n",
        "### 实现要点\n",
        "\n",
        "- **初始化**：m和v初始化为零向量，与参数形状相同\n",
        "- **时间步**：每次update时递增\n",
        "- **偏差修正**：必须正确实现 m̂_t = m_t / (1 - β₁^t)\n",
        "- **数值稳定性**：在计算√v̂_t时加上epsilon\n",
        "- **向量化**：支持多维参数\n",
        "\n",
        "### 提示\n",
        "\n",
        "- 参考公式：\n",
        "  ```\n",
        "  m_t = β₁ * m_{t-1} + (1 - β₁) * g_t\n",
        "  v_t = β₂ * v_{t-1} + (1 - β₂) * g_t²\n",
        "  m̂_t = m_t / (1 - β₁^t)\n",
        "  v̂_t = v_t / (1 - β₂^t)\n",
        "  θ_{t+1} = θ_t - η * m̂_t / (√v̂_t + ε)\n",
        "  ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: 实现Adam优化器类\n",
        "class Adam:\n",
        "    \"\"\"\n",
        "    Adam（Adaptive Moment Estimation）优化器\n",
        "    \n",
        "    参数:\n",
        "        learning_rate (float): 学习率，默认0.001\n",
        "        beta1 (float): 一阶矩衰减率，默认0.9\n",
        "        beta2 (float): 二阶矩衰减率，默认0.999\n",
        "        epsilon (float): 防止除零的小常数，默认1e-8\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        \"\"\"\n",
        "        初始化Adam优化器\n",
        "        \n",
        "        参数:\n",
        "            learning_rate: 学习率，默认0.001\n",
        "            beta1: 一阶矩衰减率，默认0.9\n",
        "            beta2: 二阶矩衰减率，默认0.999\n",
        "            epsilon: 防止除零的小常数，默认1e-8\n",
        "        \"\"\"\n",
        "        # TODO: 保存超参数\n",
        "        # self.lr = learning_rate\n",
        "        # self.beta1 = beta1\n",
        "        # self.beta2 = beta2\n",
        "        # self.epsilon = epsilon\n",
        "        \n",
        "        # TODO: 初始化一阶矩估计和二阶矩估计（初始为None，第一次update时初始化）\n",
        "        # self.m = None  # 一阶矩估计\n",
        "        # self.v = None  # 二阶矩估计\n",
        "        # self.t = 0     # 时间步\n",
        "        pass\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        \"\"\"\n",
        "        更新参数\n",
        "        \n",
        "        参数:\n",
        "            params (numpy.ndarray): 当前参数值\n",
        "            grads (numpy.ndarray): 梯度值\n",
        "        \n",
        "        返回:\n",
        "            numpy.ndarray: 更新后的参数值\n",
        "        \"\"\"\n",
        "        # TODO: 如果是第一次调用，初始化m和v\n",
        "        # if self.m is None:\n",
        "        #     self.m = np.zeros_like(params)\n",
        "        #     self.v = np.zeros_like(params)\n",
        "        \n",
        "        # TODO: 增加时间步\n",
        "        # self.t += 1\n",
        "        \n",
        "        # TODO: 更新一阶矩估计（指数移动平均）\n",
        "        # self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
        "        \n",
        "        # TODO: 更新二阶矩估计（指数移动平均）\n",
        "        # self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2\n",
        "        \n",
        "        # TODO: 偏差修正\n",
        "        # m_hat = self.m / (1 - self.beta1**self.t)\n",
        "        # v_hat = self.v / (1 - self.beta2**self.t)\n",
        "        \n",
        "        # TODO: 更新参数（注意数值稳定性：在√v_hat时加上epsilon）\n",
        "        # params_new = params - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "        # return params_new\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 任务3：定义测试问题\n",
        "\n",
        "### 测试函数：Rosenbrock函数\n",
        "\n",
        "我们将使用经典的Rosenbrock函数来测试Adam优化器：\n",
        "\n",
        "**Rosenbrock函数**：f(x, y) = (a - x)² + b * (y - x²)²\n",
        "- 全局最小值在 (a, a²) 处，值为 0\n",
        "- 这里我们设置 a=1, b=100\n",
        "- 最优解在 (1, 1) 处\n",
        "\n",
        "这个函数的特点是有一个狭窄的\"山谷\"，对优化算法有挑战性，适合测试Adam的自适应能力。\n",
        "\n",
        "### 任务要求\n",
        "\n",
        "TODO: 实现损失函数和梯度函数\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
