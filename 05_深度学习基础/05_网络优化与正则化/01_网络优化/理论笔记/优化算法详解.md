# 优化算法详解

## 1. 优化问题的本质

### 1.1 优化目标

神经网络的训练本质上是一个优化问题：

$$\min_{\theta} L(\theta) = \frac{1}{N}\sum_{i=1}^{N} \ell(f(x_i; \theta), y_i)$$

其中：
- $\theta$：模型参数（权重和偏置）
- $L(\theta)$：损失函数
- $f(x_i; \theta)$：模型在输入$x_i$下的输出
- $y_i$：真实标签

### 1.2 优化难点

1. **非凸性**：损失函数通常是非凸的，存在多个局部最优
2. **高维性**：参数空间维度极高（百万到数十亿）
3. **非光滑性**：某些激活函数（如ReLU）不可微
4. **数据量大**：无法一次性处理所有数据

## 2. 梯度下降算法族

### 2.1 批量梯度下降（BGD）

**更新规则**：
$$\theta_{t+1} = \theta_t - \eta \nabla_{\theta} L(\theta_t)$$

其中：
- $L(\theta) = \frac{1}{N}\sum_{i=1}^{N} \ell(f(x_i; \theta), y_i)$ 使用所有训练样本
- $\eta$：学习率

**特点**：
- ✅ 梯度估计准确
- ✅ 收敛稳定
- ❌ 计算慢（需要处理所有样本）
- ❌ 内存需求大
- ❌ 无法处理在线学习

### 2.2 随机梯度下降（SGD）

**更新规则**：
$$\theta_{t+1} = \theta_t - \eta \nabla_{\theta} \ell(f(x_i; \theta_t), y_i)$$

每次只使用一个随机样本。

**特点**：
- ✅ 计算快
- ✅ 内存需求小
- ✅ 可能跳出局部最优（噪声帮助）
- ❌ 梯度估计不准确（噪声大）
- ❌ 收敛不稳定
- ❌ 需要更多迭代次数

### 2.3 小批量梯度下降（Mini-batch SGD）

**更新规则**：
$$\theta_{t+1} = \theta_t - \eta \frac{1}{b}\sum_{i=1}^{b} \nabla_{\theta} \ell(f(x_{i}; \theta_t), y_i)$$

其中$b$是批量大小（batch size），通常为32、64、128等。

**特点**：
- ✅ 平衡了BGD和SGD的优点
- ✅ 梯度估计较准确
- ✅ 计算效率高
- ✅ **最常用的方法**

**批量大小选择**：
- **小批量（1-32）**：梯度估计不准确，但可能跳出局部最优
- **中等批量（32-256）**：平衡准确性和效率（推荐）
- **大批量（256+）**：梯度估计准确，但可能陷入局部最优

## 3. 改进的优化算法

### 3.1 Momentum（动量法）

**核心思想**：使用历史梯度的移动平均，加速收敛并减少震荡。

**更新规则**：
$$v_t = \beta v_{t-1} + (1-\beta) \nabla_{\theta} L(\theta_t)$$
$$\theta_{t+1} = \theta_t - \eta v_t$$

其中：
- $v_t$：动量项（速度）
- $\beta$：动量系数（通常0.9）

**物理类比**：
- 想象一个小球在损失函数曲面上滚动
- 动量使小球能够"冲过"局部最小值
- 减少在梯度方向上的震荡

**优点**：
- ✅ 加速收敛
- ✅ 减少震荡
- ✅ 有助于跳出局部最优

### 3.2 Nesterov Accelerated Gradient (NAG)

**核心思想**：在计算梯度时，先"向前看"一步。

**更新规则**：
$$v_t = \beta v_{t-1} + (1-\beta) \nabla_{\theta} L(\theta_t - \beta v_{t-1})$$
$$\theta_{t+1} = \theta_t - \eta v_t$$

**与Momentum的区别**：
- Momentum：在当前点计算梯度
- NAG：在"向前看"的点计算梯度

**优点**：
- ✅ 比Momentum收敛更快
- ✅ 对凸函数有理论保证

### 3.3 AdaGrad（Adaptive Gradient）

**核心思想**：根据历史梯度平方和自适应调整学习率。

**更新规则**：
$$G_t = G_{t-1} + (\nabla_{\theta} L(\theta_t))^2$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_{\theta} L(\theta_t)$$

其中：
- $G_t$：历史梯度平方的累积
- $\epsilon$：小常数（防止除零，通常$10^{-8}$）

**特点**：
- ✅ 自动调整学习率
- ✅ 对稀疏梯度效果好
- ❌ 学习率会逐渐变小（可能过早停止）
- ❌ 需要累积历史梯度平方（内存需求大）

### 3.4 RMSprop

**核心思想**：使用移动平均代替AdaGrad的累积，解决学习率衰减问题。

**更新规则**：
$$G_t = \beta G_{t-1} + (1-\beta)(\nabla_{\theta} L(\theta_t))^2$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_{\theta} L(\theta_t)$$

其中$\beta$通常为0.9。

**优点**：
- ✅ 解决AdaGrad的学习率衰减问题
- ✅ 对非平稳目标效果好
- ✅ 适合RNN训练

### 3.5 Adam（Adaptive Moment Estimation）

**核心思想**：结合Momentum和RMSprop，同时使用一阶矩和二阶矩的移动平均。

**更新规则**：
$$m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla_{\theta} L(\theta_t)$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2)(\nabla_{\theta} L(\theta_t))^2$$
$$\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

其中：
- $m_t$：一阶矩估计（梯度的移动平均）
- $v_t$：二阶矩估计（梯度平方的移动平均）
- $\beta_1$：通常0.9
- $\beta_2$：通常0.999
- $\hat{m}_t, \hat{v}_t$：偏差修正项

**优点**：
- ✅ 自适应学习率
- ✅ 对超参数不敏感
- ✅ **最流行的优化算法**
- ✅ 适合大多数任务

**缺点**：
- ❌ 在某些任务上可能不如SGD+Momentum
- ❌ 需要存储两个移动平均（内存需求）

### 3.6 AdamW

**核心思想**：Adam的改进版本，将权重衰减（weight decay）从梯度中分离出来。

**更新规则**：
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} (\hat{m}_t + \lambda \theta_t)$$

其中$\lambda$是权重衰减系数。

**优点**：
- ✅ 权重衰减更有效
- ✅ 在某些任务上比Adam更好

## 4. 学习率调整策略

### 4.1 固定学习率

最简单的方法，但通常不是最优的。

### 4.2 学习率衰减

**固定衰减**：
$$\eta_t = \eta_0 \times \text{decay}^{\lfloor t/\text{step} \rfloor}$$

**指数衰减**：
$$\eta_t = \eta_0 \times e^{-kt}$$

**多项式衰减**：
$$\eta_t = \eta_0 \times (1 - \frac{t}{T})^p$$

### 4.3 余弦退火（Cosine Annealing）

$$\eta_t = \eta_{\min} + (\eta_{\max} - \eta_{\min}) \times \frac{1 + \cos(\pi t / T)}{2}$$

**优点**：
- ✅ 平滑的学习率变化
- ✅ 在训练后期有小的学习率（精细调优）

### 4.4 自适应学习率

- **AdaGrad**：根据历史梯度调整
- **RMSprop**：使用移动平均
- **Adam**：结合一阶和二阶矩

## 5. 优化算法选择指南

| 算法 | 适用场景 | 优点 | 缺点 |
|------|---------|------|------|
| SGD | 凸优化、简单任务 | 简单、理论保证 | 收敛慢、需要调参 |
| Momentum | 需要加速收敛 | 加速、减少震荡 | 需要调参 |
| Adam | 大多数深度学习任务 | 自适应、效果好 | 内存需求大 |
| RMSprop | RNN、非平稳目标 | 适合非平稳 | 需要调参 |

## 6. 实践建议

1. **默认选择**：Adam（适合大多数情况）
2. **如果Adam效果不好**：尝试SGD+Momentum
3. **学习率**：Adam通常0.001，SGD需要更仔细调参
4. **批量大小**：32-256之间，根据GPU内存选择
5. **学习率衰减**：使用余弦退火或固定衰减

## 7. 总结

优化算法是神经网络训练的核心，选择合适的算法和超参数对训练效果至关重要。理解不同算法的原理，能够帮助我们做出更好的选择。

---

**继续学习，掌握更多优化技巧！** 🚀

