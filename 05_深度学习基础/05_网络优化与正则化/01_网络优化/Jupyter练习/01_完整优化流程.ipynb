{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jupyterç»ƒä¹ 1ï¼šå®Œæ•´ä¼˜åŒ–æµç¨‹\n",
        "\n",
        "## ğŸ“š ä»£ç ç¤ºä¾‹å…³è”\n",
        "\n",
        "**æœ¬ç¤ºä¾‹ä½ç½®**ï¼šè¿™æ˜¯ç½‘ç»œä¼˜åŒ–çš„ç»¼åˆç»ƒä¹ \n",
        "\n",
        "**åŸºäº**ï¼š\n",
        "- [ä»é›¶å®ç°ä¼˜åŒ–ç®—æ³•](../ä»£ç ç¤ºä¾‹/01_ä»é›¶å®ç°ä¼˜åŒ–ç®—æ³•.ipynb) - ç†è§£ä¼˜åŒ–ç®—æ³•åŸç†\n",
        "- [ä»é›¶å®ç°å‰é¦ˆç¥ç»ç½‘ç»œ](../../01_ç¥ç»ç½‘ç»œåŸºç¡€/02_å‰é¦ˆç¥ç»ç½‘ç»œ/ä»£ç ç¤ºä¾‹/01_ä»é›¶å®ç°å‰é¦ˆç¥ç»ç½‘ç»œ.ipynb) - ç†è§£ç½‘ç»œè®­ç»ƒæµç¨‹\n",
        "\n",
        "**æ‰©å±•ä¸º**ï¼š\n",
        "- [PyTorchåŸºç¡€å…¥é—¨](../../04_PyTorch_TensorFlow/01_PyTorchåŸºç¡€/ä»£ç ç¤ºä¾‹/01_PyTorchåŸºç¡€å…¥é—¨.ipynb) - ä½¿ç”¨æ¡†æ¶çš„ä¼˜åŒ–å™¨\n",
        "- [è¶…å‚æ•°ä¼˜åŒ–](../05_è¶…å‚æ•°ä¼˜åŒ–/ä»£ç ç¤ºä¾‹/) - ä¼˜åŒ–ç®—æ³•çš„è¶…å‚æ•°è°ƒä¼˜\n",
        "- [ç»¼åˆå®æˆ˜é¡¹ç›®](../../08_ç»¼åˆå®æˆ˜é¡¹ç›®/) - å®Œæ•´é¡¹ç›®ä¸­çš„ä¼˜åŒ–åº”ç”¨\n",
        "\n",
        "**ç›¸å…³ç¤ºä¾‹**ï¼š\n",
        "- [å®ç°SGDä¼˜åŒ–å™¨](../ç»ƒä¹ é¢˜/åŸºç¡€ç»ƒä¹ /ç»ƒä¹ 2_å®ç°SGDä¼˜åŒ–å™¨.ipynb) - åŸºç¡€ç»ƒä¹ \n",
        "- [å®ç°Momentumä¼˜åŒ–å™¨](../ç»ƒä¹ é¢˜/åŸºç¡€ç»ƒä¹ /ç»ƒä¹ 3_å®ç°Momentumä¼˜åŒ–å™¨.ipynb) - åŸºç¡€ç»ƒä¹ \n",
        "- [å®ç°Adamä¼˜åŒ–å™¨](../ç»ƒä¹ é¢˜/è¿›é˜¶ç»ƒä¹ /ç»ƒä¹ 1_å®ç°Adamä¼˜åŒ–å™¨.ipynb) - è¿›é˜¶ç»ƒä¹ \n",
        "- [æ‰¹é‡å¤§å°å½±å“åˆ†æ](../ç»ƒä¹ é¢˜/åŸºç¡€ç»ƒä¹ /ç»ƒä¹ 4_æ‰¹é‡å¤§å°å½±å“åˆ†æ.ipynb) - æ‰¹é‡å¤§å°åˆ†æ\n",
        "\n",
        "---\n",
        "\n",
        "## å­¦ä¹ ç›®æ ‡\n",
        "\n",
        "é€šè¿‡æœ¬ç»ƒä¹ ï¼Œä½ å°†å®Œæ•´åœ°å®ç°ä¸€ä¸ªä¼˜åŒ–æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š\n",
        "1. å®šä¹‰ä¼˜åŒ–é—®é¢˜\n",
        "2. å®ç°ä¼˜åŒ–ç®—æ³•\n",
        "3. è®­ç»ƒå’Œè¯„ä¼°\n",
        "4. ç»“æœåˆ†æ\n",
        "\n",
        "## ä»»åŠ¡è¯´æ˜\n",
        "\n",
        "æœ¬ç»ƒä¹ æ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„å®è·µé¡¹ç›®ï¼Œéœ€è¦ä½ å®Œæˆæ‰€æœ‰TODOéƒ¨åˆ†ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œç¡®ä¿å›¾è¡¨èƒ½æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n",
        "print(\"Pythonç‰ˆæœ¬è¦æ±‚ï¼š3.7+\")\n",
        "print(\"ä¸»è¦ä¾èµ–ï¼šnumpy, matplotlib\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "np.random.seed(42)\n",
        "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤1ï¼šå®šä¹‰ä¼˜åŒ–é—®é¢˜\n",
        "\n",
        "TODO: å®šä¹‰ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼ˆæŸå¤±å‡½æ•°å’Œæ¢¯åº¦å‡½æ•°ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®šä¹‰æŸå¤±å‡½æ•° - ä½¿ç”¨Rosenbrockå‡½æ•°ï¼ˆç»å…¸çš„ä¼˜åŒ–æµ‹è¯•å‡½æ•°ï¼‰\n",
        "def loss_function(params):\n",
        "    \"\"\"\n",
        "    Rosenbrockå‡½æ•°: f(x,y) = (a-x)^2 + b*(y-x^2)^2\n",
        "    å…¨å±€æœ€å°å€¼åœ¨ (a, a^2) å¤„ï¼Œå€¼ä¸º0\n",
        "    \n",
        "    æ”¯æŒå‘é‡åŒ–è®¡ç®—ï¼šparamså¯ä»¥æ˜¯å•ä¸ªæ•°ç»„æˆ–äºŒç»´æ•°ç»„\n",
        "    \"\"\"\n",
        "    a, b = 1, 100\n",
        "    if params.ndim == 1:\n",
        "        x, y = params[0], params[1]\n",
        "        return (a - x)**2 + b * (y - x**2)**2\n",
        "    else:\n",
        "        # å‘é‡åŒ–ç‰ˆæœ¬ï¼Œç”¨äºç­‰é«˜çº¿è®¡ç®—\n",
        "        x, y = params[..., 0], params[..., 1]\n",
        "        return (a - x)**2 + b * (y - x**2)**2\n",
        "\n",
        "# å®šä¹‰æ¢¯åº¦å‡½æ•°\n",
        "def compute_gradients(params):\n",
        "    \"\"\"\n",
        "    è®¡ç®—Rosenbrockå‡½æ•°çš„æ¢¯åº¦\n",
        "    \"\"\"\n",
        "    a, b = 1, 100\n",
        "    x, y = params[0], params[1]\n",
        "    \n",
        "    # å¯¹xçš„åå¯¼æ•°\n",
        "    df_dx = -2 * (a - x) - 4 * b * x * (y - x**2)\n",
        "    # å¯¹yçš„åå¯¼æ•°\n",
        "    df_dy = 2 * b * (y - x**2)\n",
        "    \n",
        "    return np.array([df_dx, df_dy])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤2ï¼šå®ç°ä¼˜åŒ–ç®—æ³•\n",
        "\n",
        "TODO: å®ç°è‡³å°‘ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼ˆSGDã€Momentumæˆ–Adamï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®ç°ä¼˜åŒ–ç®—æ³•ç±»\n",
        "class SGD:\n",
        "    \"\"\"éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨\"\"\"\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.lr = learning_rate\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        return params - self.lr * grads\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "    \"\"\"å¸¦åŠ¨é‡çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.lr = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = np.zeros_like(params)\n",
        "        \n",
        "        self.v = self.momentum * self.v - self.lr * grads\n",
        "        return params + self.v\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    \"\"\"Adamä¼˜åŒ–å™¨\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None  # ä¸€é˜¶çŸ©ä¼°è®¡\n",
        "        self.v = None  # äºŒé˜¶çŸ©ä¼°è®¡\n",
        "        self.t = 0     # æ—¶é—´æ­¥\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m = np.zeros_like(params)\n",
        "            self.v = np.zeros_like(params)\n",
        "        \n",
        "        self.t += 1\n",
        "        \n",
        "        # æ›´æ–°ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡\n",
        "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
        "        self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2\n",
        "        \n",
        "        # åå·®ä¿®æ­£\n",
        "        m_hat = self.m / (1 - self.beta1**self.t)\n",
        "        v_hat = self.v / (1 - self.beta2**self.t)\n",
        "        \n",
        "        # æ›´æ–°å‚æ•°\n",
        "        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤3ï¼šè®­ç»ƒè¿‡ç¨‹\n",
        "\n",
        "TODO: å®ç°è®­ç»ƒå¾ªç¯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆå§‹åŒ–å‚æ•° - ä»è¿œç¦»æœ€ä¼˜è§£çš„ä½ç½®å¼€å§‹\n",
        "params_sgd = np.array([-1.5, 2.0])\n",
        "params_momentum = np.array([-1.5, 2.0])\n",
        "params_adam = np.array([-1.5, 2.0])\n",
        "\n",
        "# åˆ›å»ºä¸åŒçš„ä¼˜åŒ–å™¨\n",
        "optimizer_sgd = SGD(learning_rate=0.001)\n",
        "optimizer_momentum = Momentum(learning_rate=0.001, momentum=0.9)\n",
        "optimizer_adam = Adam(learning_rate=0.01, beta1=0.9, beta2=0.999)\n",
        "\n",
        "# è®­ç»ƒå‚æ•°\n",
        "n_epochs = 1000\n",
        "\n",
        "# è®°å½•è®­ç»ƒè¿‡ç¨‹\n",
        "losses_sgd = []\n",
        "losses_momentum = []\n",
        "losses_adam = []\n",
        "params_history_sgd = [params_sgd.copy()]\n",
        "params_history_momentum = [params_momentum.copy()]\n",
        "params_history_adam = [params_adam.copy()]\n",
        "\n",
        "# å®ç°è®­ç»ƒå¾ªç¯ - SGD\n",
        "print(\"å¼€å§‹è®­ç»ƒ SGD...\")\n",
        "for epoch in range(n_epochs):\n",
        "    # è®¡ç®—æ¢¯åº¦\n",
        "    grads = compute_gradients(params_sgd)\n",
        "    \n",
        "    # æ›´æ–°å‚æ•°\n",
        "    params_sgd = optimizer_sgd.update(params_sgd, grads)\n",
        "    \n",
        "    # è®°å½•æŸå¤±å’Œå‚æ•°\n",
        "    loss = loss_function(params_sgd)\n",
        "    losses_sgd.append(loss)\n",
        "    params_history_sgd.append(params_sgd.copy())\n",
        "    \n",
        "    if (epoch + 1) % 200 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss:.6f}, Params: [{params_sgd[0]:.4f}, {params_sgd[1]:.4f}]\")\n",
        "\n",
        "# å®ç°è®­ç»ƒå¾ªç¯ - Momentum\n",
        "print(\"\\nå¼€å§‹è®­ç»ƒ Momentum...\")\n",
        "for epoch in range(n_epochs):\n",
        "    grads = compute_gradients(params_momentum)\n",
        "    params_momentum = optimizer_momentum.update(params_momentum, grads)\n",
        "    loss = loss_function(params_momentum)\n",
        "    losses_momentum.append(loss)\n",
        "    params_history_momentum.append(params_momentum.copy())\n",
        "    \n",
        "    if (epoch + 1) % 200 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss:.6f}, Params: [{params_momentum[0]:.4f}, {params_momentum[1]:.4f}]\")\n",
        "\n",
        "# å®ç°è®­ç»ƒå¾ªç¯ - Adam\n",
        "print(\"\\nå¼€å§‹è®­ç»ƒ Adam...\")\n",
        "for epoch in range(n_epochs):\n",
        "    grads = compute_gradients(params_adam)\n",
        "    params_adam = optimizer_adam.update(params_adam, grads)\n",
        "    loss = loss_function(params_adam)\n",
        "    losses_adam.append(loss)\n",
        "    params_history_adam.append(params_adam.copy())\n",
        "    \n",
        "    if (epoch + 1) % 200 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss:.6f}, Params: [{params_adam[0]:.4f}, {params_adam[1]:.4f}]\")\n",
        "\n",
        "print(\"\\nè®­ç»ƒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤4ï¼šç»“æœåˆ†æå’Œå¯è§†åŒ–\n",
        "\n",
        "TODO: å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹å’Œç»“æœ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses_sgd, label='SGD', alpha=0.7)\n",
        "plt.plot(losses_momentum, label='Momentum', alpha=0.7)\n",
        "plt.plot(losses_adam, label='Adam', alpha=0.7)\n",
        "plt.xlabel('è¿­ä»£æ¬¡æ•°')\n",
        "plt.ylabel('æŸå¤±å€¼')\n",
        "plt.title('æŸå¤±æ›²çº¿å¯¹æ¯”')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦ä»¥ä¾¿æ›´å¥½åœ°è§‚å¯Ÿ\n",
        "\n",
        "# å¯è§†åŒ–ä¼˜åŒ–è·¯å¾„ï¼ˆäºŒç»´é—®é¢˜ï¼‰\n",
        "plt.subplot(1, 2, 2)\n",
        "# åˆ›å»ºç­‰é«˜çº¿å›¾ï¼ˆä½¿ç”¨å‘é‡åŒ–è®¡ç®—æé«˜æ•ˆç‡ï¼‰\n",
        "x = np.linspace(-2, 2, 100)\n",
        "y = np.linspace(-1, 3, 100)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "# å‘é‡åŒ–è®¡ç®—æŸå¤±å‡½æ•°å€¼\n",
        "params_grid = np.stack([X, Y], axis=-1)\n",
        "Z = loss_function(params_grid)\n",
        "\n",
        "# ç»˜åˆ¶ç­‰é«˜çº¿\n",
        "contour = plt.contour(X, Y, Z, levels=20, alpha=0.5)\n",
        "plt.clabel(contour, inline=True, fontsize=8)\n",
        "\n",
        "# ç»˜åˆ¶ä¼˜åŒ–è·¯å¾„\n",
        "params_history_sgd = np.array(params_history_sgd)\n",
        "params_history_momentum = np.array(params_history_momentum)\n",
        "params_history_adam = np.array(params_history_adam)\n",
        "\n",
        "plt.plot(params_history_sgd[:, 0], params_history_sgd[:, 1], 'o-', \n",
        "         label='SGDè·¯å¾„', markersize=3, alpha=0.6, linewidth=1.5)\n",
        "plt.plot(params_history_momentum[:, 0], params_history_momentum[:, 1], 's-', \n",
        "         label='Momentumè·¯å¾„', markersize=3, alpha=0.6, linewidth=1.5)\n",
        "plt.plot(params_history_adam[:, 0], params_history_adam[:, 1], '^-', \n",
        "         label='Adamè·¯å¾„', markersize=3, alpha=0.6, linewidth=1.5)\n",
        "\n",
        "# æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹\n",
        "plt.plot(params_history_sgd[0, 0], params_history_sgd[0, 1], 'ko', \n",
        "         markersize=10, label='èµ·ç‚¹')\n",
        "plt.plot(1, 1, 'r*', markersize=15, label='æœ€ä¼˜è§£ (1, 1)')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('ä¼˜åŒ–è·¯å¾„å¯è§†åŒ–')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# åˆ†æç»“æœ\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ä¼˜åŒ–ç»“æœåˆ†æ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nSGDä¼˜åŒ–å™¨:\")\n",
        "print(f\"  æœ€ç»ˆå‚æ•°: [{params_sgd[0]:.6f}, {params_sgd[1]:.6f}]\")\n",
        "print(f\"  æœ€ç»ˆæŸå¤±: {losses_sgd[-1]:.6f}\")\n",
        "print(f\"  æœ€ä¼˜è§£: [1.0, 1.0], æœ€ä¼˜æŸå¤±: 0.0\")\n",
        "print(f\"  è¯¯å·®: {np.linalg.norm(params_sgd - np.array([1.0, 1.0])):.6f}\")\n",
        "\n",
        "print(f\"\\nMomentumä¼˜åŒ–å™¨:\")\n",
        "print(f\"  æœ€ç»ˆå‚æ•°: [{params_momentum[0]:.6f}, {params_momentum[1]:.6f}]\")\n",
        "print(f\"  æœ€ç»ˆæŸå¤±: {losses_momentum[-1]:.6f}\")\n",
        "print(f\"  è¯¯å·®: {np.linalg.norm(params_momentum - np.array([1.0, 1.0])):.6f}\")\n",
        "\n",
        "print(f\"\\nAdamä¼˜åŒ–å™¨:\")\n",
        "print(f\"  æœ€ç»ˆå‚æ•°: [{params_adam[0]:.6f}, {params_adam[1]:.6f}]\")\n",
        "print(f\"  æœ€ç»ˆæŸå¤±: {losses_adam[-1]:.6f}\")\n",
        "print(f\"  è¯¯å·®: {np.linalg.norm(params_adam - np.array([1.0, 1.0])):.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"æ€§èƒ½å¯¹æ¯”:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  æœ€å¿«æ”¶æ•›: \", end=\"\")\n",
        "if min(losses_sgd[-1], losses_momentum[-1], losses_adam[-1]) == losses_adam[-1]:\n",
        "    print(\"Adam\")\n",
        "elif min(losses_sgd[-1], losses_momentum[-1], losses_adam[-1]) == losses_momentum[-1]:\n",
        "    print(\"Momentum\")\n",
        "else:\n",
        "    print(\"SGD\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ‰©å±•ï¼šå­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥\n",
        "\n",
        "ä¸‹é¢æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡ç­–ç•¥æ¥æ”¹è¿›ä¼˜åŒ–è¿‡ç¨‹\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®ç°å¸¦å­¦ä¹ ç‡è¡°å‡çš„SGDä¼˜åŒ–å™¨\n",
        "class SGDWithDecay:\n",
        "    \"\"\"å¸¦å­¦ä¹ ç‡è¡°å‡çš„éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, decay_rate=0.95, decay_steps=100):\n",
        "        self.initial_lr = learning_rate\n",
        "        self.lr = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.step = 0\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        # æ¯decay_stepsæ­¥è¡°å‡ä¸€æ¬¡å­¦ä¹ ç‡\n",
        "        if self.step % self.decay_steps == 0 and self.step > 0:\n",
        "            self.lr *= self.decay_rate\n",
        "        self.step += 1\n",
        "        return params - self.lr * grads\n",
        "    \n",
        "    def get_lr(self):\n",
        "        return self.lr\n",
        "\n",
        "\n",
        "# ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡ç­–ç•¥é‡æ–°è®­ç»ƒ\n",
        "print(\"=\"*60)\n",
        "print(\"æ‰©å±•å®éªŒï¼šå­¦ä¹ ç‡è¡°å‡ç­–ç•¥\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "params_decay = np.array([-1.5, 2.0])\n",
        "optimizer_decay = SGDWithDecay(learning_rate=0.01, decay_rate=0.95, decay_steps=100)\n",
        "\n",
        "losses_decay = []\n",
        "lr_history = []\n",
        "params_history_decay = [params_decay.copy()]\n",
        "\n",
        "print(\"\\nå¼€å§‹è®­ç»ƒ SGD with Learning Rate Decay...\")\n",
        "for epoch in range(n_epochs):\n",
        "    grads = compute_gradients(params_decay)\n",
        "    params_decay = optimizer_decay.update(params_decay, grads)\n",
        "    loss = loss_function(params_decay)\n",
        "    losses_decay.append(loss)\n",
        "    lr_history.append(optimizer_decay.get_lr())\n",
        "    params_history_decay.append(params_decay.copy())\n",
        "    \n",
        "    if (epoch + 1) % 200 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss:.6f}, LR: {optimizer_decay.get_lr():.6f}, \"\n",
        "              f\"Params: [{params_decay[0]:.4f}, {params_decay[1]:.4f}]\")\n",
        "\n",
        "print(\"\\nè®­ç»ƒå®Œæˆï¼\")\n",
        "\n",
        "# å¯¹æ¯”å›ºå®šå­¦ä¹ ç‡å’Œè¡°å‡å­¦ä¹ ç‡\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(losses_sgd, label='SGD (å›ºå®šLR)', alpha=0.7)\n",
        "plt.plot(losses_decay, label='SGD (è¡°å‡LR)', alpha=0.7)\n",
        "plt.xlabel('è¿­ä»£æ¬¡æ•°')\n",
        "plt.ylabel('æŸå¤±å€¼')\n",
        "plt.title('å­¦ä¹ ç‡è¡°å‡æ•ˆæœå¯¹æ¯”')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(lr_history, label='å­¦ä¹ ç‡å˜åŒ–', color='orange')\n",
        "plt.xlabel('è¿­ä»£æ¬¡æ•°')\n",
        "plt.ylabel('å­¦ä¹ ç‡')\n",
        "plt.title('å­¦ä¹ ç‡è¡°å‡æ›²çº¿')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "# ç»˜åˆ¶ä¼˜åŒ–è·¯å¾„å¯¹æ¯”\n",
        "x = np.linspace(-2, 2, 100)\n",
        "y = np.linspace(-1, 3, 100)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "params_grid = np.stack([X, Y], axis=-1)\n",
        "Z = loss_function(params_grid)\n",
        "\n",
        "contour = plt.contour(X, Y, Z, levels=20, alpha=0.3)\n",
        "params_history_sgd = np.array(params_history_sgd)\n",
        "params_history_decay = np.array(params_history_decay)\n",
        "\n",
        "plt.plot(params_history_sgd[:, 0], params_history_sgd[:, 1], 'o-', \n",
        "         label='SGD (å›ºå®šLR)', markersize=2, alpha=0.6, linewidth=1)\n",
        "plt.plot(params_history_decay[:, 0], params_history_decay[:, 1], 's-', \n",
        "         label='SGD (è¡°å‡LR)', markersize=2, alpha=0.6, linewidth=1)\n",
        "plt.plot(params_history_sgd[0, 0], params_history_sgd[0, 1], 'ko', \n",
        "         markersize=8, label='èµ·ç‚¹')\n",
        "plt.plot(1, 1, 'r*', markersize=12, label='æœ€ä¼˜è§£')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('ä¼˜åŒ–è·¯å¾„å¯¹æ¯”')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ç»“æœå¯¹æ¯”\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"å­¦ä¹ ç‡è¡°å‡ç­–ç•¥æ•ˆæœåˆ†æ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nå›ºå®šå­¦ä¹ ç‡ SGD:\")\n",
        "print(f\"  æœ€ç»ˆæŸå¤±: {losses_sgd[-1]:.6f}\")\n",
        "print(f\"  è¯¯å·®: {np.linalg.norm(params_sgd - np.array([1.0, 1.0])):.6f}\")\n",
        "\n",
        "print(f\"\\nå­¦ä¹ ç‡è¡°å‡ SGD:\")\n",
        "print(f\"  æœ€ç»ˆæŸå¤±: {losses_decay[-1]:.6f}\")\n",
        "print(f\"  è¯¯å·®: {np.linalg.norm(params_decay - np.array([1.0, 1.0])):.6f}\")\n",
        "print(f\"  æœ€ç»ˆå­¦ä¹ ç‡: {optimizer_decay.get_lr():.6f}\")\n",
        "\n",
        "improvement = ((losses_sgd[-1] - losses_decay[-1]) / losses_sgd[-1]) * 100\n",
        "print(f\"\\næ”¹è¿›å¹…åº¦: {improvement:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ‰©å±•ï¼šä¸åŒå­¦ä¹ ç‡çš„å½±å“\n",
        "\n",
        "ä¸‹é¢é€šè¿‡å®éªŒæ¥è§‚å¯Ÿä¸åŒå­¦ä¹ ç‡å¯¹SGDä¼˜åŒ–è¿‡ç¨‹çš„å½±å“\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æµ‹è¯•ä¸åŒå­¦ä¹ ç‡çš„å½±å“\n",
        "print(\"=\"*60)\n",
        "print(\"æ‰©å±•å®éªŒï¼šä¸åŒå­¦ä¹ ç‡çš„å½±å“\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "n_epochs_lr = 500  # ä½¿ç”¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°ä»¥ä¾¿è§‚å¯Ÿå·®å¼‚\n",
        "\n",
        "results = {}\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\næµ‹è¯•å­¦ä¹ ç‡: {lr}\")\n",
        "    params = np.array([-1.5, 2.0])\n",
        "    optimizer = SGD(learning_rate=lr)\n",
        "    losses = []\n",
        "    params_history = [params.copy()]\n",
        "    \n",
        "    for epoch in range(n_epochs_lr):\n",
        "        grads = compute_gradients(params)\n",
        "        params = optimizer.update(params, grads)\n",
        "        loss = loss_function(params)\n",
        "        losses.append(loss)\n",
        "        params_history.append(params.copy())\n",
        "    \n",
        "    results[lr] = {\n",
        "        'losses': losses,\n",
        "        'params_history': np.array(params_history),\n",
        "        'final_loss': losses[-1],\n",
        "        'final_params': params,\n",
        "        'error': np.linalg.norm(params - np.array([1.0, 1.0]))\n",
        "    }\n",
        "    print(f\"  æœ€ç»ˆæŸå¤±: {losses[-1]:.6f}\")\n",
        "    print(f\"  æœ€ç»ˆå‚æ•°: [{params[0]:.4f}, {params[1]:.4f}]\")\n",
        "    print(f\"  è¯¯å·®: {results[lr]['error']:.6f}\")\n",
        "\n",
        "# å¯è§†åŒ–ä¸åŒå­¦ä¹ ç‡çš„æ•ˆæœ\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "# 1. æŸå¤±æ›²çº¿å¯¹æ¯”\n",
        "plt.subplot(1, 3, 1)\n",
        "colors = ['blue', 'green', 'orange', 'red']\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    plt.plot(results[lr]['losses'], label=f'LR={lr}', \n",
        "             color=colors[i], alpha=0.7, linewidth=2)\n",
        "plt.xlabel('è¿­ä»£æ¬¡æ•°')\n",
        "plt.ylabel('æŸå¤±å€¼')\n",
        "plt.title('ä¸åŒå­¦ä¹ ç‡çš„æŸå¤±æ›²çº¿')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "\n",
        "# 2. ä¼˜åŒ–è·¯å¾„å¯¹æ¯”\n",
        "plt.subplot(1, 3, 2)\n",
        "# ç»˜åˆ¶ç­‰é«˜çº¿\n",
        "x = np.linspace(-2, 2, 100)\n",
        "y = np.linspace(-1, 3, 100)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "params_grid = np.stack([X, Y], axis=-1)\n",
        "Z = loss_function(params_grid)\n",
        "contour = plt.contour(X, Y, Z, levels=20, alpha=0.3)\n",
        "\n",
        "# ç»˜åˆ¶ä¸åŒå­¦ä¹ ç‡çš„ä¼˜åŒ–è·¯å¾„\n",
        "markers = ['o', 's', '^', 'D']\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    params_hist = results[lr]['params_history']\n",
        "    # åªç»˜åˆ¶æ¯10ä¸ªç‚¹ï¼Œé¿å…è·¯å¾„è¿‡äºå¯†é›†\n",
        "    plt.plot(params_hist[::10, 0], params_hist[::10, 1], \n",
        "             marker=markers[i], markersize=4, alpha=0.6, \n",
        "             linewidth=1.5, label=f'LR={lr}')\n",
        "\n",
        "plt.plot(-1.5, 2.0, 'ko', markersize=10, label='èµ·ç‚¹')\n",
        "plt.plot(1, 1, 'r*', markersize=15, label='æœ€ä¼˜è§£')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('ä¸åŒå­¦ä¹ ç‡çš„ä¼˜åŒ–è·¯å¾„')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axis('equal')\n",
        "\n",
        "# 3. æ”¶æ•›é€Ÿåº¦å¯¹æ¯”ï¼ˆè¾¾åˆ°æŸä¸ªæŸå¤±é˜ˆå€¼æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ï¼‰\n",
        "plt.subplot(1, 3, 3)\n",
        "thresholds = [100, 10, 1, 0.1, 0.01]\n",
        "convergence_data = {lr: [] for lr in learning_rates}\n",
        "\n",
        "for threshold in thresholds:\n",
        "    for lr in learning_rates:\n",
        "        losses = results[lr]['losses']\n",
        "        # æ‰¾åˆ°é¦–æ¬¡ä½äºé˜ˆå€¼çš„è¿­ä»£æ¬¡æ•°\n",
        "        converged = False\n",
        "        for epoch, loss in enumerate(losses):\n",
        "            if loss < threshold:\n",
        "                convergence_data[lr].append(epoch)\n",
        "                converged = True\n",
        "                break\n",
        "        if not converged:\n",
        "            convergence_data[lr].append(n_epochs_lr)  # æœªæ”¶æ•›\n",
        "\n",
        "# ç»˜åˆ¶æ”¶æ•›é€Ÿåº¦\n",
        "x_pos = np.arange(len(thresholds))\n",
        "width = 0.2\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    plt.bar(x_pos + i*width, convergence_data[lr], width, \n",
        "            label=f'LR={lr}', alpha=0.7, color=colors[i])\n",
        "\n",
        "plt.xlabel('æŸå¤±é˜ˆå€¼')\n",
        "plt.ylabel('æ”¶æ•›è¿­ä»£æ¬¡æ•°')\n",
        "plt.title('ä¸åŒå­¦ä¹ ç‡çš„æ”¶æ•›é€Ÿåº¦')\n",
        "plt.xticks(x_pos + width*1.5, [f'{t}' for t in thresholds])\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# åˆ†æç»“æœ\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"å­¦ä¹ ç‡å½±å“åˆ†æ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n{'å­¦ä¹ ç‡':<10} {'æœ€ç»ˆæŸå¤±':<15} {'è¯¯å·®':<15} {'æ”¶æ•›æ€§':<15}\")\n",
        "print(\"-\" * 60)\n",
        "for lr in learning_rates:\n",
        "    final_loss = results[lr]['final_loss']\n",
        "    error = results[lr]['error']\n",
        "    # åˆ¤æ–­æ˜¯å¦æ”¶æ•›ï¼ˆæŸå¤±å°äº0.01ä¸”è¯¯å·®å°äº0.1ï¼‰\n",
        "    converged = \"âœ“ æ”¶æ•›\" if final_loss < 0.01 and error < 0.1 else \"âœ— æœªæ”¶æ•›\"\n",
        "    print(f\"{lr:<10.4f} {final_loss:<15.6f} {error:<15.6f} {converged:<15}\")\n",
        "\n",
        "print(\"\\nå…³é”®è§‚å¯Ÿ:\")\n",
        "print(\"1. å­¦ä¹ ç‡è¿‡å°ï¼šæ”¶æ•›é€Ÿåº¦æ…¢ï¼Œä½†å¯èƒ½æ›´ç¨³å®š\")\n",
        "print(\"2. å­¦ä¹ ç‡é€‚ä¸­ï¼šå¹³è¡¡æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§\")\n",
        "print(\"3. å­¦ä¹ ç‡è¿‡å¤§ï¼šå¯èƒ½éœ‡è¡æˆ–ä¸æ”¶æ•›\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ€»ç»“\n",
        "\n",
        "å®Œæˆæœ¬ç»ƒä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š\n",
        "1. ç†è§£ä¼˜åŒ–é—®é¢˜çš„å®šä¹‰\n",
        "2. å®ç°ä¼˜åŒ–ç®—æ³•\n",
        "3. ä½¿ç”¨ä¼˜åŒ–ç®—æ³•è®­ç»ƒæ¨¡å‹\n",
        "4. åˆ†æå’Œå¯è§†åŒ–ä¼˜åŒ–ç»“æœ\n",
        "\n",
        "### æ€è€ƒé—®é¢˜\n",
        "\n",
        "1. ä¸åŒä¼˜åŒ–ç®—æ³•çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "2. å­¦ä¹ ç‡å¯¹ä¼˜åŒ–è¿‡ç¨‹æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n",
        "3. å¦‚ä½•é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç®—æ³•ï¼Ÿ\n",
        "\n",
        "### æ‰©å±•ä»»åŠ¡\n",
        "\n",
        "- âœ… å°è¯•å®ç°å¤šç§ä¼˜åŒ–ç®—æ³•å¹¶å¯¹æ¯”ï¼ˆå·²å®Œæˆï¼šSGDã€Momentumã€Adamï¼‰\n",
        "- âœ… æµ‹è¯•ä¸åŒå­¦ä¹ ç‡çš„å½±å“ï¼ˆè§ä¸‹æ–¹å®éªŒï¼‰\n",
        "- âœ… å®ç°å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼ˆå·²å®Œæˆï¼šå­¦ä¹ ç‡è¡°å‡ï¼‰\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
