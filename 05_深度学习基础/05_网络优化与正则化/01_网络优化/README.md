# 网络优化

## 1. 课程概述

### 课程目标
1. 理解神经网络优化的难点和挑战
2. 掌握各种优化算法（SGD、Momentum、Adam等）
3. 理解批量大小对训练的影响
4. 掌握学习率调整策略
5. 理解梯度估计修正方法
6. 能够选择合适的优化算法

### 预计学习时间
- **理论学习**：6-8小时
- **代码实践**：8-10小时
- **练习巩固**：4-6小时
- **总计**：18-24小时（约1周）

### 难度等级
- **中等** - 需要理解优化理论和算法原理

### 课程定位
- **前置课程**：01_神经网络基础、02_数学基础（优化理论）
- **后续课程**：02_参数初始化、06_网络正则化
- **在体系中的位置**：神经网络训练的核心，决定训练效果

### 学完能做什么
- 能够理解不同优化算法的原理
- 能够选择合适的优化算法
- 能够调整学习率和批量大小
- 能够优化训练过程

---

## 2. 前置知识检查

### 必备前置概念清单
- **梯度下降**：理解梯度下降的基本原理
- **反向传播**：理解如何计算梯度
- **损失函数**：理解损失函数的作用
- **NumPy/PyTorch**：能够进行数值计算

### 回顾链接/跳转
- 如果不熟悉梯度下降：[02_数学基础/04_优化理论](../../02_数学基础/04_优化理论/)
- 如果不熟悉反向传播：[01_神经网络基础/02_前馈神经网络/反向传播](../../01_神经网络基础/02_前馈神经网络/#反向传播算法)

### 2.5 知识关联

#### 前置知识依赖链

**直接前置**：
- [前馈神经网络](../../01_神经网络基础/02_前馈神经网络/) - 理解反向传播、梯度计算
- [自动梯度与优化](../../01_神经网络基础/03_自动梯度与优化/) - 理解自动梯度原理
- [优化理论](../../02_数学基础/04_优化理论/) - 梯度下降基础

**间接前置**：
- [微积分](../../02_数学基础/03_微积分/) - 梯度、导数
- [感知器与神经元](../../01_神经网络基础/01_感知器与神经元/) - 理解网络结构

#### 相关概念交叉引用

**本模块核心概念**：
- **SGD**：本模块首次详细讲解，是最基础的优化算法
- **Momentum**：本模块首次详细讲解，是SGD的改进
- **Adam**：本模块首次详细讲解，是自适应优化算法

**相关概念**：
- **梯度计算**：[自动梯度与优化](../../01_神经网络基础/03_自动梯度与优化/) - 自动计算梯度
- **学习率**：[超参数优化](../05_超参数优化/) - 学习率是重要超参数
- **批量大小**：影响优化效果，在[数据预处理](../03_数据预处理/)中涉及

#### 后续应用场景

**直接后续**：
- [超参数优化](../05_超参数优化/) - 优化算法的超参数调优
- [参数初始化](../02_参数初始化/) - 初始化影响优化效果

**所有训练场景**：
- 所有使用神经网络的模块都需要优化算法
- [CNN](../../02_CNN/)、[RNN](../../03_RNN_LSTM/)、[Transformer](../../06_注意力机制与外部记忆/02_Transformer架构/)等都需要优化

### 入门小测

**选择题**（每题2分，共10分）

1. 批量梯度下降、随机梯度下降、小批量梯度下降的主要区别是？
   A. 学习率  B. 批量大小  C. 优化算法  D. 激活函数
   **答案**：B

2. Momentum算法的核心思想是？
   A. 使用历史梯度  B. 自适应学习率  C. 二阶信息  D. 正则化
   **答案**：A

3. Adam算法结合了哪两种方法？
   A. SGD和Momentum  B. Momentum和RMSprop  C. SGD和RMSprop  D. 都不对
   **答案**：B

**评分标准**：≥8分（80%）为通过

---

## 3. 核心知识点详解

### 3.1 网络优化的难点

#### 非凸优化问题

神经网络的损失函数通常是**非凸的**，存在多个局部最优解。

**挑战**：
- 可能陷入局部最优
- 难以找到全局最优
- 优化路径复杂

#### 梯度消失和梯度爆炸

**梯度消失（Vanishing Gradient）**：
- 深层网络中，梯度在反向传播时逐渐变小
- 导致浅层参数更新缓慢
- 常见于使用Sigmoid/Tanh激活函数的网络

**梯度爆炸（Exploding Gradient）**：
- 梯度在反向传播时逐渐变大
- 导致参数更新过大，训练不稳定
- 常见于RNN等序列模型

#### 鞍点问题

**鞍点（Saddle Point）**：
- 某些方向的梯度为0，但不是最优点
- 优化算法可能在此处停滞
- 在高维空间中更常见

### 3.2 优化算法

#### 批量梯度下降（BGD）

**更新规则**：
$$\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla_{\mathbf{w}} J(\mathbf{w})$$

其中：
- $J(\mathbf{w}) = \frac{1}{m}\sum_{i=1}^{m} L(f(\mathbf{x}^{(i)}), y^{(i)})$ 是损失函数
- $m$ 是所有训练样本数

**特点**：
- 使用所有样本计算梯度
- 梯度估计准确，但计算慢
- 内存需求大

#### 随机梯度下降（SGD）

**更新规则**：
$$\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla_{\mathbf{w}} L(f(\mathbf{x}^{(i)}), y^{(i)})$$

**特点**：
- 每次只使用一个样本
- 计算快，但梯度估计不准确
- 有噪声，但可能跳出局部最优

#### 小批量梯度下降（Mini-batch SGD）

**更新规则**：
$$\mathbf{w} \leftarrow \mathbf{w} - \eta \frac{1}{b}\sum_{i=1}^{b} \nabla_{\mathbf{w}} L(f(\mathbf{x}^{(i)}), y^{(i)})$$

其中 $b$ 是批量大小（batch size）。

**特点**：
- 平衡了BGD和SGD的优点
- 梯度估计较准确，计算效率高
- **最常用**的方法

#### Momentum（动量法）

**核心思想**：使用历史梯度的移动平均。

**更新规则**：
$$v_t = \beta v_{t-1} + (1-\beta) \nabla_{\mathbf{w}} J(\mathbf{w}_t)$$
$$\mathbf{w}_{t+1} = \mathbf{w}_t - \eta v_t$$

其中 $\beta$ 是动量系数（通常0.9）。

**优点**：
- 加速收敛
- 减少震荡
- 有助于跳出局部最优

#### Adam（Adaptive Moment Estimation）

**核心思想**：结合Momentum和RMSprop，自适应调整学习率。

**更新规则**：
$$m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla_{\mathbf{w}} J(\mathbf{w}_t)$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) (\nabla_{\mathbf{w}} J(\mathbf{w}_t))^2$$
$$\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}$$
$$\mathbf{w}_{t+1} = \mathbf{w}_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

**优点**：
- 自适应学习率
- 对超参数不敏感
- **最流行的优化算法**

### 3.3 批量大小选择

#### 批量大小的影响

- **小批量**：梯度估计不准确，但可能跳出局部最优
- **大批量**：梯度估计准确，但可能陷入局部最优
- **中等批量**：平衡两者（通常32-256）

#### 选择原则

- 根据内存大小选择
- 根据数据特点选择
- 通常使用2的幂次（32, 64, 128, 256）

### 3.4 学习率调整

#### 学习率衰减

**固定衰减**：
$$\eta_t = \eta_0 \times \text{decay}^{\lfloor t/\text{step} \rfloor}$$

**指数衰减**：
$$\eta_t = \eta_0 \times e^{-kt}$$

**余弦退火**：
$$\eta_t = \eta_{\min} + (\eta_{\max} - \eta_{\min}) \times \frac{1 + \cos(\pi t / T)}{2}$$

#### 自适应学习率

- **AdaGrad**：根据历史梯度调整
- **RMSprop**：使用移动平均
- **Adam**：结合Momentum和RMSprop

---

## 4. Python代码实践

（代码示例将在代码示例文件夹中提供，包含各种优化算法的实现和对比）

---

## 5. 动手练习

（练习题将在练习题文件夹中提供）

---

## 6. 实际案例

（实际案例将在实战案例文件夹中提供）

---

## 7. 自我评估

（自我评估题目将在自我评估文件夹中提供）

---

## 8. 拓展学习

### 论文推荐

1. **Kingma, D. P., & Ba, J. (2014). "Adam: A method for stochastic optimization."** ICLR
2. **Sutskever, I., et al. (2013). "On the importance of initialization and momentum in deep learning."** ICML

### 下节课预告

**下节课**：`02_参数初始化`

---

**继续学习，掌握网络优化技术！** 🚀

