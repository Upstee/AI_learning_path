# è°ƒè¯•æŠ€å·§å’Œå¸¸è§é”™è¯¯

> **ç›®çš„**ï¼šå¿«é€Ÿå®šä½å’Œè§£å†³æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­çš„å¸¸è§é—®é¢˜

---

## ğŸ“š ç›®å½•

- [å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ](#å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ)
- [è°ƒè¯•æŠ€å·§](#è°ƒè¯•æŠ€å·§)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### é”™è¯¯1ï¼šLossä¸ºNaN

**ç—‡çŠ¶**ï¼š
```python
loss: nan
```

**å¯èƒ½åŸå› **ï¼š
1. å­¦ä¹ ç‡å¤ªå¤§
2. æ¢¯åº¦çˆ†ç‚¸
3. æ•°æ®åŒ…å«NaNæˆ–Inf
4. æ•°å€¼ä¸ç¨³å®šï¼ˆå¦‚é™¤ä»¥0ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. æ£€æŸ¥æ•°æ®
def check_data(data):
    if torch.isnan(data).any():
        print("âš ï¸ æ•°æ®åŒ…å«NaN")
    if torch.isinf(data).any():
        print("âš ï¸ æ•°æ®åŒ…å«Inf")
    return torch.isnan(data).any() or torch.isinf(data).any()

# 2. æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 3. é™ä½å­¦ä¹ ç‡
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)  # é™ä½å­¦ä¹ ç‡

# 4. ä½¿ç”¨æ•°å€¼ç¨³å®šçš„æ“ä½œ
# é¿å…ï¼šx / yï¼ˆå¦‚æœyå¯èƒ½ä¸º0ï¼‰
# ä½¿ç”¨ï¼šx / (y + eps)
```

---

### é”™è¯¯2ï¼šCUDA out of memory

**ç—‡çŠ¶**ï¼š
```
RuntimeError: CUDA out of memory
```

**å¯èƒ½åŸå› **ï¼š
1. æ‰¹æ¬¡å¤§å°å¤ªå¤§
2. æ¨¡å‹å¤ªå¤§
3. ç´¯ç§¯æ¢¯åº¦
4. å†…å­˜æ³„æ¼

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. å‡å°æ‰¹æ¬¡å¤§å°
train_loader = DataLoader(dataset, batch_size=16)  # ä»32å‡åˆ°16

# 2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
for i, (data, target) in enumerate(train_loader):
    output = model(data)
    loss = criterion(output, target) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 3. æ¸…ç†ç¼“å­˜
torch.cuda.empty_cache()

# 4. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    output = model(data)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

---

### é”™è¯¯3ï¼šæ¨¡å‹ä¸å­¦ä¹ ï¼ˆLossä¸ä¸‹é™ï¼‰

**ç—‡çŠ¶**ï¼š
- æŸå¤±ä¸ä¸‹é™æˆ–ä¸‹é™å¾ˆæ…¢
- å‡†ç¡®ç‡ä¸æå‡

**å¯èƒ½åŸå› **ï¼š
1. å­¦ä¹ ç‡å¤ªå°
2. æ•°æ®æœªå½’ä¸€åŒ–
3. æƒé‡åˆå§‹åŒ–ä¸å½“
4. æ¢¯åº¦æ¶ˆå¤±
5. æ•°æ®æ ‡ç­¾é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. æ£€æŸ¥å­¦ä¹ ç‡
print(f"å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")

# 2. æ•°æ®å½’ä¸€åŒ–
from torchvision import transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # å½’ä¸€åŒ–åˆ°[-1, 1]
])

# 3. æ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: {param.grad.norm().item():.6f}")
    else:
        print(f"{name}: æ— æ¢¯åº¦")

# 4. æ£€æŸ¥æ•°æ®
print(f"æ•°æ®èŒƒå›´: [{data.min():.2f}, {data.max():.2f}]")
print(f"æ ‡ç­¾èŒƒå›´: [{target.min()}, {target.max()}]")

# 5. ä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾
best_lr = find_learning_rate(model, train_loader, criterion)
```

---

### é”™è¯¯4ï¼šè¿‡æ‹Ÿåˆ

**ç—‡çŠ¶**ï¼š
- è®­ç»ƒæŸå¤±ä¸‹é™ï¼ŒéªŒè¯æŸå¤±ä¸Šå‡
- è®­ç»ƒå‡†ç¡®ç‡é«˜ï¼ŒéªŒè¯å‡†ç¡®ç‡ä½

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. å¢åŠ Dropout
model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Dropout(0.5),  # å¢åŠ Dropout
    nn.Linear(128, 10)
)

# 2. å¢åŠ L2æ­£åˆ™åŒ–
optimizer = torch.optim.SGD(
    model.parameters(), 
    lr=0.01, 
    weight_decay=1e-4  # L2æ­£åˆ™åŒ–
)

# 3. æ•°æ®å¢å¼º
from torchvision import transforms
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # æ•°æ®å¢å¼º
    transforms.RandomRotation(10),
    transforms.ToTensor(),
])

# 4. æ—©åœ
early_stopping = EarlyStopping(patience=10)
```

---

### é”™è¯¯5ï¼šç»´åº¦ä¸åŒ¹é…

**ç—‡çŠ¶**ï¼š
```
RuntimeError: size mismatch
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. æ‰“å°ç»´åº¦
print(f"è¾“å…¥å½¢çŠ¶: {data.shape}")
print(f"æ¨¡å‹æœŸæœ›: {model.expected_input_shape}")

# 2. ä½¿ç”¨reshape/view
x = x.view(-1, 784)  # å±•å¹³
x = x.reshape(batch_size, -1)  # å±•å¹³

# 3. æ£€æŸ¥æ•°æ®åŠ è½½
for data, target in train_loader:
    print(f"æ‰¹æ¬¡å½¢çŠ¶: {data.shape}, {target.shape}")
    break
```

---

## è°ƒè¯•æŠ€å·§

### æŠ€å·§1ï¼šæ·»åŠ æ–­è¨€ï¼ˆAssertionsï¼‰

```python
def forward(self, x):
    assert x.dim() == 4, f"è¾“å…¥åº”è¯¥æ˜¯4Dï¼Œå¾—åˆ°{x.dim()}D"
    assert x.size(0) > 0, "æ‰¹æ¬¡å¤§å°å¿…é¡»>0"
    
    x = self.conv1(x)
    assert not torch.isnan(x).any(), "å·ç§¯åå‡ºç°NaN"
    
    return x
```

---

### æŠ€å·§2ï¼šä½¿ç”¨è°ƒè¯•å™¨

```python
# åœ¨ä»£ç ä¸­è®¾ç½®æ–­ç‚¹
import pdb; pdb.set_trace()

# æˆ–è€…ä½¿ç”¨IPythonè°ƒè¯•å™¨
from IPython.core.debugger import set_trace
set_trace()
```

---

### æŠ€å·§3ï¼šé€æ­¥éªŒè¯

```python
# 1. éªŒè¯æ•°æ®åŠ è½½
for data, target in train_loader:
    print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"æ ‡ç­¾å½¢çŠ¶: {target.shape}")
    print(f"æ•°æ®èŒƒå›´: [{data.min()}, {data.max()}]")
    break

# 2. éªŒè¯å‰å‘ä¼ æ’­
model.eval()
with torch.no_grad():
    output = model(data)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{output.min()}, {output.max()}]")

# 3. éªŒè¯æŸå¤±è®¡ç®—
loss = criterion(output, target)
print(f"æŸå¤±å€¼: {loss.item()}")

# 4. éªŒè¯åå‘ä¼ æ’­
loss.backward()
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: æ¢¯åº¦å­˜åœ¨")
    else:
        print(f"{name}: æ— æ¢¯åº¦")
```

---

### æŠ€å·§4ï¼šä½¿ç”¨æ—¥å¿—

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)
    val_loss = validate(model, val_loader, criterion)
    
    logger.info(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}")
    
    # æ£€æŸ¥å¼‚å¸¸
    if torch.isnan(torch.tensor(train_loss)):
        logger.error("è®­ç»ƒæŸå¤±ä¸ºNaNï¼")
        break
```

---

### æŠ€å·§5ï¼šå¯è§†åŒ–ä¸­é—´ç»“æœ

```python
def visualize_forward_pass(model, data):
    """å¯è§†åŒ–å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœ"""
    activations = {}
    
    def hook_fn(name):
        def hook(module, input, output):
            activations[name] = output.detach()
        return hook
    
    hooks = []
    for name, module in model.named_modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            hook = module.register_forward_hook(hook_fn(name))
            hooks.append(hook)
    
    model.eval()
    with torch.no_grad():
        output = model(data)
    
    # å¯è§†åŒ–
    for name, activation in activations.items():
        print(f"{name}: {activation.shape}, èŒƒå›´: [{activation.min():.2f}, {activation.max():.2f}]")
        if activation.dim() == 2:
            plt.figure(figsize=(10, 4))
            plt.hist(activation.flatten().cpu().numpy(), bins=50)
            plt.title(f"{name} æ¿€æ´»å€¼åˆ†å¸ƒ")
            plt.show()
    
    # ç§»é™¤hook
    for hook in hooks:
        hook.remove()
```

---

## æ€§èƒ½ä¼˜åŒ–

### ä¼˜åŒ–1ï¼šä½¿ç”¨DataLoaderçš„num_workers

```python
# å¤šè¿›ç¨‹åŠ è½½æ•°æ®
train_loader = DataLoader(
    dataset, 
    batch_size=32, 
    num_workers=4,  # ä½¿ç”¨4ä¸ªè¿›ç¨‹
    pin_memory=True  # åŠ é€ŸGPUä¼ è¾“
)
```

---

### ä¼˜åŒ–2ï¼šä½¿ç”¨torch.compileï¼ˆPyTorch 2.0+ï¼‰

```python
# ç¼–è¯‘æ¨¡å‹ï¼ˆåŠ é€Ÿï¼‰
model = torch.compile(model)
```

---

### ä¼˜åŒ–3ï¼šä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in train_loader:
    optimizer.zero_grad()
    
    with autocast():
        output = model(data)
        loss = criterion(output, target)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

---

### ä¼˜åŒ–4ï¼šä½¿ç”¨checkpointèŠ‚çœå†…å­˜

```python
from torch.utils.checkpoint import checkpoint

def forward(self, x):
    # ä½¿ç”¨checkpointèŠ‚çœå†…å­˜
    x = checkpoint(self.layer1, x)
    x = checkpoint(self.layer2, x)
    return x
```

---

## æœ€ä½³å®è·µ

### å®è·µ1ï¼šä»£ç ç»“æ„

```python
# å¥½çš„ç»“æ„
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self._build_model()
    
    def _build_model(self):
        """æ„å»ºæ¨¡å‹"""
        self.layer1 = nn.Linear(784, 128)
        self.layer2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = x.view(-1, 784)
        x = F.relu(self.layer1(x))
        x = self.layer2(x)
        return x

# è®­ç»ƒå‡½æ•°
def train_one_epoch(model, train_loader, optimizer, criterion):
    model.train()
    total_loss = 0
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(train_loader)
```

---

### å®è·µ2ï¼šå¯å¤ç°æ€§

```python
import torch
import numpy as np
import random

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)
```

---

### å®è·µ3ï¼šæ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# ä¿å­˜æœ€ä½³æ¨¡å‹
def save_best_model(model, optimizer, epoch, loss, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, path)

# åŠ è½½æ¨¡å‹
def load_model(model, optimizer, path):
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['epoch'], checkpoint['loss']
```

---

### å®è·µ4ï¼šé”™è¯¯å¤„ç†

```python
def safe_train(model, train_loader, optimizer, criterion):
    """å®‰å…¨çš„è®­ç»ƒå‡½æ•°ï¼ŒåŒ…å«é”™è¯¯å¤„ç†"""
    try:
        model.train()
        total_loss = 0
        for batch_idx, (data, target) in enumerate(train_loader):
            try:
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                
                # æ£€æŸ¥NaN
                if torch.isnan(loss):
                    print(f"âš ï¸ æ‰¹æ¬¡{batch_idx}: æŸå¤±ä¸ºNaNï¼Œè·³è¿‡")
                    continue
                
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                total_loss += loss.item()
            except Exception as e:
                print(f"âš ï¸ æ‰¹æ¬¡{batch_idx}å‡ºé”™: {e}")
                continue
        
        return total_loss / len(train_loader)
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        raise
```

---

## ğŸ“– æ›´å¤šèµ„æº

- **ç¥ç»ç½‘ç»œå¯è§†åŒ–å·¥å…·**ï¼š[ç¥ç»ç½‘ç»œå¯è§†åŒ–å·¥å…·.md](./ç¥ç»ç½‘ç»œå¯è§†åŒ–å·¥å…·.md)
- **è®­ç»ƒè¿‡ç¨‹ç›‘æ§æŒ‡å—**ï¼š[è®­ç»ƒè¿‡ç¨‹ç›‘æ§æŒ‡å—.md](./è®­ç»ƒè¿‡ç¨‹ç›‘æ§æŒ‡å—.md)
- **ä»£ç ç¤ºä¾‹**ï¼š[ä»£ç ç¤ºä¾‹/](./ä»£ç ç¤ºä¾‹/)

---

## ğŸ¯ è°ƒè¯•æ£€æŸ¥æ¸…å•

åœ¨é‡åˆ°é—®é¢˜æ—¶ï¼ŒæŒ‰ä»¥ä¸‹é¡ºåºæ£€æŸ¥ï¼š

- [ ] æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½ï¼Ÿ
- [ ] æ•°æ®æ˜¯å¦å½’ä¸€åŒ–ï¼Ÿ
- [ ] æ¨¡å‹ç»“æ„æ˜¯å¦æ­£ç¡®ï¼Ÿ
- [ ] æŸå¤±å‡½æ•°æ˜¯å¦æ­£ç¡®ï¼Ÿ
- [ ] ä¼˜åŒ–å™¨è®¾ç½®æ˜¯å¦æ­£ç¡®ï¼Ÿ
- [ ] å­¦ä¹ ç‡æ˜¯å¦åˆé€‚ï¼Ÿ
- [ ] æ¢¯åº¦æ˜¯å¦å­˜åœ¨ï¼Ÿ
- [ ] æ˜¯å¦æœ‰NaNæˆ–Infï¼Ÿ
- [ ] å†…å­˜æ˜¯å¦è¶³å¤Ÿï¼Ÿ
- [ ] éšæœºç§å­æ˜¯å¦è®¾ç½®ï¼Ÿ

---

**é€šè¿‡ç³»ç»ŸåŒ–çš„è°ƒè¯•ï¼Œä½ å¯ä»¥å¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜ï¼** ğŸ”§
