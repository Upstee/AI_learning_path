# 自动梯度与优化

## ⚠️ 模块说明

本模块是优化后的结构，整合了原 `06_自动梯度计算/` 和 `07_优化问题/` 的内容，对应教材：
- **第4章4.5**：自动梯度计算
- **第4章4.6**：优化问题

**优化理由**：自动梯度是优化问题的工具，两者关联紧密，合并后学习更顺畅，便于理解从手动梯度到自动梯度的演进。

---

## 1. 课程概述

### 课程目标

1. **理解自动梯度计算**
   - 理解计算图（Computational Graph）的概念
   - 掌握前向模式和反向模式自动微分
   - 理解自动梯度计算的优势

2. **掌握优化算法**
   - 理解梯度下降的基本原理
   - 掌握批量梯度下降、随机梯度下降、小批量梯度下降
   - 理解学习率的作用和调整策略

3. **实践能力**
   - 能够实现简单的自动梯度系统
   - 能够实现不同的优化算法
   - 能够选择合适的优化方法

### 预计学习时间

- **理论学习**：6-8小时（自动梯度3-4小时 + 优化问题3-4小时）
- **代码实践**：8-10小时
- **练习巩固**：4-6小时
- **总计**：18-24小时（约3-4天）

### 难度等级

- **中等到较难** - 需要理解计算图、自动微分、优化理论

### 课程定位

- **前置课程**：02_前馈神经网络（理解手动反向传播）、02_数学基础（微积分、优化理论）
- **后续课程**：04_PyTorch_TensorFlow（使用框架的自动梯度）、05_网络优化与正则化（高级优化算法）
- **在体系中的位置**：连接手动实现和框架使用的桥梁

### 学完能做什么

- ✅ 理解自动梯度计算的原理
- ✅ 能够实现简单的自动梯度系统
- ✅ 理解不同优化算法的特点
- ✅ 能够选择合适的优化方法
- ✅ 理解学习率的作用和调整策略
- ✅ 为使用深度学习框架打下基础

---

## 2. 前置知识检查

### 必备前置概念清单

- **反向传播**：理解手动反向传播算法
- **微积分**：导数、偏导数、链式法则
- **优化理论**：梯度下降、凸优化基础
- **计算图**：理解有向无环图（DAG）
- **Python基础**：类、装饰器、上下文管理器

### 回顾链接/跳转

- 如果不熟悉反向传播：[02_前馈神经网络](../02_前馈神经网络/)
- 如果不熟悉微积分：[02_数学基础/03_微积分](../../02_数学基础/03_微积分/)
- 如果不熟悉优化理论：[02_数学基础/04_优化理论](../../02_数学基础/04_优化理论/)

### 2.5 知识关联

#### 前置知识依赖链

**直接前置**：
- [前馈神经网络](../02_前馈神经网络/) - 理解手动反向传播算法
- [微积分](../../02_数学基础/03_微积分/) - 导数、偏导数、链式法则
- [优化理论](../../02_数学基础/04_优化理论/) - 梯度下降、凸优化基础

**间接前置**：
- [感知器与神经元](../01_感知器与神经元/) - 理解网络结构
- [线性代数](../../02_数学基础/01_线性代数/) - 矩阵运算

#### 相关概念交叉引用

**本模块核心概念**：
- **计算图**：本模块首次详细讲解，是自动梯度的基础
- **自动梯度**：本模块首次详细讲解，是深度学习框架的核心
- **优化算法**：本模块讲解基础优化，[网络优化](../../05_网络优化与正则化/01_网络优化/)会深入

**相关概念**：
- **反向传播**：[前馈神经网络/反向传播](../02_前馈神经网络/#反向传播算法) - 手动实现
- **梯度下降**：[网络优化/梯度下降](../../05_网络优化与正则化/01_网络优化/#梯度下降) - 优化算法基础

#### 后续应用场景

**直接后续**：
- [PyTorch基础](../../04_PyTorch_TensorFlow/01_PyTorch基础/) - PyTorch的autograd系统
- [TensorFlow基础](../../04_PyTorch_TensorFlow/02_TensorFlow基础/) - TensorFlow的计算图
- [网络优化](../../05_网络优化与正则化/01_网络优化/) - 高级优化算法

**框架应用**：
- 所有使用框架的模块都会用到自动梯度系统

### 入门小测

**选择题**（每题2分，共10分）

1. 自动梯度计算的核心是？
   A. 前向传播  B. 计算图  C. 矩阵乘法  D. 激活函数
   **答案**：B
   **解释**：自动梯度通过构建计算图，自动计算梯度。

2. 反向模式自动微分的优势是？
   A. 内存占用小  B. 计算所有输入的梯度效率高  C. 计算简单  D. 不需要计算图
   **答案**：B
   **解释**：反向模式对于多输入单输出的情况，计算所有输入的梯度只需要一次反向传播。

3. 批量梯度下降的特点是？
   A. 每次使用一个样本  B. 每次使用全部样本  C. 每次使用一小批样本  D. 随机选择样本
   **答案**：B
   **解释**：批量梯度下降使用全部训练样本计算梯度。

4. 学习率过大会导致？
   A. 收敛慢  B. 无法收敛或震荡  C. 梯度消失  D. 过拟合
   **答案**：B
   **解释**：学习率过大会导致参数更新过大，可能跳过最优解或震荡。

5. 小批量梯度下降的批量大小通常是？
   A. 1  B. 全部样本  C. 32, 64, 128等  D. 固定值
   **答案**：C
   **解释**：小批量梯度下降通常使用32、64、128等批量大小，平衡效率和稳定性。

**评分标准**：≥8分（80%）为通过

---

## 3. 核心知识点详解

### 3.1 自动梯度计算

#### 3.1.1 计算图（Computational Graph）

**计算图**是有向无环图（DAG），表示计算过程。

**节点**：表示操作（加法、乘法、激活函数等）
**边**：表示数据流

**示例**：
```
x → [*] → z1 → [+] → z2 → [sigmoid] → y
w ↗        b ↗
```

#### 3.1.2 前向模式自动微分（Forward Mode）

**原理**：从输入到输出，同时计算函数值和梯度。

**特点**：
- 适合单输入多输出的情况
- 需要多次前向传播（每个输入一次）

#### 3.1.3 反向模式自动微分（Reverse Mode / Backpropagation）

**原理**：先前向传播计算函数值，再反向传播计算梯度。

**特点**：
- 适合多输入单输出的情况（深度学习常见）
- 只需要一次反向传播
- 需要存储中间结果（内存开销）

**算法**：
1. 前向传播：构建计算图，计算所有中间值
2. 反向传播：从输出到输入，使用链式法则计算梯度

#### 3.1.4 自动梯度的优势

- ✅ **自动化**：无需手动推导梯度公式
- ✅ **准确**：避免手动实现的错误
- ✅ **灵活**：支持任意复杂的计算图
- ✅ **高效**：优化的实现

---

### 3.2 优化问题

#### 3.2.1 优化目标

**目标**：最小化损失函数
$$\min_{\theta} L(\theta) = \frac{1}{m} \sum_{i=1}^{m} \ell(f(x^{(i)}; \theta), y^{(i)})$$

其中$\theta$是模型参数。

#### 3.2.2 梯度下降（Gradient Descent）

**基本更新规则**：
$$\theta \leftarrow \theta - \eta \nabla_{\theta} L(\theta)$$

其中$\eta$是学习率。

#### 3.2.3 批量梯度下降（Batch Gradient Descent, BGD）

**特点**：
- 使用全部训练样本计算梯度
- 梯度准确，但计算慢
- 适合小数据集

**更新规则**：
$$\theta \leftarrow \theta - \eta \frac{1}{m} \sum_{i=1}^{m} \nabla_{\theta} \ell(f(x^{(i)}; \theta), y^{(i)})$$

#### 3.2.4 随机梯度下降（Stochastic Gradient Descent, SGD）

**特点**：
- 每次使用一个样本计算梯度
- 计算快，但梯度噪声大
- 适合大数据集

**更新规则**：
$$\theta \leftarrow \theta - \eta \nabla_{\theta} \ell(f(x^{(i)}; \theta), y^{(i)})$$

#### 3.2.5 小批量梯度下降（Mini-batch Gradient Descent）

**特点**：
- 每次使用一小批样本（32, 64, 128等）
- 平衡效率和稳定性
- 深度学习中最常用

**更新规则**：
$$\theta \leftarrow \theta - \eta \frac{1}{|B|} \sum_{i \in B} \nabla_{\theta} \ell(f(x^{(i)}; \theta), y^{(i)})$$

其中$B$是当前批次。

---

### 3.3 学习率（Learning Rate）

#### 3.3.1 学习率的作用

学习率$\eta$控制参数更新的步长：
- **太大**：可能跳过最优解，无法收敛或震荡
- **太小**：收敛慢，可能陷入局部最优
- **合适**：快速收敛到最优解

#### 3.3.2 学习率调整策略

1. **固定学习率**：整个训练过程使用固定值
2. **学习率衰减**：
   - 阶梯衰减：每隔一定epoch降低
   - 指数衰减：$\eta_t = \eta_0 \cdot \gamma^t$
   - 余弦退火：$\eta_t = \eta_{\min} + (\eta_{\max} - \eta_{\min}) \cdot \frac{1 + \cos(\pi t / T)}{2}$
3. **自适应学习率**：根据梯度自动调整（如Adam）

---

### 3.4 关键性质总结

#### 自动梯度

**优点**：
- ✅ 自动化，减少错误
- ✅ 支持复杂计算图
- ✅ 高效实现

**缺点**：
- ❌ 需要存储中间结果（内存开销）
- ❌ 实现复杂

#### 优化算法

**BGD**：
- ✅ 梯度准确
- ❌ 计算慢，内存占用大

**SGD**：
- ✅ 计算快，内存占用小
- ❌ 梯度噪声大，收敛不稳定

**Mini-batch GD**：
- ✅ 平衡效率和稳定性
- ✅ 深度学习中最常用

---

## 4. Python代码实践

### 4.1 环境与依赖版本

```python
# Python 3.8+
# NumPy 1.19+
# Matplotlib 3.3+
```

### 4.2 实现简单的自动梯度系统

详细代码请参考：`代码示例/01_实现自动梯度系统.ipynb`

### 4.3 实现优化算法

详细代码请参考：`代码示例/02_实现优化算法.ipynb`

---

## 5. 动手练习（分层次）

### 基础练习（3-5题）

#### 练习1：理解计算图
**目标**：手动构建简单计算图

**难度**：⭐⭐

#### 练习2：实现简单的自动梯度
**目标**：实现支持基本运算的自动梯度

**难度**：⭐⭐⭐

#### 练习3：比较不同优化算法
**目标**：实现BGD、SGD、Mini-batch GD并对比

**难度**：⭐⭐⭐⭐

### 进阶练习（2-3题）

#### 练习1：实现学习率衰减
**目标**：实现多种学习率衰减策略

**难度**：⭐⭐⭐⭐

### 挑战练习（1-2题）

#### 练习1：实现完整的自动梯度框架
**目标**：实现类似PyTorch的自动梯度系统

**难度**：⭐⭐⭐⭐⭐

---

## 6. 实际案例

详细内容请参考：`实战案例/` 文件夹

---

## 7. 自我评估

详细评估题目请参考：`自我评估/` 文件夹

---

## 8. 拓展学习

### 论文推荐

1. **自动微分相关论文**
   - 计算图的理论基础
   - 难度：⭐⭐⭐⭐

### 书籍推荐

1. **《神经网络与深度学习-邱锡鹏》**
   - 第4章4.5：自动梯度计算
   - 第4章4.6：优化问题

### 下节课预告

**下节课**：`04_PyTorch_TensorFlow`

**内容预告**：
- PyTorch的自动梯度系统
- TensorFlow的计算图
- 框架的使用方法

---

**继续学习，成为深度学习专家！** 🚀

