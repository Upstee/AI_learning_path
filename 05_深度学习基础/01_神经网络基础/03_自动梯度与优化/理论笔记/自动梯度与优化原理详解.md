# 自动梯度与优化原理详解

## 1. 自动梯度计算的历史背景

### 1.1 从手动梯度到自动梯度

**传统方法**：
- 手动推导梯度公式
- 手动实现反向传播
- 容易出错，难以维护

**自动梯度**：
- 自动计算梯度
- 支持任意复杂的计算图
- 准确、高效、灵活

### 1.2 自动微分的发展

- **1960s**：自动微分的理论基础建立
- **1980s**：反向模式自动微分（Backpropagation）提出
- **2000s**：深度学习框架开始使用自动梯度
- **2010s**：PyTorch、TensorFlow等框架普及自动梯度

---

## 2. 计算图（Computational Graph）

### 2.1 计算图的定义

**计算图**是有向无环图（DAG），表示计算过程：
- **节点（Node）**：表示操作（加法、乘法、激活函数等）
- **边（Edge）**：表示数据流（张量）

### 2.2 计算图的构建

**示例**：计算 $y = \sigma(wx + b)$

```
输入: x, w, b
  ↓
乘法: z1 = w * x
  ↓
加法: z2 = z1 + b
  ↓
激活: y = σ(z2)
```

**计算图表示**：
```
x ──┐
    ├→ [*] → z1 ──┐
w ──┘              ├→ [+] → z2 → [σ] → y
                   │
b ─────────────────┘
```

### 2.3 计算图的优势

1. **可视化**：清晰展示计算流程
2. **自动梯度**：基于计算图自动计算梯度
3. **优化**：可以优化计算顺序
4. **并行化**：可以并行计算独立分支

---

## 3. 自动微分（Automatic Differentiation）

### 3.1 自动微分 vs 数值微分 vs 符号微分

| 方法 | 优点 | 缺点 |
|------|------|------|
| **数值微分** | 实现简单 | 精度低，计算慢 |
| **符号微分** | 精确 | 表达式复杂，难以处理控制流 |
| **自动微分** | 精确、高效、灵活 | 需要存储中间结果 |

### 3.2 前向模式自动微分（Forward Mode）

**原理**：从输入到输出，同时计算函数值和梯度。

**算法**：
1. 对每个输入变量，计算其对输出的导数
2. 使用链式法则传播梯度

**示例**：计算 $f(x, y) = x^2 + xy$ 在 $(x=2, y=3)$ 处的梯度

**前向传播表**：
| 变量 | 值 | $\frac{\partial}{\partial x}$ | $\frac{\partial}{\partial y}$ |
|------|-----|------------------------------|------------------------------|
| $x$ | 2 | 1 | 0 |
| $y$ | 3 | 0 | 1 |
| $x^2$ | 4 | $2x = 4$ | 0 |
| $xy$ | 6 | $y = 3$ | $x = 2$ |
| $f$ | 10 | $4 + 3 = 7$ | $0 + 2 = 2$ |

**特点**：
- ✅ 适合单输入多输出的情况
- ❌ 多输入时需要多次前向传播（每个输入一次）

### 3.3 反向模式自动微分（Reverse Mode / Backpropagation）

**原理**：先前向传播计算函数值，再反向传播计算梯度。

**算法**：
1. **前向传播**：构建计算图，计算所有中间值
2. **反向传播**：从输出到输入，使用链式法则计算梯度

**示例**：计算 $f(x, y) = x^2 + xy$ 在 $(x=2, y=3)$ 处的梯度

**前向传播**：
- $v_1 = x = 2$
- $v_2 = y = 3$
- $v_3 = v_1^2 = 4$
- $v_4 = v_1 \cdot v_2 = 6$
- $v_5 = v_3 + v_4 = 10$

**反向传播**（从输出到输入）：
- $\bar{v}_5 = \frac{\partial f}{\partial v_5} = 1$
- $\bar{v}_3 = \bar{v}_5 \cdot \frac{\partial v_5}{\partial v_3} = 1 \cdot 1 = 1$
- $\bar{v}_4 = \bar{v}_5 \cdot \frac{\partial v_5}{\partial v_4} = 1 \cdot 1 = 1$
- $\bar{v}_1^{(1)} = \bar{v}_3 \cdot \frac{\partial v_3}{\partial v_1} = 1 \cdot 2v_1 = 4$
- $\bar{v}_1^{(2)} = \bar{v}_4 \cdot \frac{\partial v_4}{\partial v_1} = 1 \cdot v_2 = 3$
- $\bar{v}_1 = \bar{v}_1^{(1)} + \bar{v}_1^{(2)} = 7$（$\frac{\partial f}{\partial x}$）
- $\bar{v}_2 = \bar{v}_4 \cdot \frac{\partial v_4}{\partial v_2} = 1 \cdot v_1 = 2$（$\frac{\partial f}{\partial y}$）

**特点**：
- ✅ 适合多输入单输出的情况（深度学习常见）
- ✅ 只需要一次反向传播
- ❌ 需要存储中间结果（内存开销）

### 3.4 为什么深度学习使用反向模式？

**原因**：
1. **多输入单输出**：神经网络有大量参数（输入），但只有一个损失值（输出）
2. **效率**：一次反向传播可以计算所有参数的梯度
3. **内存权衡**：虽然需要存储中间结果，但计算效率更重要

---

## 4. 自动梯度系统的实现

### 4.1 基本组件

**1. Variable类**：
- 存储数据（value）
- 存储梯度（grad）
- 记录计算历史（用于反向传播）

**2. Function类**：
- 前向传播（forward）
- 反向传播（backward）

**3. 计算图**：
- 自动构建
- 自动反向传播

### 4.2 实现示例（简化版）

```python
class Variable:
    def __init__(self, data):
        self.data = data
        self.grad = None
        self.creator = None  # 创建此变量的函数
    
    def backward(self):
        # 反向传播
        if self.creator is not None:
            self.creator.backward(self.grad)
    
    def __add__(self, other):
        return Add()(self, other)
    
    def __mul__(self, other):
        return Mul()(self, other)

class Function:
    def __call__(self, *inputs):
        # 前向传播
        xs = [x.data for x in inputs]
        ys = self.forward(*xs)
        
        outputs = [Variable(y) for y in ys]
        for output in outputs:
            output.creator = self
        
        self.inputs = inputs
        self.outputs = outputs
        return outputs[0] if len(outputs) == 1 else outputs
    
    def forward(self, *xs):
        raise NotImplementedError
    
    def backward(self, gy):
        raise NotImplementedError

class Add(Function):
    def forward(self, x0, x1):
        return x0 + x1
    
    def backward(self, gy):
        return gy, gy

class Mul(Function):
    def forward(self, x0, x1):
        return x0 * x1
    
    def backward(self, gy):
        x0, x1 = self.inputs[0].data, self.inputs[1].data
        return gy * x1, gy * x0
```

---

## 5. 优化问题

### 5.1 优化目标

**目标**：最小化损失函数
$$\min_{\theta} L(\theta) = \frac{1}{m} \sum_{i=1}^{m} \ell(f(x^{(i)}; \theta), y^{(i)})$$

其中：
- $\theta$：模型参数
- $m$：训练样本数
- $\ell$：损失函数
- $f$：模型函数

### 5.2 梯度下降（Gradient Descent）

**基本思想**：沿着梯度的反方向更新参数，使损失函数减小。

**更新规则**：
$$\theta \leftarrow \theta - \eta \nabla_{\theta} L(\theta)$$

其中：
- $\eta$：学习率（步长）
- $\nabla_{\theta} L(\theta)$：损失函数对参数的梯度

**几何解释**：
- 梯度指向函数值增加最快的方向
- 负梯度指向函数值减小最快的方向
- 沿着负梯度方向更新参数，使损失函数减小

### 5.3 批量梯度下降（Batch Gradient Descent, BGD）

**特点**：
- 使用**全部**训练样本计算梯度
- 梯度准确，但计算慢
- 适合小数据集

**更新规则**：
$$\theta \leftarrow \theta - \eta \frac{1}{m} \sum_{i=1}^{m} \nabla_{\theta} \ell(f(x^{(i)}; \theta), y^{(i)})$$

**优点**：
- ✅ 梯度准确
- ✅ 收敛稳定

**缺点**：
- ❌ 计算慢（需要遍历所有样本）
- ❌ 内存占用大
- ❌ 不适合大数据集

### 5.4 随机梯度下降（Stochastic Gradient Descent, SGD）

**特点**：
- 每次使用**一个**样本计算梯度
- 计算快，但梯度噪声大
- 适合大数据集

**更新规则**：
$$\theta \leftarrow \theta - \eta \nabla_{\theta} \ell(f(x^{(i)}; \theta), y^{(i)})$$

其中 $i$ 是随机选择的样本索引。

**优点**：
- ✅ 计算快
- ✅ 内存占用小
- ✅ 可以跳出局部最优

**缺点**：
- ❌ 梯度噪声大，收敛不稳定
- ❌ 需要更多迭代次数

### 5.5 小批量梯度下降（Mini-batch Gradient Descent）

**特点**：
- 每次使用**一小批**样本（32, 64, 128等）
- 平衡效率和稳定性
- **深度学习中最常用**

**更新规则**：
$$\theta \leftarrow \theta - \eta \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{\theta} \ell(f(x^{(i)}; \theta), y^{(i)})$$

其中 $\mathcal{B}$ 是当前批次。

**优点**：
- ✅ 平衡效率和稳定性
- ✅ 可以利用GPU并行计算
- ✅ 梯度噪声适中

**缺点**：
- ❌ 需要选择合适的批量大小

### 5.6 三种方法的对比

| 方法 | 批量大小 | 梯度准确性 | 计算速度 | 内存占用 | 适用场景 |
|------|---------|-----------|---------|---------|---------|
| **BGD** | $m$（全部） | 高 | 慢 | 大 | 小数据集 |
| **SGD** | $1$ | 低 | 快 | 小 | 大数据集 |
| **Mini-batch** | $32-128$ | 中 | 中 | 中 | **深度学习** |

---

## 6. 学习率（Learning Rate）

### 6.1 学习率的作用

**学习率 $\eta$** 控制参数更新的步长：
- **过大**：可能跳过最优解，无法收敛或震荡
- **过小**：收敛慢，可能陷入局部最优
- **合适**：快速收敛到最优解

### 6.2 学习率调整策略

**1. 固定学习率**：
- 最简单，但需要手动调整

**2. 学习率衰减（Learning Rate Decay）**：
- **时间衰减**：$\eta_t = \eta_0 / (1 + \alpha t)$
- **指数衰减**：$\eta_t = \eta_0 \cdot \beta^t$
- **阶梯衰减**：每隔一定轮数降低学习率

**3. 自适应学习率**：
- **AdaGrad**：根据历史梯度调整
- **RMSprop**：使用移动平均
- **Adam**：结合动量和自适应学习率

### 6.3 学习率选择

**经验法则**：
- 从较大的学习率开始（如0.01）
- 观察训练曲线
- 如果损失不下降，减小学习率
- 如果损失震荡，减小学习率
- 如果收敛太慢，增大学习率

---

## 7. 优化算法的选择

### 7.1 选择标准

1. **数据集大小**：
   - 小数据集：BGD
   - 大数据集：SGD或Mini-batch

2. **模型复杂度**：
   - 简单模型：SGD
   - 复杂模型：Mini-batch + 自适应学习率

3. **计算资源**：
   - CPU：小批量
   - GPU：大批量

### 7.2 深度学习中的选择

**推荐**：
- **Mini-batch Gradient Descent**（批量大小32-128）
- **Adam优化器**（自适应学习率）
- **学习率衰减**

---

## 8. 总结

### 8.1 核心概念

1. **计算图**：表示计算过程的有向无环图
2. **自动微分**：自动计算梯度的方法
3. **反向模式**：适合多输入单输出的情况
4. **梯度下降**：优化参数的基本方法
5. **学习率**：控制参数更新步长

### 8.2 关键公式

**梯度下降**：
$$\theta \leftarrow \theta - \eta \nabla_{\theta} L(\theta)$$

**小批量梯度下降**：
$$\theta \leftarrow \theta - \eta \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{\theta} \ell(f(x^{(i)}; \theta), y^{(i)})$$

### 8.3 学习建议

1. **理解计算图**：这是自动梯度的基础
2. **动手实现**：实现简单的自动梯度系统，加深理解
3. **对比实验**：对比不同优化算法的效果
4. **调参实践**：学习如何选择学习率和批量大小

---

**继续学习**：
- [PyTorch基础](../../04_PyTorch_TensorFlow/01_PyTorch基础/) - PyTorch的autograd系统
- [TensorFlow基础](../../04_PyTorch_TensorFlow/02_TensorFlow基础/) - TensorFlow的计算图
- [网络优化](../../05_网络优化与正则化/01_网络优化/) - 高级优化算法（Momentum、Adam等）

