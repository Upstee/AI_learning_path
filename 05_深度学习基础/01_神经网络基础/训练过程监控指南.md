# è®­ç»ƒè¿‡ç¨‹ç›‘æ§æŒ‡å—

> **ç›®çš„**ï¼šå­¦ä¼šç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜ï¼Œæé«˜è®­ç»ƒæ•ˆç‡

---

## ğŸ“š ç›®å½•

- [å…³é”®æŒ‡æ ‡ç›‘æ§](#å…³é”®æŒ‡æ ‡ç›‘æ§)
- [å®æ—¶ç›‘æ§å·¥å…·](#å®æ—¶ç›‘æ§å·¥å…·)
- [å¸¸è§é—®é¢˜è¯†åˆ«](#å¸¸è§é—®é¢˜è¯†åˆ«)
- [ä¼˜åŒ–å»ºè®®](#ä¼˜åŒ–å»ºè®®)

---

## å…³é”®æŒ‡æ ‡ç›‘æ§

### 1. æŸå¤±å‡½æ•°ï¼ˆLossï¼‰

**ç›‘æ§å†…å®¹**ï¼š
- è®­ç»ƒæŸå¤±ï¼ˆTraining Lossï¼‰
- éªŒè¯æŸå¤±ï¼ˆValidation Lossï¼‰
- æŸå¤±å·®å€¼ï¼ˆGapï¼‰

**æ­£å¸¸æƒ…å†µ**ï¼š
```
è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±éƒ½åº”è¯¥é€æ¸ä¸‹é™
éªŒè¯æŸå¤±åº”è¯¥ç•¥é«˜äºè®­ç»ƒæŸå¤±ï¼ˆä½†å·®è·ä¸åº”å¤ªå¤§ï¼‰
```

**å¼‚å¸¸æƒ…å†µ**ï¼š
- **è¿‡æ‹Ÿåˆ**ï¼šè®­ç»ƒæŸå¤±ä¸‹é™ï¼ŒéªŒè¯æŸå¤±ä¸Šå‡æˆ–ä¸å˜
- **æ¬ æ‹Ÿåˆ**ï¼šè®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±éƒ½å¾ˆé«˜ï¼Œä¸”ä¸‹é™ç¼“æ…¢
- **ä¸æ”¶æ•›**ï¼šæŸå¤±ä¸ä¸‹é™æˆ–éœ‡è¡

**ä»£ç ç¤ºä¾‹**ï¼š
```python
import matplotlib.pyplot as plt

def plot_loss(train_losses, val_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', marker='o')
    plt.plot(val_losses, label='éªŒè¯æŸå¤±', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # è®¡ç®—æŸå¤±å·®å€¼
    gap = [val - train for train, val in zip(train_losses, val_losses)]
    plt.figure(figsize=(10, 4))
    plt.plot(gap, label='æŸå¤±å·®å€¼', marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Loss Gap')
    plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±å·®å€¼')
    plt.axhline(y=0, color='r', linestyle='--')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
```

---

### 2. å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰

**ç›‘æ§å†…å®¹**ï¼š
- è®­ç»ƒå‡†ç¡®ç‡ï¼ˆTraining Accuracyï¼‰
- éªŒè¯å‡†ç¡®ç‡ï¼ˆValidation Accuracyï¼‰
- å‡†ç¡®ç‡å·®å€¼ï¼ˆGapï¼‰

**æ­£å¸¸æƒ…å†µ**ï¼š
```
è®­ç»ƒå‡†ç¡®ç‡å’ŒéªŒè¯å‡†ç¡®ç‡éƒ½åº”è¯¥é€æ¸ä¸Šå‡
éªŒè¯å‡†ç¡®ç‡åº”è¯¥ç•¥ä½äºè®­ç»ƒå‡†ç¡®ç‡ï¼ˆä½†å·®è·ä¸åº”å¤ªå¤§ï¼‰
```

**å¼‚å¸¸æƒ…å†µ**ï¼š
- **è¿‡æ‹Ÿåˆ**ï¼šè®­ç»ƒå‡†ç¡®ç‡å¾ˆé«˜ï¼ŒéªŒè¯å‡†ç¡®ç‡è¾ƒä½
- **æ¬ æ‹Ÿåˆ**ï¼šè®­ç»ƒå‡†ç¡®ç‡å’ŒéªŒè¯å‡†ç¡®ç‡éƒ½å¾ˆä½
- **ä¸å­¦ä¹ **ï¼šå‡†ç¡®ç‡ä¸æå‡æˆ–ä¸‹é™

**ä»£ç ç¤ºä¾‹**ï¼š
```python
def plot_accuracy(train_accs, val_accs):
    plt.figure(figsize=(10, 6))
    plt.plot(train_accs, label='è®­ç»ƒå‡†ç¡®ç‡', marker='o')
    plt.plot(val_accs, label='éªŒè¯å‡†ç¡®ç‡', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡æ›²çº¿')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
```

---

### 3. å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰

**ç›‘æ§å†…å®¹**ï¼š
- å½“å‰å­¦ä¹ ç‡
- å­¦ä¹ ç‡è°ƒåº¦æ•ˆæœ

**æ­£å¸¸æƒ…å†µ**ï¼š
```
å­¦ä¹ ç‡åº”è¯¥æ ¹æ®è°ƒåº¦ç­–ç•¥å˜åŒ–
æŸå¤±åº”è¯¥éšç€å­¦ä¹ ç‡è°ƒæ•´è€Œæ”¹å–„
```

**å¼‚å¸¸æƒ…å†µ**ï¼š
- **å­¦ä¹ ç‡å¤ªå¤§**ï¼šæŸå¤±éœ‡è¡æˆ–çˆ†ç‚¸
- **å­¦ä¹ ç‡å¤ªå°**ï¼šæŸå¤±ä¸‹é™å¾ˆæ…¢
- **å­¦ä¹ ç‡è°ƒåº¦ä¸å½“**ï¼šæŸå¤±ä¸æ”¹å–„

**ä»£ç ç¤ºä¾‹**ï¼š
```python
def plot_learning_rate(learning_rates, losses):
    fig, axes = plt.subplots(2, 1, figsize=(10, 8))
    
    # å­¦ä¹ ç‡æ›²çº¿
    axes[0].plot(learning_rates, marker='o')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Learning Rate')
    axes[0].set_title('å­¦ä¹ ç‡å˜åŒ–')
    axes[0].set_yscale('log')
    axes[0].grid(True, alpha=0.3)
    
    # æŸå¤±æ›²çº¿
    axes[1].plot(losses, marker='s')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].set_title('æŸå¤±å˜åŒ–')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
```

---

### 4. æ¢¯åº¦ï¼ˆGradientsï¼‰

**ç›‘æ§å†…å®¹**ï¼š
- æ¢¯åº¦èŒƒæ•°ï¼ˆGradient Normï¼‰
- æ¢¯åº¦åˆ†å¸ƒ
- æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

**æ­£å¸¸æƒ…å†µ**ï¼š
```
æ¢¯åº¦èŒƒæ•°åº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼ˆé€šå¸¸0.1-10ï¼‰
æ¢¯åº¦åº”è¯¥åœ¨å„å±‚ä¹‹é—´æ­£å¸¸æµåŠ¨
```

**å¼‚å¸¸æƒ…å†µ**ï¼š
- **æ¢¯åº¦æ¶ˆå¤±**ï¼šæ¢¯åº¦èŒƒæ•° < 1e-6
- **æ¢¯åº¦çˆ†ç‚¸**ï¼šæ¢¯åº¦èŒƒæ•° > 100
- **æ¢¯åº¦ä¸ç¨³å®š**ï¼šæ¢¯åº¦èŒƒæ•°å‰§çƒˆæ³¢åŠ¨

**ä»£ç ç¤ºä¾‹**ï¼š
```python
def monitor_gradients(model):
    """ç›‘æ§å„å±‚çš„æ¢¯åº¦"""
    grad_norms = {}
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            grad_norms[name] = grad_norm
            
            # æ£€æŸ¥å¼‚å¸¸
            if grad_norm < 1e-6:
                print(f"âš ï¸ {name}: æ¢¯åº¦æ¶ˆå¤± (norm={grad_norm:.2e})")
            elif grad_norm > 100:
                print(f"âš ï¸ {name}: æ¢¯åº¦çˆ†ç‚¸ (norm={grad_norm:.2e})")
    
    # å¯è§†åŒ–
    if grad_norms:
        plt.figure(figsize=(12, 6))
        names = list(grad_norms.keys())
        norms = list(grad_norms.values())
        plt.bar(range(len(names)), norms)
        plt.xlabel('å±‚')
        plt.ylabel('æ¢¯åº¦èŒƒæ•°')
        plt.title('å„å±‚æ¢¯åº¦èŒƒæ•°')
        plt.xticks(range(len(names)), names, rotation=45)
        plt.yscale('log')
        plt.grid(True, alpha=0.3)
        plt.show()
    
    return grad_norms
```

---

## å®æ—¶ç›‘æ§å·¥å…·

### 1. TensorBoardï¼ˆæ¨èï¼‰

**å®‰è£…**ï¼š
```bash
pip install tensorboard
```

**ä½¿ç”¨**ï¼š
```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/experiment_1')

for epoch in range(num_epochs):
    # è®­ç»ƒ...
    train_loss = ...
    val_loss = ...
    train_acc = ...
    val_acc = ...
    
    # è®°å½•æŒ‡æ ‡
    writer.add_scalar('Loss/Train', train_loss, epoch)
    writer.add_scalar('Loss/Validation', val_loss, epoch)
    writer.add_scalar('Accuracy/Train', train_acc, epoch)
    writer.add_scalar('Accuracy/Validation', val_acc, epoch)
    
    # è®°å½•å­¦ä¹ ç‡
    writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)
    
    # è®°å½•æƒé‡å’Œæ¢¯åº¦
    for name, param in model.named_parameters():
        writer.add_histogram(f'Weights/{name}', param, epoch)
        if param.grad is not None:
            writer.add_histogram(f'Gradients/{name}', param.grad, epoch)
    
    # è®°å½•å›¾åƒï¼ˆå¦‚æœé€‚ç”¨ï¼‰
    if epoch % 10 == 0:
        # è®°å½•ä¸€äº›æ ·æœ¬å›¾åƒ
        writer.add_images('Images/Samples', sample_images, epoch)

writer.close()
```

**å¯åŠ¨**ï¼š
```bash
tensorboard --logdir=runs
```

**è®¿é—®**ï¼š`http://localhost:6006`

---

### 2. Weights & Biases (W&B)

**å®‰è£…**ï¼š
```bash
pip install wandb
```

**ä½¿ç”¨**ï¼š
```python
import wandb

# åˆå§‹åŒ–
wandb.init(project="my-project", name="experiment-1")

# é…ç½®
wandb.config = {
    "learning_rate": 0.001,
    "batch_size": 32,
    "epochs": 100
}

for epoch in range(num_epochs):
    # è®­ç»ƒ...
    train_loss = ...
    val_loss = ...
    
    # è®°å½•
    wandb.log({
        "train_loss": train_loss,
        "val_loss": val_loss,
        "epoch": epoch
    })

wandb.finish()
```

**ä¼˜åŠ¿**ï¼š
- äº‘ç«¯å­˜å‚¨ï¼Œå¯åˆ†äº«
- è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜
- å›¢é˜Ÿåä½œ

---

### 3. è‡ªå®šä¹‰ç›‘æ§ç±»

```python
class TrainingMonitor:
    def __init__(self):
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        self.learning_rates = []
    
    def update(self, train_loss, val_loss, train_acc, val_acc, lr):
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.train_accs.append(train_acc)
        self.val_accs.append(val_acc)
        self.learning_rates.append(lr)
    
    def plot(self):
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŸå¤±
        axes[0, 0].plot(self.train_losses, label='è®­ç»ƒ')
        axes[0, 0].plot(self.val_losses, label='éªŒè¯')
        axes[0, 0].set_title('æŸå¤±')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡
        axes[0, 1].plot(self.train_accs, label='è®­ç»ƒ')
        axes[0, 1].plot(self.val_accs, label='éªŒè¯')
        axes[0, 1].set_title('å‡†ç¡®ç‡')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡
        axes[1, 0].plot(self.learning_rates)
        axes[1, 0].set_title('å­¦ä¹ ç‡')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æŸå¤±å·®å€¼
        gap = [v - t for t, v in zip(self.train_losses, self.val_losses)]
        axes[1, 1].plot(gap)
        axes[1, 1].set_title('æŸå¤±å·®å€¼ï¼ˆéªŒè¯-è®­ç»ƒï¼‰')
        axes[1, 1].axhline(y=0, color='r', linestyle='--')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def check_issues(self):
        """æ£€æŸ¥è®­ç»ƒé—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥è¿‡æ‹Ÿåˆ
        if len(self.val_losses) > 10:
            recent_train = np.mean(self.train_losses[-10:])
            recent_val = np.mean(self.val_losses[-10:])
            if recent_val > recent_train * 1.5:
                issues.append("âš ï¸ å¯èƒ½è¿‡æ‹Ÿåˆï¼šéªŒè¯æŸå¤±æ˜æ˜¾é«˜äºè®­ç»ƒæŸå¤±")
        
        # æ£€æŸ¥æ¬ æ‹Ÿåˆ
        if len(self.train_losses) > 10:
            if self.train_losses[-1] > self.train_losses[0] * 0.8:
                issues.append("âš ï¸ å¯èƒ½æ¬ æ‹Ÿåˆï¼šè®­ç»ƒæŸå¤±ä¸‹é™ç¼“æ…¢")
        
        # æ£€æŸ¥ä¸æ”¶æ•›
        if len(self.train_losses) > 20:
            recent_trend = np.mean(self.train_losses[-10:]) - np.mean(self.train_losses[-20:-10])
            if abs(recent_trend) < 1e-6:
                issues.append("âš ï¸ å¯èƒ½ä¸æ”¶æ•›ï¼šæŸå¤±ä¸å†ä¸‹é™")
        
        return issues

# ä½¿ç”¨
monitor = TrainingMonitor()

for epoch in range(num_epochs):
    # è®­ç»ƒ...
    monitor.update(train_loss, val_loss, train_acc, val_acc, lr)
    
    if epoch % 10 == 0:
        monitor.plot()
        issues = monitor.check_issues()
        for issue in issues:
            print(issue)
```

---

## å¸¸è§é—®é¢˜è¯†åˆ«

### é—®é¢˜1ï¼šè¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰

**ç—‡çŠ¶**ï¼š
- è®­ç»ƒæŸå¤±ä¸‹é™ï¼ŒéªŒè¯æŸå¤±ä¸Šå‡
- è®­ç»ƒå‡†ç¡®ç‡é«˜ï¼ŒéªŒè¯å‡†ç¡®ç‡ä½
- æŸå¤±å·®å€¼é€æ¸å¢å¤§

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å¢åŠ æ­£åˆ™åŒ–ï¼ˆDropoutã€L2æ­£åˆ™åŒ–ï¼‰
2. å¢åŠ æ•°æ®é‡æˆ–æ•°æ®å¢å¼º
3. å‡å°‘æ¨¡å‹å¤æ‚åº¦
4. æ—©åœï¼ˆEarly Stoppingï¼‰

---

### é—®é¢˜2ï¼šæ¬ æ‹Ÿåˆï¼ˆUnderfittingï¼‰

**ç—‡çŠ¶**ï¼š
- è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±éƒ½å¾ˆé«˜
- è®­ç»ƒå‡†ç¡®ç‡å’ŒéªŒè¯å‡†ç¡®ç‡éƒ½å¾ˆä½
- æŸå¤±ä¸‹é™ç¼“æ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å¢åŠ æ¨¡å‹å¤æ‚åº¦
2. å‡å°‘æ­£åˆ™åŒ–
3. å¢åŠ è®­ç»ƒæ—¶é—´
4. è°ƒæ•´å­¦ä¹ ç‡

---

### é—®é¢˜3ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

**ç—‡çŠ¶**ï¼š
- æ¢¯åº¦èŒƒæ•° < 1e-6ï¼ˆæ¶ˆå¤±ï¼‰æˆ– > 100ï¼ˆçˆ†ç‚¸ï¼‰
- æŸå¤±ä¸ä¸‹é™æˆ–NaN
- æƒé‡æ›´æ–°å¼‚å¸¸

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰
2. ä½¿ç”¨Batch Normalization
3. ä½¿ç”¨æ®‹å·®è¿æ¥
4. è°ƒæ•´åˆå§‹åŒ–æ–¹æ³•

---

### é—®é¢˜4ï¼šå­¦ä¹ ç‡ä¸å½“

**ç—‡çŠ¶**ï¼š
- æŸå¤±éœ‡è¡ï¼ˆå­¦ä¹ ç‡å¤ªå¤§ï¼‰
- æŸå¤±ä¸‹é™å¾ˆæ…¢ï¼ˆå­¦ä¹ ç‡å¤ªå°ï¼‰
- æŸå¤±ä¸ä¸‹é™ï¼ˆå­¦ä¹ ç‡è°ƒåº¦ä¸å½“ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾ï¼ˆLearning Rate Finderï¼‰
2. ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
3. ä½¿ç”¨è‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼ˆAdamç­‰ï¼‰

---

## ä¼˜åŒ–å»ºè®®

### 1. æ—©åœï¼ˆEarly Stoppingï¼‰

```python
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
    
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience

# ä½¿ç”¨
early_stopping = EarlyStopping(patience=10)

for epoch in range(num_epochs):
    # è®­ç»ƒ...
    val_loss = ...
    
    if early_stopping(val_loss):
        print("æ—©åœï¼šéªŒè¯æŸå¤±ä¸å†æ”¹å–„")
        break
```

---

### 2. æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆCheckpointï¼‰

```python
def save_checkpoint(model, optimizer, epoch, loss, path):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }
    torch.save(checkpoint, path)

def load_checkpoint(model, optimizer, path):
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    return model, optimizer, epoch, loss
```

---

### 3. å­¦ä¹ ç‡æŸ¥æ‰¾

```python
def find_learning_rate(model, train_loader, criterion, init_lr=1e-8, final_lr=10, num_iter=100):
    """æŸ¥æ‰¾æœ€ä½³å­¦ä¹ ç‡"""
    lrs = np.logspace(np.log10(init_lr), np.log10(final_lr), num_iter)
    losses = []
    
    optimizer = torch.optim.SGD(model.parameters(), lr=init_lr)
    
    for i, (data, target) in enumerate(train_loader):
        if i >= num_iter:
            break
        
        # æ›´æ–°å­¦ä¹ ç‡
        lr = lrs[i]
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        
        # è®­ç»ƒä¸€æ­¥
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
    
    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    plt.plot(lrs, losses)
    plt.xscale('log')
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss')
    plt.title('å­¦ä¹ ç‡æŸ¥æ‰¾')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # æ‰¾åˆ°æŸå¤±ä¸‹é™æœ€å¿«çš„ç‚¹
    best_idx = np.argmin(losses)
    best_lr = lrs[best_idx]
    print(f"å»ºè®®å­¦ä¹ ç‡: {best_lr:.2e}")
    
    return best_lr
```

---

## ğŸ“– æ›´å¤šèµ„æº

- **ç¥ç»ç½‘ç»œå¯è§†åŒ–å·¥å…·**ï¼š[ç¥ç»ç½‘ç»œå¯è§†åŒ–å·¥å…·.md](./ç¥ç»ç½‘ç»œå¯è§†åŒ–å·¥å…·.md)
- **è°ƒè¯•æŠ€å·§å’Œå¸¸è§é”™è¯¯**ï¼š[è°ƒè¯•æŠ€å·§å’Œå¸¸è§é”™è¯¯.md](./è°ƒè¯•æŠ€å·§å’Œå¸¸è§é”™è¯¯.md)

---

**é€šè¿‡ç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼Œä½ å¯ä»¥åŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼** ğŸ“Š
