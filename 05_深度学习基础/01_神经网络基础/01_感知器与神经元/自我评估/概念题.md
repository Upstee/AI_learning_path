# 自我评估：概念题

## 选择题（每题2分，共20分）

### 1. 神经元与感知器的主要区别是？

A. 输入数量不同  
B. 激活函数不同  
C. 权重数量不同  
D. 没有区别

**答案**：B

**解析**：感知器使用阶跃函数（不可微），神经元使用可微的激活函数（如Sigmoid、ReLU等）。

---

### 2. Sigmoid函数的输出范围是？

A. [0, 1]  
B. [-1, 1]  
C. [0, ∞)  
D. (-∞, ∞)

**答案**：A

**解析**：Sigmoid函数的输出范围是[0, 1]，适合表示概率。

---

### 3. ReLU函数的主要优点是？

A. 输出范围大  
B. 计算简单，解决梯度消失  
C. 可微  
D. 对称

**答案**：B

**解析**：ReLU函数计算简单（max(0, z)），在z>0时梯度恒为1，解决了梯度消失问题。

---

### 4. 梯度消失问题主要出现在哪种激活函数中？

A. ReLU  
B. Sigmoid  
C. Leaky ReLU  
D. 所有激活函数

**答案**：B

**解析**：Sigmoid函数在输入很大或很小时，导数接近0，容易导致梯度消失。

---

### 5. Tanh函数相比Sigmoid函数的主要优势是？

A. 输出范围更大  
B. 零中心化  
C. 计算更快  
D. 没有优势

**答案**：B

**解析**：Tanh函数输出范围是[-1, 1]，是零中心化的，这有助于训练。

---

### 6. Dead ReLU问题是指？

A. ReLU函数不可微  
B. 神经元输出恒为0，无法更新  
C. ReLU函数计算慢  
D. ReLU函数输出太大

**答案**：B

**解析**：当神经元的输入总是小于0时，ReLU输出恒为0，梯度也为0，导致参数无法更新。

---

### 7. 在隐藏层中，最常用的激活函数是？

A. Sigmoid  
B. Tanh  
C. ReLU  
D. 线性函数

**答案**：C

**解析**：ReLU是最常用的隐藏层激活函数，因为计算简单且能解决梯度消失问题。

---

### 8. 激活函数的主要作用是？

A. 增加计算复杂度  
B. 引入非线性，使网络能学习复杂模式  
C. 减少参数数量  
D. 提高计算速度

**答案**：B

**解析**：激活函数引入非线性，使神经网络能够学习复杂的非线性关系。

---

### 9. Leaky ReLU相比ReLU的主要改进是？

A. 计算更快  
B. 输出范围更大  
C. 解决Dead ReLU问题  
D. 梯度更大

**答案**：C

**解析**：Leaky ReLU在z<0时也有小的梯度（αz），解决了Dead ReLU问题。

---

### 10. 在二分类问题的输出层，应该使用哪种激活函数？

A. ReLU  
B. Tanh  
C. Sigmoid  
D. 线性函数

**答案**：C

**解析**：Sigmoid函数输出范围[0,1]，适合表示概率，常用于二分类问题的输出层。

---

## 简答题（每题10分，共50分）

### 1. 解释为什么感知器只能解决线性可分问题，而神经元可以解决更复杂的问题？

**参考答案**：
- 感知器使用阶跃函数（不可微），只能学习线性决策边界
- 神经元使用可微的激活函数，可以使用梯度下降训练
- 多个神经元可以组合成多层网络，学习非线性关系
- 激活函数引入非线性，使网络能够逼近任意复杂函数

---

### 2. 详细说明梯度消失问题是如何产生的，以及如何解决？

**参考答案**：
- **产生原因**：
  - 深层网络中，梯度通过链式法则反向传播
  - 如果激活函数导数很小（如Sigmoid在极值处），梯度会逐渐变小
  - 导致浅层参数更新缓慢，训练困难
  
- **解决方法**：
  - 使用ReLU等激活函数（梯度在z>0时为1）
  - 使用批量归一化（Batch Normalization）
  - 使用残差连接（Residual Connections）
  - 合适的权重初始化（Xavier、He初始化）

---

### 3. 对比Sigmoid、Tanh和ReLU三种激活函数的优缺点。

**参考答案**：

**Sigmoid**：
- 优点：输出[0,1]，适合概率输出；平滑可微
- 缺点：容易梯度消失；输出不是零中心化

**Tanh**：
- 优点：零中心化；梯度比Sigmoid大
- 缺点：仍然存在梯度消失问题

**ReLU**：
- 优点：计算简单；解决梯度消失；稀疏激活
- 缺点：可能Dead ReLU；输出不是零中心化

---

### 4. 解释为什么ReLU在隐藏层中比Sigmoid更常用？

**参考答案**：
- **计算效率**：ReLU计算简单（max(0, z)），比Sigmoid快
- **梯度特性**：ReLU在z>0时梯度恒为1，避免梯度消失
- **稀疏性**：ReLU约50%神经元输出0，提高计算效率
- **实践效果**：在大多数任务上，ReLU比Sigmoid表现更好

---

### 5. 描述神经元的完整前向传播和反向传播过程。

**参考答案**：

**前向传播**：
1. 计算净输入：$z = \mathbf{w}^T \mathbf{x} + b$
2. 应用激活函数：$y = f(z)$

**反向传播**：
1. 计算损失函数对输出的梯度：$\frac{\partial L}{\partial y}$
2. 计算激活函数导数：$f'(z)$
3. 链式法则：$\frac{\partial L}{\partial z} = \frac{\partial L}{\partial y} \cdot f'(z)$
4. 计算权重梯度：$\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial z} \cdot x_i$
5. 计算偏置梯度：$\frac{\partial L}{\partial b} = \frac{\partial L}{\partial z}$
6. 更新参数：$w_i \leftarrow w_i - \eta \frac{\partial L}{\partial w_i}$

---

## 评分标准

- **选择题**：每题2分，共20分
- **简答题**：每题10分，共50分
- **总分**：70分
- **通过标准**：≥56分（80%）

---

**完成评估后，检查你的理解程度，如有不足请复习相关章节！** 📚

