{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ä»é›¶å®ç°ç¥ç»å…ƒ\n",
        "\n",
        "## ğŸ“š ä»£ç ç¤ºä¾‹å…³è”\n",
        "\n",
        "**æœ¬ç¤ºä¾‹ä½ç½®**ï¼šè¿™æ˜¯ç¥ç»ç½‘ç»œåŸºç¡€çš„ç¬¬äºŒä¸ªä»£ç ç¤ºä¾‹\n",
        "\n",
        "**åŸºäº**ï¼š\n",
        "- [ä»é›¶å®ç°æ„ŸçŸ¥å™¨](../01_ä»é›¶å®ç°æ„ŸçŸ¥å™¨.ipynb) - ç¥ç»å…ƒæ˜¯æ„ŸçŸ¥å™¨çš„æ‰©å±•ï¼Œæ·»åŠ äº†æ¿€æ´»å‡½æ•°\n",
        "\n",
        "**æ‰©å±•ä¸º**ï¼š\n",
        "- [ä»é›¶å®ç°å‰é¦ˆç¥ç»ç½‘ç»œ](../../02_å‰é¦ˆç¥ç»ç½‘ç»œ/ä»£ç ç¤ºä¾‹/01_ä»é›¶å®ç°å‰é¦ˆç¥ç»ç½‘ç»œ.ipynb) - å‰é¦ˆç¥ç»ç½‘ç»œç”±å¤šä¸ªç¥ç»å…ƒç»„æˆ\n",
        "- [PyTorchåŸºç¡€å…¥é—¨](../../../04_PyTorch_TensorFlow/01_PyTorchåŸºç¡€/ä»£ç ç¤ºä¾‹/01_PyTorchåŸºç¡€å…¥é—¨.ipynb) - ä½¿ç”¨æ¡†æ¶å®ç°ç¥ç»å…ƒ\n",
        "\n",
        "**ç›¸å…³ç¤ºä¾‹**ï¼š\n",
        "- [ç†è§£æ¿€æ´»å‡½æ•°](../../ç»ƒä¹ é¢˜/åŸºç¡€ç»ƒä¹ /ç»ƒä¹ 1_ç†è§£æ¿€æ´»å‡½æ•°.ipynb) - åŸºç¡€ç»ƒä¹ \n",
        "- [å®Œæ•´ç¥ç»å…ƒè®­ç»ƒæµç¨‹](../../Jupyterç»ƒä¹ /01_å®Œæ•´ç¥ç»å…ƒè®­ç»ƒæµç¨‹.ipynb) - å®Œæ•´è®­ç»ƒæµç¨‹\n",
        "\n",
        "---\n",
        "\n",
        "## å­¦ä¹ ç›®æ ‡\n",
        "- ç†è§£ç¥ç»å…ƒçš„åŸºæœ¬ç»“æ„å’Œå·¥ä½œåŸç†\n",
        "- æŒæ¡ä¸åŒç±»å‹çš„æ¿€æ´»å‡½æ•°ï¼ˆSigmoidã€Tanhã€ReLUç­‰ï¼‰\n",
        "- èƒ½å¤Ÿä»é›¶å®ç°å¸¦ä¸åŒæ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒ\n",
        "- ç†è§£æ¿€æ´»å‡½æ•°å¯¹è¾“å‡ºçš„å½±å“\n",
        "- å¯è§†åŒ–æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°\n",
        "\n",
        "## è¯¾ç¨‹æ¦‚è¿°\n",
        "æœ¬notebookå°†å¸¦ä½ ä»é›¶å¼€å§‹å®ç°ç¥ç»å…ƒï¼ŒåŒ…æ‹¬ï¼š\n",
        "1. æ¿€æ´»å‡½æ•°çš„å®ç°ï¼ˆSigmoidã€Tanhã€ReLUç­‰ï¼‰\n",
        "2. ç¥ç»å…ƒç±»çš„å®ç°\n",
        "3. æ¿€æ´»å‡½æ•°çš„å¯è§†åŒ–\n",
        "4. ä¸åŒæ¿€æ´»å‡½æ•°çš„å¯¹æ¯”åˆ†æ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾\n",
        "plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. æ¿€æ´»å‡½æ•°å®ç°\n",
        "\n",
        "æˆ‘ä»¬å°†å®ç°å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬å‡½æ•°æœ¬èº«å’Œå…¶å¯¼æ•°ã€‚\n",
        "\n",
        "### 1.1 æ¿€æ´»å‡½æ•°ç±»å®šä¹‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActivationFunctions:\n",
        "    \"\"\"\n",
        "    æ¿€æ´»å‡½æ•°é›†åˆ\n",
        "    åŒ…å«å¸¸ç”¨æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "        \"\"\"\n",
        "        Sigmoidæ¿€æ´»å‡½æ•°\n",
        "        å…¬å¼: Ïƒ(z) = 1 / (1 + e^(-z))\n",
        "        è¾“å‡ºèŒƒå›´: [0, 1]\n",
        "        \"\"\"\n",
        "        z = np.clip(z, -500, 500)  # é˜²æ­¢æº¢å‡º\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(z):\n",
        "        \"\"\"Sigmoidå‡½æ•°çš„å¯¼æ•°: Ïƒ'(z) = Ïƒ(z) * (1 - Ïƒ(z))\"\"\"\n",
        "        s = ActivationFunctions.sigmoid(z)\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tanh(z):\n",
        "        \"\"\"Tanhæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºèŒƒå›´: [-1, 1]\"\"\"\n",
        "        return np.tanh(z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tanh_derivative(z):\n",
        "        \"\"\"Tanhå‡½æ•°çš„å¯¼æ•°: tanh'(z) = 1 - tanh^2(z)\"\"\"\n",
        "        return 1 - np.tanh(z) ** 2\n",
        "    \n",
        "    @staticmethod\n",
        "    def relu(z):\n",
        "        \"\"\"ReLUæ¿€æ´»å‡½æ•°: ReLU(z) = max(0, z)\"\"\"\n",
        "        return np.maximum(0, z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def relu_derivative(z):\n",
        "        \"\"\"ReLUå‡½æ•°çš„å¯¼æ•°: ReLU'(z) = 1 if z > 0 else 0\"\"\"\n",
        "        return (z > 0).astype(float)\n",
        "    \n",
        "    @staticmethod\n",
        "    def leaky_relu(z, alpha=0.01):\n",
        "        \"\"\"Leaky ReLUæ¿€æ´»å‡½æ•°: LeakyReLU(z) = max(Î±z, z)\"\"\"\n",
        "        return np.maximum(alpha * z, z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def leaky_relu_derivative(z, alpha=0.01):\n",
        "        \"\"\"Leaky ReLUå‡½æ•°çš„å¯¼æ•°\"\"\"\n",
        "        return np.where(z > 0, 1.0, alpha)\n",
        "\n",
        "print(\"æ¿€æ´»å‡½æ•°å®šä¹‰å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. å¯è§†åŒ–æ¿€æ´»å‡½æ•°\n",
        "\n",
        "ç»˜åˆ¶ä¸åŒæ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°çš„å›¾åƒï¼Œç†è§£å®ƒä»¬çš„ç‰¹æ€§ã€‚\n",
        "\n",
        "### 2.1 ç”Ÿæˆè¾“å…¥å€¼å’Œè®¡ç®—æ¿€æ´»å‡½æ•°\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. æµ‹è¯•ä¸åŒæ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒ\n",
        "\n",
        "ä½¿ç”¨ç›¸åŒçš„æ•°æ®å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œæµ‹è¯•ä¸åŒæ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç”Ÿæˆç®€å•çš„è®­ç»ƒæ•°æ®\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "\n",
        "# æ­£ç±»æ ·æœ¬ï¼ˆæ ‡ç­¾=1ï¼‰\n",
        "X1 = np.random.randn(n_samples // 2, 2) + [1, 1]\n",
        "y1 = np.ones(n_samples // 2)\n",
        "\n",
        "# è´Ÿç±»æ ·æœ¬ï¼ˆæ ‡ç­¾=0ï¼‰\n",
        "X0 = np.random.randn(n_samples // 2, 2) + [-1, -1]\n",
        "y0 = np.zeros(n_samples // 2)\n",
        "\n",
        "# åˆå¹¶æ•°æ®\n",
        "X = np.vstack([X1, X0])\n",
        "y = np.hstack([y1, y0])\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"æ•°æ®ä¿¡æ¯\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"æ€»æ ·æœ¬æ•°: {len(X)}\")\n",
        "print(f\"ç‰¹å¾æ•°: {X.shape[1]}\")\n",
        "print(f\"æ­£ç±»æ ·æœ¬æ•°: {np.sum(y == 1)}\")\n",
        "print(f\"è´Ÿç±»æ ·æœ¬æ•°: {np.sum(y == 0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®­ç»ƒä¸åŒæ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒ\n",
        "activations = ['sigmoid', 'tanh', 'relu', 'leaky_relu']\n",
        "neurons = {}\n",
        "losses_history = {}\n",
        "\n",
        "learning_rate = 0.1\n",
        "n_epochs = 100\n",
        "\n",
        "for activation in activations:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"è®­ç»ƒ {activation.upper()} ç¥ç»å…ƒ\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # åˆ›å»ºç¥ç»å…ƒ\n",
        "    neuron = Neuron(n_features=2, activation=activation)\n",
        "    neurons[activation] = neuron\n",
        "    \n",
        "    # è®°å½•æŸå¤±å†å²\n",
        "    losses = []\n",
        "    \n",
        "    # è®­ç»ƒå¾ªç¯\n",
        "    for epoch in range(n_epochs):\n",
        "        # å‰å‘ä¼ æ’­\n",
        "        output, z = neuron.forward(X)\n",
        "        \n",
        "        # è®¡ç®—æŸå¤±ï¼ˆå‡æ–¹è¯¯å·®ï¼‰\n",
        "        loss = np.mean((output - y) ** 2)\n",
        "        losses.append(loss)\n",
        "        \n",
        "        # è®¡ç®—æ¢¯åº¦\n",
        "        grad_output = 2 * (output - y) / len(y)\n",
        "        \n",
        "        # åå‘ä¼ æ’­\n",
        "        grad_weights, grad_bias = neuron.backward(X, z, output, grad_output)\n",
        "        \n",
        "        # æ›´æ–°å‚æ•°\n",
        "        neuron.update(grad_weights, grad_bias, learning_rate)\n",
        "        \n",
        "        # æ¯10ä¸ªepochæ‰“å°ä¸€æ¬¡\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.4f}\")\n",
        "    \n",
        "    losses_history[activation] = losses\n",
        "    \n",
        "    # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡\n",
        "    final_output, _ = neuron.forward(X)\n",
        "    predictions = (final_output > 0.5).astype(int)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "    print(f\"\\næœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç”Ÿæˆç®€å•çš„è®­ç»ƒæ•°æ®\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "\n",
        "# æ­£ç±»æ ·æœ¬ï¼ˆæ ‡ç­¾=1ï¼‰\n",
        "X1 = np.random.randn(n_samples // 2, 2) + [1, 1]\n",
        "y1 = np.ones(n_samples // 2)\n",
        "\n",
        "# è´Ÿç±»æ ·æœ¬ï¼ˆæ ‡ç­¾=0ï¼‰\n",
        "X0 = np.random.randn(n_samples // 2, 2) + [-1, -1]\n",
        "y0 = np.zeros(n_samples // 2)\n",
        "\n",
        "# åˆå¹¶æ•°æ®\n",
        "X = np.vstack([X1, X0])\n",
        "y = np.hstack([y1, y0])\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"æ•°æ®ä¿¡æ¯\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"æ€»æ ·æœ¬æ•°: {len(X)}\")\n",
        "print(f\"ç‰¹å¾æ•°: {X.shape[1]}\")\n",
        "print(f\"æ­£ç±»æ ·æœ¬æ•°: {np.sum(y == 1)}\")\n",
        "print(f\"è´Ÿç±»æ ·æœ¬æ•°: {np.sum(y == 0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n",
        "\n",
        "å¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°çš„è®­ç»ƒæŸå¤±æ›²çº¿ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®­ç»ƒä¸åŒæ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒ\n",
        "activations = ['sigmoid', 'tanh', 'relu', 'leaky_relu']\n",
        "neurons = {}\n",
        "losses_history = {}\n",
        "\n",
        "learning_rate = 0.1\n",
        "n_epochs = 100\n",
        "\n",
        "for activation in activations:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"è®­ç»ƒ {activation.upper()} ç¥ç»å…ƒ\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # åˆ›å»ºç¥ç»å…ƒ\n",
        "    neuron = Neuron(n_features=2, activation=activation)\n",
        "    neurons[activation] = neuron\n",
        "    \n",
        "    # è®°å½•æŸå¤±å†å²\n",
        "    losses = []\n",
        "    \n",
        "    # è®­ç»ƒå¾ªç¯\n",
        "    for epoch in range(n_epochs):\n",
        "        # å‰å‘ä¼ æ’­\n",
        "        output, z = neuron.forward(X)\n",
        "        \n",
        "        # è®¡ç®—æŸå¤±ï¼ˆå‡æ–¹è¯¯å·®ï¼‰\n",
        "        loss = np.mean((output - y) ** 2)\n",
        "        losses.append(loss)\n",
        "        \n",
        "        # è®¡ç®—æ¢¯åº¦\n",
        "        grad_output = 2 * (output - y) / len(y)\n",
        "        \n",
        "        # åå‘ä¼ æ’­\n",
        "        grad_weights, grad_bias = neuron.backward(X, z, output, grad_output)\n",
        "        \n",
        "        # æ›´æ–°å‚æ•°\n",
        "        neuron.update(grad_weights, grad_bias, learning_rate)\n",
        "        \n",
        "        # æ¯10ä¸ªepochæ‰“å°ä¸€æ¬¡\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.4f}\")\n",
        "    \n",
        "    losses_history[activation] = losses\n",
        "    \n",
        "    # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡\n",
        "    final_output, _ = neuron.forward(X)\n",
        "    predictions = (final_output > 0.5).astype(int)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "    print(f\"\\næœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for activation in activations:\n",
        "    plt.plot(losses_history[activation], label=activation.upper(), linewidth=2)\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss (MSE)', fontsize=12)\n",
        "plt.title('ä¸åŒæ¿€æ´»å‡½æ•°çš„è®­ç»ƒæŸå¤±å¯¹æ¯”', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"è®­ç»ƒç»“æœåˆ†æ\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. Sigmoidå’ŒTanhï¼šé€‚åˆäºŒåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºèŒƒå›´æœ‰é™\")\n",
        "print(\"2. ReLUå’ŒLeaky ReLUï¼šå¯èƒ½è¾“å‡ºè¾ƒå¤§å€¼ï¼Œéœ€è¦è°ƒæ•´æŸå¤±å‡½æ•°æˆ–é˜ˆå€¼\")\n",
        "print(\"3. ä¸åŒæ¿€æ´»å‡½æ•°æœ‰ä¸åŒçš„æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç”Ÿæˆè¾“å…¥å€¼èŒƒå›´\n",
        "z = np.linspace(-5, 5, 1000)\n",
        "\n",
        "# è®¡ç®—å„ç§æ¿€æ´»å‡½æ•°çš„å€¼\n",
        "sigmoid_z = ActivationFunctions.sigmoid(z)\n",
        "tanh_z = ActivationFunctions.tanh(z)\n",
        "relu_z = ActivationFunctions.relu(z)\n",
        "leaky_relu_z = ActivationFunctions.leaky_relu(z)\n",
        "\n",
        "# è®¡ç®—å¯¼æ•°\n",
        "sigmoid_derivative_z = ActivationFunctions.sigmoid_derivative(z)\n",
        "tanh_derivative_z = ActivationFunctions.tanh_derivative(z)\n",
        "relu_derivative_z = ActivationFunctions.relu_derivative(z)\n",
        "leaky_relu_derivative_z = ActivationFunctions.leaky_relu_derivative(z)\n",
        "\n",
        "# ç»˜åˆ¶æ¿€æ´»å‡½æ•°\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# ç¬¬ä¸€è¡Œï¼šæ¿€æ´»å‡½æ•°\n",
        "axes[0, 0].plot(z, sigmoid_z, 'b-', linewidth=2, label='Sigmoid')\n",
        "axes[0, 0].plot(z, tanh_z, 'r-', linewidth=2, label='Tanh')\n",
        "axes[0, 0].set_xlabel('è¾“å…¥ z', fontsize=12)\n",
        "axes[0, 0].set_ylabel('è¾“å‡º f(z)', fontsize=12)\n",
        "axes[0, 0].set_title('Sigmoid vs Tanh', fontsize=14)\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].plot(z, relu_z, 'g-', linewidth=2, label='ReLU')\n",
        "axes[0, 1].plot(z, leaky_relu_z, 'm-', linewidth=2, label='Leaky ReLU (Î±=0.01)')\n",
        "axes[0, 1].set_xlabel('è¾“å…¥ z', fontsize=12)\n",
        "axes[0, 1].set_ylabel('è¾“å‡º f(z)', fontsize=12)\n",
        "axes[0, 1].set_title('ReLU vs Leaky ReLU', fontsize=14)\n",
        "axes[0, 1].legend(fontsize=10)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# ç¬¬äºŒè¡Œï¼šå¯¼æ•°\n",
        "axes[1, 0].plot(z, sigmoid_derivative_z, 'b-', linewidth=2, label=\"Sigmoid'\")\n",
        "axes[1, 0].plot(z, tanh_derivative_z, 'r-', linewidth=2, label=\"Tanh'\")\n",
        "axes[1, 0].set_xlabel('è¾“å…¥ z', fontsize=12)\n",
        "axes[1, 0].set_ylabel(\"å¯¼æ•° f'(z)\", fontsize=12)\n",
        "axes[1, 0].set_title('Sigmoid vs Tanh å¯¼æ•°', fontsize=14)\n",
        "axes[1, 0].legend(fontsize=10)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].plot(z, relu_derivative_z, 'g-', linewidth=2, label=\"ReLU'\")\n",
        "axes[1, 1].plot(z, leaky_relu_derivative_z, 'm-', linewidth=2, label=\"Leaky ReLU'\")\n",
        "axes[1, 1].set_xlabel('è¾“å…¥ z', fontsize=12)\n",
        "axes[1, 1].set_ylabel(\"å¯¼æ•° f'(z)\", fontsize=12)\n",
        "axes[1, 1].set_title('ReLU vs Leaky ReLU å¯¼æ•°', fontsize=14)\n",
        "axes[1, 1].legend(fontsize=10)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"æ¿€æ´»å‡½æ•°ç‰¹æ€§æ€»ç»“\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Sigmoid: è¾“å‡º[0,1]ï¼Œé€‚åˆæ¦‚ç‡è¾“å‡ºï¼Œä½†å®¹æ˜“æ¢¯åº¦æ¶ˆå¤±\")\n",
        "print(\"Tanh: è¾“å‡º[-1,1]ï¼Œé›¶ä¸­å¿ƒåŒ–ï¼Œæ¢¯åº¦æ¯”Sigmoidå¤§\")\n",
        "print(\"ReLU: è¾“å‡º[0,+âˆ)ï¼Œè®¡ç®—ç®€å•ï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œæœ€å¸¸ç”¨\")\n",
        "print(\"Leaky ReLU: ReLUçš„æ”¹è¿›ï¼Œè§£å†³Dead ReLUé—®é¢˜\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    \"\"\"\n",
        "    ç¥ç»å…ƒç±»\n",
        "    æ”¯æŒä¸åŒçš„æ¿€æ´»å‡½æ•°\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_features, activation='sigmoid'):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–ç¥ç»å…ƒ\n",
        "        å‚æ•°:\n",
        "            n_features: è¾“å…¥ç‰¹å¾æ•°\n",
        "            activation: æ¿€æ´»å‡½æ•°ç±»å‹ï¼ˆ'sigmoid', 'tanh', 'relu', 'leaky_relu'ï¼‰\n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.activation = activation\n",
        "        \n",
        "        # åˆå§‹åŒ–æƒé‡å’Œåç½®ï¼ˆä½¿ç”¨Xavieræˆ–Heåˆå§‹åŒ–ï¼‰\n",
        "        if activation in ['sigmoid', 'tanh']:\n",
        "            limit = np.sqrt(1.0 / n_features)\n",
        "            self.weights = np.random.uniform(-limit, limit, n_features)\n",
        "        else:  # ReLU, Leaky ReLU\n",
        "            self.weights = np.random.randn(n_features) * np.sqrt(2.0 / n_features)\n",
        "        \n",
        "        self.bias = 0.0\n",
        "        self._set_activation_functions()\n",
        "    \n",
        "    def _set_activation_functions(self):\n",
        "        \"\"\"è®¾ç½®æ¿€æ´»å‡½æ•°å’Œå¯¼æ•°å‡½æ•°\"\"\"\n",
        "        if self.activation == 'sigmoid':\n",
        "            self.activation_func = ActivationFunctions.sigmoid\n",
        "            self.activation_derivative = ActivationFunctions.sigmoid_derivative\n",
        "        elif self.activation == 'tanh':\n",
        "            self.activation_func = ActivationFunctions.tanh\n",
        "            self.activation_derivative = ActivationFunctions.tanh_derivative\n",
        "        elif self.activation == 'relu':\n",
        "            self.activation_func = ActivationFunctions.relu\n",
        "            self.activation_derivative = ActivationFunctions.relu_derivative\n",
        "        elif self.activation == 'leaky_relu':\n",
        "            self.activation_func = ActivationFunctions.leaky_relu\n",
        "            self.activation_derivative = ActivationFunctions.leaky_relu_derivative\n",
        "        else:\n",
        "            raise ValueError(f\"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {self.activation}\")\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        å‰å‘ä¼ æ’­\n",
        "        å‚æ•°:\n",
        "            X: è¾“å…¥çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(n_samples, n_features)\n",
        "        è¿”å›:\n",
        "            output: è¾“å‡ºï¼Œå½¢çŠ¶ä¸º(n_samples,)\n",
        "            z: å‡€è¾“å…¥ï¼Œå½¢çŠ¶ä¸º(n_samples,)\n",
        "        \"\"\"\n",
        "        # è®¡ç®—å‡€è¾“å…¥ï¼šz = w^T * x + b\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        # åº”ç”¨æ¿€æ´»å‡½æ•°\n",
        "        output = self.activation_func(z)\n",
        "        return output, z\n",
        "    \n",
        "    def backward(self, X, z, output, grad_output):\n",
        "        \"\"\"\n",
        "        åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰\n",
        "        \"\"\"\n",
        "        # è®¡ç®—æ¿€æ´»å‡½æ•°çš„å¯¼æ•°\n",
        "        activation_derivative = self.activation_derivative(z)\n",
        "        # é“¾å¼æ³•åˆ™ï¼šâˆ‚L/âˆ‚z = âˆ‚L/âˆ‚output * âˆ‚output/âˆ‚z\n",
        "        grad_z = grad_output * activation_derivative\n",
        "        # è®¡ç®—æƒé‡å’Œåç½®æ¢¯åº¦\n",
        "        grad_weights = np.mean(grad_z[:, np.newaxis] * X, axis=0)\n",
        "        grad_bias = np.mean(grad_z)\n",
        "        return grad_weights, grad_bias\n",
        "    \n",
        "    def update(self, grad_weights, grad_bias, learning_rate=0.01):\n",
        "        \"\"\"æ›´æ–°æƒé‡å’Œåç½®\"\"\"\n",
        "        self.weights -= learning_rate * grad_weights\n",
        "        self.bias -= learning_rate * grad_bias\n",
        "\n",
        "print(\"ç¥ç»å…ƒç±»å®šä¹‰å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. æ€»ç»“ä¸æ€è€ƒ\n",
        "\n",
        "### å…³é”®çŸ¥è¯†ç‚¹æ€»ç»“\n",
        "\n",
        "1. **æ¿€æ´»å‡½æ•°çš„ä½œç”¨**ï¼š\n",
        "   - å¼•å…¥éçº¿æ€§ï¼Œä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚æ¨¡å¼\n",
        "   - æ§åˆ¶è¾“å‡ºèŒƒå›´\n",
        "   - å½±å“æ¢¯åº¦ä¼ æ’­\n",
        "\n",
        "2. **å¸¸ç”¨æ¿€æ´»å‡½æ•°**ï¼š\n",
        "   - **Sigmoid**ï¼šè¾“å‡º[0,1]ï¼Œé€‚åˆæ¦‚ç‡è¾“å‡ºï¼Œä½†å®¹æ˜“æ¢¯åº¦æ¶ˆå¤±\n",
        "   - **Tanh**ï¼šè¾“å‡º[-1,1]ï¼Œé›¶ä¸­å¿ƒåŒ–ï¼Œæ¯”Sigmoidæ¢¯åº¦æ›´å¤§\n",
        "   - **ReLU**ï¼šæœ€å¸¸ç”¨ï¼Œè®¡ç®—ç®€å•ï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œä½†å¯èƒ½Dead ReLU\n",
        "   - **Leaky ReLU**ï¼šReLUçš„æ”¹è¿›ï¼Œè§£å†³Dead ReLUé—®é¢˜\n",
        "\n",
        "3. **é€‰æ‹©åŸåˆ™**ï¼š\n",
        "   - éšè—å±‚ï¼šé€šå¸¸ä½¿ç”¨ReLUæˆ–Leaky ReLU\n",
        "   - è¾“å‡ºå±‚ï¼šäºŒåˆ†ç±»ç”¨Sigmoidï¼Œå¤šåˆ†ç±»ç”¨Softmax\n",
        "\n",
        "4. **ç¥ç»å…ƒä¸æ„ŸçŸ¥å™¨çš„åŒºåˆ«**ï¼š\n",
        "   - æ„ŸçŸ¥å™¨ä½¿ç”¨é˜¶è·ƒå‡½æ•°ï¼ˆä¸å¯å¾®ï¼‰\n",
        "   - ç¥ç»å…ƒä½¿ç”¨å¯å¾®æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™è®­ç»ƒ\n",
        "\n",
        "### æ€è€ƒé—®é¢˜\n",
        "\n",
        "1. ä¸ºä»€ä¹ˆReLUåœ¨éšè—å±‚ä¸­æ¯”Sigmoidæ›´å¸¸ç”¨ï¼Ÿ\n",
        "2. æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ˜¯å¦‚ä½•äº§ç”Ÿçš„ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ\n",
        "3. ä¸ºä»€ä¹ˆTanhæ¯”Sigmoidæ›´é€‚åˆéšè—å±‚ï¼Ÿ\n",
        "\n",
        "### ä¸‹ä¸€æ­¥å­¦ä¹ \n",
        "\n",
        "- å­¦ä¹ å¦‚ä½•ç»„åˆå¤šä¸ªç¥ç»å…ƒï¼ˆç½‘ç»œç»“æ„ï¼‰\n",
        "- å­¦ä¹ å‰å‘ä¼ æ’­ç®—æ³•ï¼ˆå¤šå±‚ç½‘ç»œï¼‰\n",
        "- å­¦ä¹ åå‘ä¼ æ’­ç®—æ³•ï¼ˆå¦‚ä½•è®­ç»ƒå¤šå±‚ç½‘ç»œï¼‰\n",
        "\n",
        "---\n",
        "\n",
        "**æ­å–œï¼ä½ å·²ç»æŒæ¡äº†ç¥ç»å…ƒçš„åŸºæœ¬åŸç†å’Œå®ç°ï¼** ğŸ‰\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
