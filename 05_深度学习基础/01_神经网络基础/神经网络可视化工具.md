# ç¥ç»ç½‘ç»œå¯è§†åŒ–å·¥å…·æŒ‡å—

> **ç›®çš„**ï¼šå¸®åŠ©ç†è§£ç¥ç»ç½‘ç»œçš„å†…éƒ¨å·¥ä½œåŸç†ï¼Œé€šè¿‡å¯è§†åŒ–åŠ æ·±ç†è§£

---

## ğŸ“š ç›®å½•

- [ç½‘ç»œç»“æ„å¯è§†åŒ–](#ç½‘ç»œç»“æ„å¯è§†åŒ–)
- [æ¿€æ´»å€¼å¯è§†åŒ–](#æ¿€æ´»å€¼å¯è§†åŒ–)
- [æ¢¯åº¦å¯è§†åŒ–](#æ¢¯åº¦å¯è§†åŒ–)
- [æƒé‡å¯è§†åŒ–](#æƒé‡å¯è§†åŒ–)
- [è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–](#è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–)

---

## ç½‘ç»œç»“æ„å¯è§†åŒ–

### æ–¹æ³•1ï¼šä½¿ç”¨torchsummaryï¼ˆPyTorchï¼‰

**å®‰è£…**ï¼š
```bash
pip install torchsummary
```

**ä½¿ç”¨**ï¼š
```python
from torchsummary import summary
import torch.nn as nn

# å®šä¹‰ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
    
    def forward(self, x):
        x = x.view(-1, 784)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# å¯è§†åŒ–ç½‘ç»œç»“æ„
model = SimpleNet()
summary(model, input_size=(1, 28, 28))
```

**è¾“å‡º**ï¼š
```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1               [-1, 128]          100,480
            Linear-2                [-1, 64]           8,256
            Linear-3                [-1, 10]             650
================================================================
Total params: 109,386
Trainable params: 109,386
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.42
Estimated Total Size (MB): 0.42
----------------------------------------------------------------
```

---

### æ–¹æ³•2ï¼šä½¿ç”¨TensorBoardï¼ˆPyTorchï¼‰

**å®‰è£…**ï¼š
```bash
pip install tensorboard
```

**ä½¿ç”¨**ï¼š
```python
from torch.utils.tensorboard import SummaryWriter
import torch

# åˆ›å»ºwriter
writer = SummaryWriter('runs/network_structure')

# æ·»åŠ ç½‘ç»œå›¾
dummy_input = torch.randn(1, 1, 28, 28)
writer.add_graph(model, dummy_input)

# å¯åŠ¨TensorBoard
# åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir=runs
```

**æŸ¥çœ‹**ï¼šåœ¨æµè§ˆå™¨æ‰“å¼€ `http://localhost:6006`

---

### æ–¹æ³•3ï¼šä½¿ç”¨Netronï¼ˆé€šç”¨å·¥å…·ï¼‰

**å®‰è£…**ï¼š
```bash
pip install netron
```

**ä½¿ç”¨**ï¼š
```python
import torch
import netron

# ä¿å­˜æ¨¡å‹
torch.save(model, 'model.pth')

# å¯åŠ¨Netron
netron.start('model.pth', port=8080)
```

**æŸ¥çœ‹**ï¼šåœ¨æµè§ˆå™¨æ‰“å¼€ `http://localhost:8080`

---

## æ¿€æ´»å€¼å¯è§†åŒ–

### å¯è§†åŒ–æ¯å±‚çš„æ¿€æ´»å€¼

```python
import matplotlib.pyplot as plt
import torch.nn.functional as F

def visualize_activations(model, input_data, layer_names):
    """å¯è§†åŒ–æŒ‡å®šå±‚çš„æ¿€æ´»å€¼"""
    activations = {}
    
    def hook_fn(name):
        def hook(module, input, output):
            activations[name] = output.detach()
        return hook
    
    # æ³¨å†Œhook
    hooks = []
    for name, module in model.named_modules():
        if name in layer_names:
            hook = module.register_forward_hook(hook_fn(name))
            hooks.append(hook)
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        _ = model(input_data)
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, len(layer_names), figsize=(15, 3))
    for idx, name in enumerate(layer_names):
        activation = activations[name]
        # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªé€šé“
        if len(activation.shape) == 4:  # Convå±‚
            img = activation[0, 0].cpu().numpy()
        else:  # FCå±‚
            img = activation[0].cpu().numpy()
        
        axes[idx].imshow(img, cmap='viridis')
        axes[idx].set_title(f'{name}\nShape: {activation.shape}')
        axes[idx].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # ç§»é™¤hook
    for hook in hooks:
        hook.remove()

# ä½¿ç”¨ç¤ºä¾‹
visualize_activations(model, input_data, ['fc1', 'fc2', 'fc3'])
```

---

### å¯è§†åŒ–æ¿€æ´»å€¼åˆ†å¸ƒ

```python
import numpy as np

def visualize_activation_distribution(model, input_data, layer_name):
    """å¯è§†åŒ–æ¿€æ´»å€¼çš„åˆ†å¸ƒ"""
    activation_values = []
    
    def hook_fn(module, input, output):
        activation_values.append(output.detach().cpu().numpy().flatten())
    
    # æ³¨å†Œhook
    for name, module in model.named_modules():
        if name == layer_name:
            hook = module.register_forward_hook(hook_fn)
            break
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        _ = model(input_data)
    
    # å¯è§†åŒ–åˆ†å¸ƒ
    plt.figure(figsize=(10, 5))
    plt.hist(activation_values[0], bins=50, alpha=0.7)
    plt.xlabel('æ¿€æ´»å€¼')
    plt.ylabel('é¢‘æ•°')
    plt.title(f'{layer_name} æ¿€æ´»å€¼åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    hook.remove()
```

---

## æ¢¯åº¦å¯è§†åŒ–

### å¯è§†åŒ–æ¢¯åº¦æµ

```python
def visualize_gradients(model, input_data, target, loss_fn):
    """å¯è§†åŒ–å„å±‚çš„æ¢¯åº¦"""
    gradients = {}
    
    def hook_fn(name):
        def hook(module, grad_input, grad_output):
            if grad_output[0] is not None:
                gradients[name] = grad_output[0].detach()
        return hook
    
    # æ³¨å†Œhook
    hooks = []
    for name, module in model.named_modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            hook = module.register_backward_hook(hook_fn(name))
            hooks.append(hook)
    
    # å‰å‘å’Œåå‘ä¼ æ’­
    model.train()
    output = model(input_data)
    loss = loss_fn(output, target)
    loss.backward()
    
    # å¯è§†åŒ–æ¢¯åº¦
    layer_names = list(gradients.keys())
    grad_norms = [gradients[name].norm().item() for name in layer_names]
    
    plt.figure(figsize=(12, 6))
    plt.bar(range(len(layer_names)), grad_norms)
    plt.xlabel('å±‚')
    plt.ylabel('æ¢¯åº¦èŒƒæ•°')
    plt.title('å„å±‚æ¢¯åº¦èŒƒæ•°')
    plt.xticks(range(len(layer_names)), layer_names, rotation=45)
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # ç§»é™¤hook
    for hook in hooks:
        hook.remove()
```

---

### æ£€æµ‹æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

```python
def check_gradient_flow(model, input_data, target, loss_fn):
    """æ£€æŸ¥æ¢¯åº¦æµï¼Œæ£€æµ‹æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸"""
    gradients = {}
    
    def hook_fn(name):
        def hook(module, grad_input, grad_output):
            if grad_output[0] is not None:
                grad_norm = grad_output[0].norm().item()
                gradients[name] = grad_norm
                # æ‰“å°è­¦å‘Š
                if grad_norm < 1e-6:
                    print(f"âš ï¸ {name}: æ¢¯åº¦æ¶ˆå¤± (norm={grad_norm:.2e})")
                elif grad_norm > 100:
                    print(f"âš ï¸ {name}: æ¢¯åº¦çˆ†ç‚¸ (norm={grad_norm:.2e})")
        return hook
    
    # æ³¨å†Œhook
    hooks = []
    for name, module in model.named_modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            hook = module.register_backward_hook(hook_fn(name))
            hooks.append(hook)
    
    # å‰å‘å’Œåå‘ä¼ æ’­
    model.train()
    output = model(input_data)
    loss = loss_fn(output, target)
    loss.backward()
    
    # å¯è§†åŒ–
    layer_names = list(gradients.keys())
    grad_norms = [gradients[name] for name in layer_names]
    
    plt.figure(figsize=(12, 6))
    plt.semilogy(range(len(layer_names)), grad_norms, marker='o')
    plt.xlabel('å±‚ï¼ˆä»å‰åˆ°åï¼‰')
    plt.ylabel('æ¢¯åº¦èŒƒæ•°ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰')
    plt.title('æ¢¯åº¦æµæ£€æŸ¥')
    plt.xticks(range(len(layer_names)), layer_names, rotation=45)
    plt.axhline(y=1e-6, color='r', linestyle='--', label='æ¢¯åº¦æ¶ˆå¤±é˜ˆå€¼')
    plt.axhline(y=100, color='r', linestyle='--', label='æ¢¯åº¦çˆ†ç‚¸é˜ˆå€¼')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # ç§»é™¤hook
    for hook in hooks:
        hook.remove()
```

---

## æƒé‡å¯è§†åŒ–

### å¯è§†åŒ–æƒé‡åˆ†å¸ƒ

```python
def visualize_weights(model, layer_name):
    """å¯è§†åŒ–æŒ‡å®šå±‚çš„æƒé‡åˆ†å¸ƒ"""
    for name, param in model.named_parameters():
        if name == layer_name:
            weights = param.data.cpu().numpy().flatten()
            
            plt.figure(figsize=(12, 5))
            
            # ç›´æ–¹å›¾
            plt.subplot(1, 2, 1)
            plt.hist(weights, bins=50, alpha=0.7)
            plt.xlabel('æƒé‡å€¼')
            plt.ylabel('é¢‘æ•°')
            plt.title(f'{layer_name} æƒé‡åˆ†å¸ƒ')
            plt.grid(True, alpha=0.3)
            
            # æƒé‡çŸ©é˜µï¼ˆå¦‚æœæ˜¯2Dï¼‰
            if len(param.data.shape) == 2:
                plt.subplot(1, 2, 2)
                plt.imshow(param.data.cpu().numpy(), cmap='viridis', aspect='auto')
                plt.colorbar()
                plt.title(f'{layer_name} æƒé‡çŸ©é˜µ')
            
            plt.tight_layout()
            plt.show()
            break
```

---

### å¯è§†åŒ–æƒé‡å˜åŒ–

```python
def track_weight_changes(model, layer_name, optimizer, num_steps=100):
    """è·Ÿè¸ªæƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–"""
    weight_history = []
    
    for step in range(num_steps):
        # è®­ç»ƒä¸€æ­¥
        # ... (è®­ç»ƒä»£ç )
        
        # è®°å½•æƒé‡
        for name, param in model.named_parameters():
            if name == layer_name:
                weight_history.append(param.data.cpu().numpy().flatten())
                break
    
    # å¯è§†åŒ–
    weight_history = np.array(weight_history)
    plt.figure(figsize=(12, 6))
    for i in range(min(10, weight_history.shape[1])):  # åªæ˜¾ç¤ºå‰10ä¸ªæƒé‡
        plt.plot(weight_history[:, i], alpha=0.5, label=f'æƒé‡{i}')
    plt.xlabel('è®­ç»ƒæ­¥æ•°')
    plt.ylabel('æƒé‡å€¼')
    plt.title(f'{layer_name} æƒé‡å˜åŒ–')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
```

---

## è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–

### ä½¿ç”¨TensorBoardå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/experiment_1')

for epoch in range(num_epochs):
    # è®­ç»ƒ
    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)
    val_loss = validate(model, val_loader, criterion)
    
    # è®°å½•åˆ°TensorBoard
    writer.add_scalar('Loss/Train', train_loss, epoch)
    writer.add_scalar('Loss/Validation', val_loss, epoch)
    writer.add_scalar('Accuracy/Train', train_acc, epoch)
    writer.add_scalar('Accuracy/Validation', val_acc, epoch)
    
    # è®°å½•æƒé‡å’Œæ¢¯åº¦
    for name, param in model.named_parameters():
        writer.add_histogram(f'Weights/{name}', param, epoch)
        if param.grad is not None:
            writer.add_histogram(f'Gradients/{name}', param.grad, epoch)

writer.close()
```

**æŸ¥çœ‹**ï¼šè¿è¡Œ `tensorboard --logdir=runs`

---

### ä½¿ç”¨Matplotlibå®æ—¶å¯è§†åŒ–

```python
import matplotlib.pyplot as plt
from IPython.display import clear_output

def plot_training_progress(train_losses, val_losses, train_accs, val_accs):
    """å®æ—¶ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹"""
    clear_output(wait=True)
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # æŸå¤±æ›²çº¿
    axes[0].plot(train_losses, label='è®­ç»ƒæŸå¤±')
    axes[0].plot(val_losses, label='éªŒè¯æŸå¤±')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡æ›²çº¿
    axes[1].plot(train_accs, label='è®­ç»ƒå‡†ç¡®ç‡')
    axes[1].plot(val_accs, label='éªŒè¯å‡†ç¡®ç‡')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
train_losses, val_losses = [], []
train_accs, val_accs = [], []

for epoch in range(num_epochs):
    # è®­ç»ƒ...
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accs.append(train_acc)
    val_accs.append(val_acc)
    
    # æ¯10ä¸ªepochç»˜åˆ¶ä¸€æ¬¡
    if epoch % 10 == 0:
        plot_training_progress(train_losses, val_losses, train_accs, val_accs)
```

---

## ğŸ“– æ›´å¤šèµ„æº

- **è®­ç»ƒè¿‡ç¨‹ç›‘æ§æŒ‡å—**ï¼š[è®­ç»ƒè¿‡ç¨‹ç›‘æ§æŒ‡å—.md](./è®­ç»ƒè¿‡ç¨‹ç›‘æ§æŒ‡å—.md)
- **è°ƒè¯•æŠ€å·§å’Œå¸¸è§é”™è¯¯**ï¼š[è°ƒè¯•æŠ€å·§å’Œå¸¸è§é”™è¯¯.md](./è°ƒè¯•æŠ€å·§å’Œå¸¸è§é”™è¯¯.md)
- **ä»£ç ç¤ºä¾‹**ï¼š[ä»£ç ç¤ºä¾‹/](./ä»£ç ç¤ºä¾‹/)

---

**é€šè¿‡å¯è§†åŒ–ï¼Œä½ å¯ä»¥æ›´ç›´è§‚åœ°ç†è§£ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ï¼** ğŸ¨
