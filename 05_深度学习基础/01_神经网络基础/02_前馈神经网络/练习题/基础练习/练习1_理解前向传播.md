# 练习1：理解前向传播

## 练习目标

通过手动计算，深入理解前馈神经网络的前向传播过程。

## 练习要求

### 任务1：手动计算单层前向传播

给定一个单层网络：
- 输入：$\mathbf{x} = [1, 2]$
- 权重矩阵：$W = \begin{bmatrix} 0.5 & -0.3 \\ 0.2 & 0.4 \end{bmatrix}$
- 偏置：$\mathbf{b} = [0.1, -0.2]$
- 激活函数：ReLU

**要求**：
1. 计算线性变换：$\mathbf{z} = W \mathbf{x} + \mathbf{b}$
2. 应用激活函数：$\mathbf{a} = \text{ReLU}(\mathbf{z})$
3. 解释每一步的含义

**参考答案**：
```python
import numpy as np

x = np.array([1, 2])
W = np.array([[0.5, -0.3],
              [0.2, 0.4]])
b = np.array([0.1, -0.2])

# 线性变换
z = W @ x + b  # z = [0.5*1 + (-0.3)*2 + 0.1, 0.2*1 + 0.4*2 + (-0.2)]
              # z = [-0.1, 0.8]

# ReLU激活函数
a = np.maximum(0, z)  # a = [0, 0.8]

print(f"线性变换 z = {z}")
print(f"激活值 a = {a}")
```

**解释**：
- 线性变换将输入映射到新的空间
- ReLU激活函数引入非线性，将负值置为0

---

### 任务2：计算多层前向传播

给定一个2层网络：
- 输入：$\mathbf{x} = [1, 2]$
- 第1层（隐藏层）：
  - $W^{(1)} = \begin{bmatrix} 0.5 & -0.3 \\ 0.2 & 0.4 \end{bmatrix}$
  - $\mathbf{b}^{(1)} = [0.1, -0.2]$
  - 激活函数：ReLU
- 第2层（输出层）：
  - $W^{(2)} = \begin{bmatrix} 0.3 & -0.5 \end{bmatrix}$
  - $b^{(2)} = 0.2$
  - 激活函数：Sigmoid

**要求**：
1. 计算第1层的输出：$\mathbf{a}^{(1)}$
2. 计算第2层的输出：$a^{(2)}$
3. 绘制数据流动图

**参考答案**：
```python
import numpy as np

x = np.array([1, 2])

# 第1层
W1 = np.array([[0.5, -0.3],
               [0.2, 0.4]])
b1 = np.array([0.1, -0.2])
z1 = W1 @ x + b1  # z1 = [-0.1, 0.8]
a1 = np.maximum(0, z1)  # a1 = [0, 0.8]

# 第2层
W2 = np.array([[0.3, -0.5]])
b2 = np.array([0.2])
z2 = W2 @ a1 + b2  # z2 = [0.3*0 + (-0.5)*0.8 + 0.2] = [-0.2]
a2 = 1 / (1 + np.exp(-z2))  # a2 ≈ 0.45

print(f"第1层输出 a1 = {a1}")
print(f"第2层输出 a2 = {a2:.4f}")
```

---

### 任务3：批量前向传播

给定批量输入：
- $X = \begin{bmatrix} 1 & 2 \\ -1 & 1 \\ 0 & 0 \end{bmatrix}$（3个样本）

使用任务2的网络结构，计算批量前向传播。

**要求**：
1. 理解批量处理的矩阵形式
2. 计算所有样本的输出
3. 解释批量处理的优势

**参考答案**：
```python
import numpy as np

X = np.array([[1, 2],
              [-1, 1],
              [0, 0]])

# 第1层
W1 = np.array([[0.5, -0.3],
               [0.2, 0.4]])
b1 = np.array([0.1, -0.2])
Z1 = X @ W1.T + b1  # 注意转置
A1 = np.maximum(0, Z1)

# 第2层
W2 = np.array([[0.3, -0.5]])
b2 = np.array([0.2])
Z2 = A1 @ W2.T + b2
A2 = 1 / (1 + np.exp(-Z2))

print(f"批量输出形状: {A2.shape}")  # (3, 1)
print(f"批量输出:\n{A2}")
```

---

## 练习总结

1. **前向传播的核心**：
   - 线性变换：$\mathbf{z} = W \mathbf{x} + \mathbf{b}$
   - 激活函数：$\mathbf{a} = f(\mathbf{z})$

2. **多层网络**：
   - 每层的输出是下一层的输入
   - 数据从输入层流向输出层

3. **批量处理**：
   - 使用矩阵运算提高效率
   - 一次处理多个样本

