# 练习2：理解反向传播

## 练习目标

通过手动计算，深入理解反向传播算法的核心：链式法则和误差反向传播。

## 练习要求

### 任务1：理解链式法则

给定一个简单的单层网络：
- 输入：$x = 2$
- 权重：$w = 0.5$
- 偏置：$b = 0.3$
- 激活函数：$f(z) = z^2$（简化示例）
- 损失函数：$L = (y - \hat{y})^2$，其中$y = 1$（真实值）

**要求**：
1. 计算前向传播：$\hat{y} = f(wx + b)$
2. 计算损失：$L = (y - \hat{y})^2$
3. 使用链式法则计算梯度：$\frac{\partial L}{\partial w}$和$\frac{\partial L}{\partial b}$

**参考答案**：
```python
import numpy as np

# 前向传播
x = 2
w = 0.5
b = 0.3
y_true = 1

z = w * x + b  # z = 0.5*2 + 0.3 = 1.3
y_pred = z ** 2  # y_pred = 1.69
loss = (y_true - y_pred) ** 2  # loss = (1 - 1.69)^2 = 0.4761

# 反向传播（链式法则）
# ∂L/∂w = ∂L/∂y_pred * ∂y_pred/∂z * ∂z/∂w
dL_dy_pred = 2 * (y_pred - y_true)  # = 2*(1.69-1) = 1.38
dy_pred_dz = 2 * z  # = 2*1.3 = 2.6
dz_dw = x  # = 2

dL_dw = dL_dy_pred * dy_pred_dz * dz_dw  # = 1.38 * 2.6 * 2 = 7.176

# ∂L/∂b = ∂L/∂y_pred * ∂y_pred/∂z * ∂z/∂b
dz_db = 1
dL_db = dL_dy_pred * dy_pred_dz * dz_db  # = 1.38 * 2.6 * 1 = 3.588

print(f"损失: {loss:.4f}")
print(f"梯度 ∂L/∂w: {dL_dw:.4f}")
print(f"梯度 ∂L/∂b: {dL_db:.4f}")
```

---

### 任务2：计算多层网络的梯度

给定一个2层网络（与练习1任务2相同）：
- 输入：$\mathbf{x} = [1, 2]$
- 第1层：ReLU激活
- 第2层：Sigmoid激活
- 损失：$L = (y - \hat{y})^2$，其中$y = 1$

**要求**：
1. 完成前向传播
2. 计算输出层误差：$\delta^{(2)}$
3. 计算第1层误差：$\delta^{(1)}$
4. 计算所有参数的梯度

**参考答案**：
```python
import numpy as np

# 前向传播（与练习1任务2相同）
x = np.array([1, 2])
W1 = np.array([[0.5, -0.3],
               [0.2, 0.4]])
b1 = np.array([0.1, -0.2])
z1 = W1 @ x + b1  # z1 = [-0.1, 0.8]
a1 = np.maximum(0, z1)  # a1 = [0, 0.8]

W2 = np.array([[0.3, -0.5]])
b2 = np.array([0.2])
z2 = W2 @ a1 + b2  # z2 = [-0.2]
a2 = 1 / (1 + np.exp(-z2))  # a2 ≈ 0.45

y_true = 1
loss = (a2 - y_true) ** 2

# 反向传播
# 输出层误差: δ^(2) = ∂L/∂a2 * σ'(z2)
dL_da2 = 2 * (a2 - y_true)  # = 2*(0.45-1) = -1.1
sigmoid_derivative = a2 * (1 - a2)  # ≈ 0.45 * 0.55 = 0.2475
delta2 = dL_da2 * sigmoid_derivative  # ≈ -0.272

# 第1层误差: δ^(1) = (W2)^T @ δ^(2) * ReLU'(z1)
delta1 = (W2.T @ delta2) * (z1 > 0).astype(float)  # = [[0.3], [-0.5]] * [0, 1]
                                                      # = [[0], [-0.136]]

# 计算梯度
dW2 = delta2 @ a1.T  # = [-0.272] @ [0, 0.8] = [[0, -0.218]]
db2 = delta2  # = [-0.272]

dW1 = delta1 @ x.reshape(1, -1)  # = [[0, -0.136]] @ [[1, 2]] = [[0, 0], [0, -0.272]]
db1 = delta1  # = [[0], [-0.136]]

print(f"输出层误差 δ2: {delta2}")
print(f"第1层误差 δ1:\n{delta1}")
print(f"梯度 dW2:\n{dW2}")
print(f"梯度 dW1:\n{dW1}")
```

---

### 任务3：理解梯度消失问题

给定一个深层网络（4层），每层使用Sigmoid激活函数：
- 输入：$x = 1$
- 每层权重：$w = 0.5$（标量，简化）
- 每层偏置：$b = 0$
- 激活函数：Sigmoid

**要求**：
1. 计算前向传播
2. 计算每层的梯度（相对于第1层权重）
3. 观察梯度如何逐层衰减
4. 解释梯度消失的原因

**参考答案**：
```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(z):
    s = sigmoid(z)
    return s * (1 - s)

# 前向传播
x = 1
w = 0.5
b = 0

z1 = w * x + b  # z1 = 0.5
a1 = sigmoid(z1)  # a1 ≈ 0.622

z2 = w * a1 + b  # z2 ≈ 0.311
a2 = sigmoid(z2)  # a2 ≈ 0.577

z3 = w * a2 + b  # z3 ≈ 0.289
a3 = sigmoid(z3)  # a3 ≈ 0.572

z4 = w * a3 + b  # z4 ≈ 0.286
a4 = sigmoid(z4)  # a4 ≈ 0.571

# 假设损失函数对输出的梯度为1
dL_da4 = 1

# 反向传播，计算每层的梯度因子
grad_factor_4 = sigmoid_derivative(z4)  # ≈ 0.245
grad_factor_3 = sigmoid_derivative(z3) * w * grad_factor_4  # ≈ 0.245 * 0.5 * 0.245 ≈ 0.030
grad_factor_2 = sigmoid_derivative(z2) * w * grad_factor_3  # ≈ 0.245 * 0.5 * 0.030 ≈ 0.004
grad_factor_1 = sigmoid_derivative(z1) * w * grad_factor_2  # ≈ 0.245 * 0.5 * 0.004 ≈ 0.0005

print(f"第4层梯度因子: {grad_factor_4:.6f}")
print(f"第3层梯度因子: {grad_factor_3:.6f}")
print(f"第2层梯度因子: {grad_factor_2:.6f}")
print(f"第1层梯度因子: {grad_factor_1:.6f}")
print("\n观察：梯度逐层衰减，第1层的梯度非常小（梯度消失）")
```

---

## 练习总结

1. **链式法则**：
   - 反向传播的核心是链式法则
   - 梯度 = 损失对输出的梯度 × 激活函数导数 × 权重

2. **误差反向传播**：
   - 从输出层到输入层，逐层计算误差
   - 每层的误差 = 后一层的误差 × 权重转置 × 激活函数导数

3. **梯度消失**：
   - 深层网络中，梯度会逐层衰减
   - Sigmoid/Tanh的导数在两端接近0，导致梯度消失
   - 解决方案：使用ReLU、残差连接、批量归一化等

