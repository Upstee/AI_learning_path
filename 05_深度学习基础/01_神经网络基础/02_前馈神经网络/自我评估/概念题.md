# 前馈神经网络 - 概念题

## 一、选择题（每题2分，共20分）

### 1. 前馈神经网络中，信息流动的方向是？
A. 双向流动  
B. 从输入层到输出层  
C. 从输出层到输入层  
D. 随机流动  

**答案**：B  
**解释**：前馈神经网络中，信息只能从输入层流向输出层，没有反馈连接。

---

### 2. 反向传播算法的核心数学工具是？
A. 矩阵乘法  
B. 链式法则  
C. 梯度下降  
D. 激活函数  

**答案**：B  
**解释**：反向传播使用链式法则计算复合函数的梯度。

---

### 3. 梯度消失问题主要发生在？
A. 使用ReLU激活函数  
B. 使用Sigmoid激活函数且网络较深  
C. 学习率太大  
D. 数据量太少  

**答案**：B  
**解释**：Sigmoid函数的导数在两端接近0，深层网络中梯度会逐层衰减。

---

### 4. 前向传播中，第l层的输出计算公式是？
A. $a^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$  
B. $a^{(l)} = f(W^{(l)} a^{(l-1)} + b^{(l)})$  
C. $a^{(l)} = W^{(l)} x$  
D. $a^{(l)} = f(x)$  

**答案**：B  
**解释**：前向传播包括线性变换和激活函数。

---

### 5. 反向传播中，输出层的误差计算公式是？
A. $\delta^{(L)} = y - \hat{y}$  
B. $\delta^{(L)} = \frac{\partial L}{\partial a^{(L)}} \odot f'(z^{(L)})$  
C. $\delta^{(L)} = W^{(L)}$  
D. $\delta^{(L)} = a^{(L)}$  

**答案**：B  
**解释**：输出层误差是损失函数对输出的梯度乘以激活函数的导数。

---

### 6. 全连接层的特点是？
A. 每个神经元只连接部分输入  
B. 每个神经元连接所有输入  
C. 没有连接  
D. 随机连接  

**答案**：B  
**解释**：全连接层中，每个神经元都与上一层的所有神经元连接。

---

### 7. 批量归一化的主要作用是？
A. 减少参数数量  
B. 加速训练，稳定梯度  
C. 增加网络深度  
D. 减少计算量  

**答案**：B  
**解释**：批量归一化通过归一化激活值，加速训练并稳定梯度。

---

### 8. 梯度爆炸问题的解决方案不包括？
A. 梯度裁剪  
B. 权重初始化  
C. 增加学习率  
D. 批量归一化  

**答案**：C  
**解释**：增加学习率会加剧梯度爆炸，应该减小学习率或使用梯度裁剪。

---

### 9. 前馈神经网络无法直接处理？
A. 分类问题  
B. 回归问题  
C. 序列数据（需要记忆）  
D. 非线性问题  

**答案**：C  
**解释**：前馈网络没有记忆能力，无法直接处理序列数据，需要使用RNN。

---

### 10. 网络容量主要取决于？
A. 输入维度  
B. 输出维度  
C. 网络深度和宽度  
D. 学习率  

**答案**：C  
**解释**：网络容量（表达能力）主要取决于深度和宽度。

---

## 二、填空题（每空2分，共20分）

### 1. 前向传播的数学公式：
- 线性变换：$z^{(l)} = $________
- 激活函数：$a^{(l)} = $________

**答案**：
- $z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$
- $a^{(l)} = f^{(l)}(z^{(l)})$

---

### 2. 反向传播的误差传播公式：
- 输出层：$\delta^{(L)} = $________
- 隐藏层：$\delta^{(l)} = $________

**答案**：
- $\delta^{(L)} = \frac{\partial L}{\partial a^{(L)}} \odot f'^{(L)}(z^{(L)})$
- $\delta^{(l)} = ((W^{(l+1)})^T \delta^{(l+1)}) \odot f'^{(l)}(z^{(l)})$

---

### 3. 参数梯度计算公式：
- 权重梯度：$\frac{\partial L}{\partial W^{(l)}} = $________
- 偏置梯度：$\frac{\partial L}{\partial b^{(l)}} = $________

**答案**：
- $\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T$
- $\frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}$

---

### 4. 梯度消失的原因：
- 激活函数导数________（填"接近0"或"接近1"）
- 权重初始化________（填"过小"或"过大"）

**答案**：
- 接近0
- 过小

---

### 5. 解决梯度消失的方法（至少两个）：
- ________
- ________

**答案**：使用ReLU激活函数、残差连接、批量归一化、梯度裁剪等（任选两个）

---

## 三、简答题（每题10分，共30分）

### 1. 解释前向传播和反向传播的区别和联系。

**参考答案**：
- **区别**：
  - 前向传播：从输入到输出，计算预测值
  - 反向传播：从输出到输入，计算梯度
- **联系**：
  - 反向传播需要前向传播的结果（激活值、净输入）
  - 两者结合完成一次训练迭代

---

### 2. 为什么深层网络比浅层网络更难训练？

**参考答案**：
- **梯度消失/爆炸**：深层网络中梯度传播困难
- **优化困难**：损失函数非凸，容易陷入局部最优
- **过拟合**：参数多，容易过拟合
- **计算复杂度**：计算量大，训练时间长

---

### 3. 解释全连接层、隐藏层、输出层的作用。

**参考答案**：
- **全连接层**：每个神经元连接所有输入，学习特征组合
- **隐藏层**：提取特征，学习数据的层次化表示
- **输出层**：产生最终预测，根据任务选择激活函数

---

## 四、计算题（每题15分，共30分）

### 1. 给定一个2层网络：
- 输入：$x = [1, 2]$
- 第1层：$W^{(1)} = \begin{bmatrix} 0.5 & -0.3 \\ 0.2 & 0.4 \end{bmatrix}$，$b^{(1)} = [0.1, -0.2]$，激活函数：ReLU
- 第2层：$W^{(2)} = \begin{bmatrix} 0.3 & -0.5 \end{bmatrix}$，$b^{(2)} = 0.2$，激活函数：Sigmoid

计算前向传播的完整过程。

**参考答案**：
1. 第1层线性变换：$z^{(1)} = W^{(1)} x + b^{(1)} = [-0.1, 0.8]$
2. 第1层激活：$a^{(1)} = \text{ReLU}(z^{(1)}) = [0, 0.8]$
3. 第2层线性变换：$z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} = -0.2$
4. 第2层激活：$a^{(2)} = \sigma(z^{(2)}) \approx 0.45$

---

### 2. 在题目1的基础上，假设真实标签$y = 1$，损失函数为$L = (y - \hat{y})^2$，计算反向传播的梯度。

**参考答案**：
1. 输出层误差：$\delta^{(2)} = 2(a^{(2)} - y) \cdot \sigma'(z^{(2)}) \approx -0.272$
2. 第1层误差：$\delta^{(1)} = (W^{(2)})^T \delta^{(2)} \odot \text{ReLU}'(z^{(1)}) = [0, -0.136]$
3. 梯度：
   - $\frac{\partial L}{\partial W^{(2)}} = \delta^{(2)} (a^{(1)})^T = [[0, -0.218]]$
   - $\frac{\partial L}{\partial b^{(2)}} = \delta^{(2)} = -0.272$
   - $\frac{\partial L}{\partial W^{(1)}} = \delta^{(1)} x^T = [[0, 0], [0, -0.272]]$
   - $\frac{\partial L}{\partial b^{(1)}} = \delta^{(1)} = [0, -0.136]$

---

## 评分标准

- **选择题**：每题2分，共20分
- **填空题**：每空2分，共20分
- **简答题**：每题10分，共30分
- **计算题**：每题15分，共30分
- **总分**：100分
- **及格线**：60分

---

## 参考答案获取

详细答案请参考：
- [前馈神经网络原理详解](../../理论笔记/前馈神经网络原理详解.md)
- [从零实现前馈神经网络](../../代码示例/01_从零实现前馈神经网络.ipynb)

