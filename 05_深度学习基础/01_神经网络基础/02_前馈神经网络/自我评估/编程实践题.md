# 前馈神经网络 - 编程实践题

## 题目1：实现前向传播（30分）

### 要求

实现一个函数`forward_propagation`，计算前馈神经网络的前向传播。

**函数签名**：
```python
def forward_propagation(X, weights, biases, activations):
    """
    参数:
    - X: 输入数据，形状为 (n_samples, n_features)
    - weights: 权重列表，每个元素是 (n_l, n_{l-1}) 的矩阵
    - biases: 偏置列表，每个元素是 (n_l, 1) 的向量
    - activations: 激活函数列表
    
    返回:
    - activations_list: 所有层的激活值列表
    - zs_list: 所有层的净输入列表
    """
    pass
```

### 测试用例

```python
import numpy as np

# 测试数据
X = np.array([[1, 2], [3, 4]])
weights = [np.array([[0.5, -0.3], [0.2, 0.4]]), 
           np.array([[0.3, -0.5]])]
biases = [np.array([[0.1], [-0.2]]), 
          np.array([[0.2]])]

def relu(z):
    return np.maximum(0, z)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

activations = [relu, sigmoid]

# 调用函数
activations_list, zs_list = forward_propagation(X, weights, biases, activations)

# 验证结果
assert len(activations_list) == 3  # 输入层 + 2个隐藏层
assert len(zs_list) == 2  # 2个隐藏层
```

### 评分标准

- 正确实现线性变换：10分
- 正确应用激活函数：10分
- 正确处理批量输入：10分

---

## 题目2：实现反向传播（40分）

### 要求

实现一个函数`backward_propagation`，计算前馈神经网络的反向传播。

**函数签名**：
```python
def backward_propagation(X, y, activations_list, zs_list, weights, 
                         activation_derivatives, loss_derivative):
    """
    参数:
    - X: 输入数据
    - y: 真实标签
    - activations_list: 前向传播得到的激活值列表
    - zs_list: 前向传播得到的净输入列表
    - weights: 权重列表
    - activation_derivatives: 激活函数导数列表
    - loss_derivative: 损失函数对输出的导数函数
    
    返回:
    - dW: 权重梯度列表
    - db: 偏置梯度列表
    """
    pass
```

### 测试用例

```python
# 使用题目1的前向传播结果
# 假设 y = np.array([[1], [0]])
# loss_derivative = lambda y_pred, y_true: y_pred - y_true

dW, db = backward_propagation(X, y, activations_list, zs_list, 
                              weights, activation_derivatives, loss_derivative)

# 验证结果
assert len(dW) == len(weights)
assert len(db) == len(biases)
assert dW[0].shape == weights[0].shape
```

### 评分标准

- 正确计算输出层误差：15分
- 正确实现误差反向传播：15分
- 正确计算参数梯度：10分

---

## 题目3：完整训练循环（30分）

### 要求

实现一个完整的训练函数，包括前向传播、反向传播和参数更新。

**函数签名**：
```python
def train_network(X_train, y_train, network, epochs=1000, learning_rate=0.01):
    """
    参数:
    - X_train: 训练数据
    - y_train: 训练标签
    - network: 神经网络对象（包含forward、backward、update方法）
    - epochs: 训练轮数
    - learning_rate: 学习率
    
    返回:
    - history: 训练历史（损失列表）
    """
    pass
```

### 测试用例

```python
# 使用XOR数据集
X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_train = np.array([[0], [1], [1], [0]])

# 创建网络（需要自己实现网络类）
network = SimpleNeuralNetwork()

# 训练
history = train_network(X_train, y_train, network, epochs=1000)

# 验证
assert len(history['loss']) == 1000
assert history['loss'][-1] < history['loss'][0]  # 损失应该下降
```

### 评分标准

- 正确实现训练循环：15分
- 正确计算和记录损失：10分
- 代码结构清晰，有注释：5分

---

## 提交要求

1. 所有代码必须可以运行
2. 添加必要的注释
3. 提交Jupyter Notebook或Python脚本
4. 包含测试用例的运行结果

---

## 参考答案

可以参考：
- [从零实现前馈神经网络](../../代码示例/01_从零实现前馈神经网络.ipynb)
- [练习3：实现简单前馈网络](../../练习题/基础练习/练习3_实现简单前馈网络.ipynb)

