{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# æ–‡æœ¬æƒ…æ„Ÿåˆ†æ\n",
        "\n",
        "## ğŸ“‹ æ¦‚è¿°\n",
        "\n",
        "æ–‡æœ¬æƒ…æ„Ÿåˆ†ææ˜¯è‚¡ç¥¨åˆ†æä¸­çš„é‡è¦æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ–‡æœ¬æ•°æ®ï¼ˆæ–°é—»ã€ç¤¾äº¤åª’ä½“ã€ç ”æŠ¥ç­‰ï¼‰çš„æƒ…æ„Ÿå€¾å‘æ¥é¢„æµ‹å¸‚åœºæƒ…ç»ªå’Œè‚¡ä»·èµ°åŠ¿ã€‚æœ¬ç« èŠ‚å°†è¯¦ç»†ä»‹ç»æ–‡æœ¬æƒ…æ„Ÿåˆ†æçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿè¯å…¸ã€æœºå™¨å­¦ä¹ æ–¹æ³•ã€æ·±åº¦å­¦ä¹ æ–¹æ³•ç­‰ã€‚\n",
        "\n",
        "**å­¦ä¹ æ–¹å¼**ï¼šæœ¬æ–‡ä»¶æ˜¯Jupyter Notebookæ ¼å¼ï¼Œä½ å¯ä»¥è¾¹çœ‹è¾¹è¿è¡Œä»£ç ï¼Œé€šè¿‡å®é™…ä»£ç ç¤ºä¾‹ç†è§£æ–‡æœ¬æƒ…æ„Ÿåˆ†æã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…å¿…è¦çš„åº“ï¼ˆå¦‚æœè¿˜æ²¡æœ‰å®‰è£…ï¼‰\n",
        "# !pip install pandas numpy matplotlib jieba scikit-learn\n",
        "\n",
        "# å¯¼å…¥åº“\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# å°è¯•å¯¼å…¥jiebaï¼ˆä¸­æ–‡åˆ†è¯ï¼‰\n",
        "try:\n",
        "    import jieba\n",
        "    JIEBA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    JIEBA_AVAILABLE = False\n",
        "    print(\"jiebaä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬\")\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“– æ ¸å¿ƒå†…å®¹\n",
        "\n",
        "### 1. æ–‡æœ¬æƒ…æ„Ÿåˆ†æçš„åŸºæœ¬æ¦‚å¿µ\n",
        "\n",
        "#### 1.1 ä»€ä¹ˆæ˜¯æƒ…æ„Ÿåˆ†æ\n",
        "\n",
        "**æƒ…æ„Ÿåˆ†æï¼ˆSentiment Analysisï¼‰**ï¼š\n",
        "- åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘\n",
        "- åˆ¤æ–­æ–‡æœ¬æ˜¯æ­£é¢ã€è´Ÿé¢è¿˜æ˜¯ä¸­æ€§\n",
        "- é‡åŒ–æƒ…æ„Ÿå¼ºåº¦\n",
        "\n",
        "#### 1.2 åº”ç”¨åœºæ™¯\n",
        "\n",
        "**å¸‚åœºæƒ…ç»ªåˆ†æ**ï¼š\n",
        "- åˆ†ææ–°é—»æƒ…æ„Ÿ\n",
        "- åˆ†æç¤¾äº¤åª’ä½“æƒ…æ„Ÿ\n",
        "- åˆ†æç ”æŠ¥æƒ…æ„Ÿ\n",
        "\n",
        "**é¢„æµ‹åº”ç”¨**ï¼š\n",
        "- æƒ…æ„Ÿä¸è‚¡ä»·çš„å…³ç³»\n",
        "- æƒ…æ„ŸæŒ‡æ ‡æ„å»º\n",
        "- äº¤æ˜“ä¿¡å·ç”Ÿæˆ\n",
        "\n",
        "è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„æƒ…æ„Ÿåˆ†æå·¥å…·ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç®€å•çš„æƒ…æ„Ÿåˆ†æå·¥å…·\n",
        "class SentimentAnalyzer:\n",
        "    \"\"\"æƒ…æ„Ÿåˆ†æå™¨\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # æ­£é¢è¯æ±‡\n",
        "        self.positive_words = ['æ¶¨', 'ä¸Šæ¶¨', 'åˆ©å¥½', 'çœ‹å¥½', 'æ¨è', 'ä¹°å…¥', 'å¼ºåŠ¿', \n",
        "                              'çªç ´', 'å¢é•¿', 'ç›ˆåˆ©', 'ä¼˜ç§€', 'è‰¯å¥½', 'ç§¯æ', 'ä¹è§‚']\n",
        "        # è´Ÿé¢è¯æ±‡\n",
        "        self.negative_words = ['è·Œ', 'ä¸‹è·Œ', 'åˆ©ç©º', 'çœ‹ç©º', 'å–å‡º', 'å¼±åŠ¿', \n",
        "                              'ç ´ä½', 'ä¸‹é™', 'äºæŸ', 'ç³Ÿç³•', 'ä¸è‰¯', 'æ¶ˆæ', 'æ‚²è§‚']\n",
        "        # ç¨‹åº¦è¯\n",
        "        self.intensity_words = {\n",
        "            'éå¸¸': 1.5, 'æå…¶': 1.5, 'ç‰¹åˆ«': 1.3, 'å¾ˆ': 1.2, 'è¾ƒ': 1.1,\n",
        "            'ç¨å¾®': 0.8, 'ç•¥å¾®': 0.8, 'æœ‰ç‚¹': 0.9\n",
        "        }\n",
        "    \n",
        "    def analyze(self, text):\n",
        "        \"\"\"\n",
        "        åˆ†ææ–‡æœ¬æƒ…æ„Ÿ\n",
        "        \n",
        "        å‚æ•°:\n",
        "        text: æ–‡æœ¬å†…å®¹\n",
        "        \n",
        "        è¿”å›:\n",
        "        sentiment_score: æƒ…æ„Ÿåˆ†æ•°ï¼ˆ-1åˆ°1ä¹‹é—´ï¼‰\n",
        "        \"\"\"\n",
        "        if not text or pd.isna(text):\n",
        "            return 0\n",
        "        \n",
        "        text = str(text)\n",
        "        score = 0\n",
        "        \n",
        "        # æ£€æŸ¥æ­£é¢è¯æ±‡\n",
        "        for word in self.positive_words:\n",
        "            count = text.count(word)\n",
        "            if count > 0:\n",
        "                score += count * 0.1\n",
        "        \n",
        "        # æ£€æŸ¥è´Ÿé¢è¯æ±‡\n",
        "        for word in self.negative_words:\n",
        "            count = text.count(word)\n",
        "            if count > 0:\n",
        "                score -= count * 0.1\n",
        "        \n",
        "        # è°ƒæ•´ç¨‹åº¦\n",
        "        for intensity_word, multiplier in self.intensity_words.items():\n",
        "            if intensity_word in text:\n",
        "                # ç®€åŒ–å¤„ç†ï¼šå¦‚æœç¨‹åº¦è¯åœ¨æ–‡æœ¬ä¸­ï¼Œè°ƒæ•´åˆ†æ•°\n",
        "                score *= multiplier\n",
        "        \n",
        "        # å½’ä¸€åŒ–åˆ°-1åˆ°1ä¹‹é—´\n",
        "        score = np.clip(score, -1, 1)\n",
        "        \n",
        "        return score\n",
        "    \n",
        "    def classify(self, text):\n",
        "        \"\"\"åˆ†ç±»æƒ…æ„Ÿ\"\"\"\n",
        "        score = self.analyze(text)\n",
        "        if score > 0.1:\n",
        "            return 'æ­£é¢'\n",
        "        elif score < -0.1:\n",
        "            return 'è´Ÿé¢'\n",
        "        else:\n",
        "            return 'ä¸­æ€§'\n",
        "\n",
        "# ç¤ºä¾‹æ–‡æœ¬\n",
        "sample_texts = [\n",
        "    \"è¯¥è‚¡ç¥¨ä»Šæ—¥å¤§å¹…ä¸Šæ¶¨ï¼Œå¸‚åœºçœ‹å¥½å…¶æœªæ¥å‘å±•å‰æ™¯\",\n",
        "    \"å…¬å¸ä¸šç»©ä¸‹æ»‘ï¼Œè‚¡ä»·å¯èƒ½ç»§ç»­ä¸‹è·Œ\",\n",
        "    \"å¸‚åœºè¡¨ç°å¹³ç¨³ï¼ŒæŠ•èµ„è€…ä¿æŒè§‚æœ›æ€åº¦\",\n",
        "    \"éå¸¸çœ‹å¥½è¿™åªè‚¡ç¥¨ï¼Œå¼ºçƒˆæ¨èä¹°å…¥\",\n",
        "    \"åˆ©ç©ºæ¶ˆæ¯ä¸æ–­ï¼Œå»ºè®®å–å‡º\"\n",
        "]\n",
        "\n",
        "# åˆ›å»ºæƒ…æ„Ÿåˆ†æå™¨\n",
        "analyzer = SentimentAnalyzer()\n",
        "\n",
        "# åˆ†ææƒ…æ„Ÿ\n",
        "results = []\n",
        "for text in sample_texts:\n",
        "    score = analyzer.analyze(text)\n",
        "    sentiment = analyzer.classify(text)\n",
        "    results.append({\n",
        "        'æ–‡æœ¬': text,\n",
        "        'æƒ…æ„Ÿåˆ†æ•°': score,\n",
        "        'æƒ…æ„Ÿåˆ†ç±»': sentiment\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"æ–‡æœ¬æƒ…æ„Ÿåˆ†æç»“æœï¼š\")\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯è§†åŒ–æƒ…æ„Ÿåˆ†æç»“æœ\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "scores = results_df['æƒ…æ„Ÿåˆ†æ•°'].values\n",
        "colors = ['green' if s > 0.1 else 'red' if s < -0.1 else 'gray' for s in scores]\n",
        "\n",
        "bars = ax.barh(range(len(scores)), scores, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "ax.set_yticks(range(len(sample_texts)))\n",
        "ax.set_yticklabels([f\"æ–‡æœ¬{i+1}\" for i in range(len(sample_texts))])\n",
        "ax.set_xlabel('æƒ…æ„Ÿåˆ†æ•°')\n",
        "ax.set_title('æ–‡æœ¬æƒ…æ„Ÿåˆ†æç»“æœ', fontsize=14, fontweight='bold')\n",
        "ax.set_xlim(-1, 1)\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# æ ‡æ³¨æ•°å€¼\n",
        "for i, (score, sentiment) in enumerate(zip(scores, results_df['æƒ…æ„Ÿåˆ†ç±»'])):\n",
        "    ax.text(score, i, f'{score:.2f} ({sentiment})', \n",
        "           ha='left' if score >= 0 else 'right', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. æƒ…æ„Ÿåˆ†æä¸è‚¡ä»·å…³ç³»\n",
        "\n",
        "è®©æˆ‘ä»¬åˆ†ææƒ…æ„Ÿåˆ†æ•°ä¸è‚¡ä»·çš„å…³ç³»ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ¨¡æ‹Ÿæƒ…æ„Ÿæ•°æ®ä¸è‚¡ä»·å…³ç³»\n",
        "np.random.seed(42)\n",
        "n_days = 100\n",
        "dates = pd.date_range(start='2024-01-01', periods=n_days, freq='D')\n",
        "dates = dates[dates.weekday < 5]\n",
        "\n",
        "# ç”Ÿæˆæƒ…æ„Ÿåˆ†æ•°ï¼ˆä¸æœªæ¥æ”¶ç›Šç›¸å…³ï¼‰\n",
        "sentiment_scores = np.random.normal(0, 0.3, len(dates))\n",
        "# æƒ…æ„Ÿåˆ†æ•°å½±å“æœªæ¥æ”¶ç›Š\n",
        "future_returns = sentiment_scores * 0.02 + np.random.randn(len(dates)) * 0.01\n",
        "\n",
        "# è®¡ç®—ç´¯è®¡ä»·æ ¼\n",
        "prices = 10 * (1 + future_returns).cumprod()\n",
        "\n",
        "# å¯è§†åŒ–\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
        "\n",
        "axes[0].plot(dates, sentiment_scores, linewidth=1.5, color='blue', alpha=0.7, label='æƒ…æ„Ÿåˆ†æ•°')\n",
        "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "axes[0].fill_between(dates, 0, sentiment_scores, where=(sentiment_scores > 0), \n",
        "                     alpha=0.3, color='green', label='æ­£é¢æƒ…æ„Ÿ')\n",
        "axes[0].fill_between(dates, 0, sentiment_scores, where=(sentiment_scores < 0), \n",
        "                     alpha=0.3, color='red', label='è´Ÿé¢æƒ…æ„Ÿ')\n",
        "axes[0].set_title('æƒ…æ„Ÿåˆ†æ•°æ—¶é—´åºåˆ—', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('æƒ…æ„Ÿåˆ†æ•°')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(dates, prices, linewidth=2, color='black', alpha=0.7, label='è‚¡ä»·')\n",
        "axes[1].set_title('è‚¡ä»·èµ°åŠ¿', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('æ—¥æœŸ')\n",
        "axes[1].set_ylabel('ä»·æ ¼')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# è®¡ç®—ç›¸å…³æ€§\n",
        "correlation = np.corrcoef(sentiment_scores, future_returns)[0, 1]\n",
        "print(f\"\\næƒ…æ„Ÿåˆ†æ•°ä¸æœªæ¥æ”¶ç›Šç‡çš„ç›¸å…³æ€§ï¼š{correlation:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
