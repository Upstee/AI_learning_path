{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# å¼ºåŒ–å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨\n",
        "\n",
        "## ğŸ“‹ æ¦‚è¿°\n",
        "\n",
        "å¼ºåŒ–å­¦ä¹ æ˜¯é‡åŒ–äº¤æ˜“ä¸­åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›çš„æ–¹æ³•ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜äº¤æ˜“ç­–ç•¥ã€‚æœ¬ç« èŠ‚å°†è¯¦ç»†ä»‹ç»å¼ºåŒ–å­¦ä¹ åœ¨é‡åŒ–äº¤æ˜“ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬Q-learningã€DQNã€PPOç­‰ç®—æ³•åœ¨äº¤æ˜“ç­–ç•¥ä¼˜åŒ–ä¸­çš„åº”ç”¨ã€‚\n",
        "\n",
        "**å­¦ä¹ æ–¹å¼**ï¼šæœ¬æ–‡ä»¶æ˜¯Jupyter Notebookæ ¼å¼ï¼Œä½ å¯ä»¥è¾¹çœ‹è¾¹è¿è¡Œä»£ç ï¼Œé€šè¿‡å®é™…ä»£ç ç¤ºä¾‹ç†è§£å¼ºåŒ–å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨ã€‚\n",
        "\n",
        "**æ³¨æ„**ï¼šæœ¬ç¤ºä¾‹ä½¿ç”¨ç®€åŒ–çš„æ¨¡å‹ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç®—æ³•å’Œæ›´å¤šçš„è®­ç»ƒã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…å¿…è¦çš„åº“ï¼ˆå¦‚æœè¿˜æ²¡æœ‰å®‰è£…ï¼‰\n",
        "# !pip install pandas numpy matplotlib gym\n",
        "\n",
        "# å¯¼å…¥åº“\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“– æ ¸å¿ƒå†…å®¹\n",
        "\n",
        "### 1. å¼ºåŒ–å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨åœºæ™¯\n",
        "\n",
        "#### 1.1 äº¤æ˜“ç­–ç•¥ä¼˜åŒ–\n",
        "\n",
        "**ç­–ç•¥å­¦ä¹ **ï¼š\n",
        "- å­¦ä¹ æœ€ä¼˜ä¹°å–æ—¶æœº\n",
        "- å­¦ä¹ æœ€ä¼˜ä»“ä½ç®¡ç†\n",
        "- å­¦ä¹ é£é™©æ§åˆ¶ç­–ç•¥\n",
        "\n",
        "**åŠ¨æ€è°ƒæ•´**ï¼š\n",
        "- æ ¹æ®å¸‚åœºçŠ¶æ€è°ƒæ•´ç­–ç•¥\n",
        "- è‡ªé€‚åº”å‚æ•°ä¼˜åŒ–\n",
        "\n",
        "#### 1.2 å¼ºåŒ–å­¦ä¹ è¦ç´ \n",
        "\n",
        "**çŠ¶æ€ï¼ˆStateï¼‰**ï¼š\n",
        "- ä»·æ ¼ã€æˆäº¤é‡ã€æŠ€æœ¯æŒ‡æ ‡\n",
        "- æŒä»“çŠ¶æ€ã€è´¦æˆ·èµ„é‡‘\n",
        "\n",
        "**åŠ¨ä½œï¼ˆActionï¼‰**ï¼š\n",
        "- ä¹°å…¥ã€å–å‡ºã€æŒæœ‰\n",
        "- ä»“ä½è°ƒæ•´\n",
        "\n",
        "**å¥–åŠ±ï¼ˆRewardï¼‰**ï¼š\n",
        "- æ”¶ç›Šã€å¤æ™®æ¯”ç‡\n",
        "- é£é™©è°ƒæ•´åæ”¶ç›Š\n",
        "\n",
        "è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç¯å¢ƒï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç®€åŒ–çš„äº¤æ˜“ç¯å¢ƒ\n",
        "class TradingEnvironment:\n",
        "    \"\"\"äº¤æ˜“ç¯å¢ƒ\"\"\"\n",
        "    \n",
        "    def __init__(self, prices, initial_capital=1000000):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–äº¤æ˜“ç¯å¢ƒ\n",
        "        \n",
        "        å‚æ•°:\n",
        "        prices: ä»·æ ¼åºåˆ—\n",
        "        initial_capital: åˆå§‹èµ„é‡‘\n",
        "        \"\"\"\n",
        "        self.prices = prices\n",
        "        self.initial_capital = initial_capital\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"é‡ç½®ç¯å¢ƒ\"\"\"\n",
        "        self.current_step = 0\n",
        "        self.capital = self.initial_capital\n",
        "        self.position = 0  # æŒä»“æ•°é‡\n",
        "        self.total_value = self.initial_capital\n",
        "        self.trades = []\n",
        "        return self.get_state()\n",
        "    \n",
        "    def get_state(self):\n",
        "        \"\"\"è·å–å½“å‰çŠ¶æ€\"\"\"\n",
        "        if self.current_step >= len(self.prices):\n",
        "            return None\n",
        "        \n",
        "        current_price = self.prices[self.current_step]\n",
        "        \n",
        "        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
        "        if self.current_step >= 20:\n",
        "            recent_prices = self.prices[max(0, self.current_step-20):self.current_step+1]\n",
        "            ma = np.mean(recent_prices)\n",
        "            volatility = np.std(recent_prices) / ma if ma > 0 else 0\n",
        "        else:\n",
        "            ma = current_price\n",
        "            volatility = 0\n",
        "        \n",
        "        # çŠ¶æ€ï¼šä»·æ ¼ã€å‡çº¿ã€æ³¢åŠ¨ç‡ã€æŒä»“æ¯”ä¾‹ã€èµ„é‡‘æ¯”ä¾‹\n",
        "        position_ratio = (self.position * current_price) / self.total_value if self.total_value > 0 else 0\n",
        "        capital_ratio = self.capital / self.initial_capital\n",
        "        \n",
        "        state = np.array([\n",
        "            current_price / self.prices[0],  # å½’ä¸€åŒ–ä»·æ ¼\n",
        "            ma / self.prices[0],  # å½’ä¸€åŒ–å‡çº¿\n",
        "            volatility,  # æ³¢åŠ¨ç‡\n",
        "            position_ratio,  # æŒä»“æ¯”ä¾‹\n",
        "            capital_ratio  # èµ„é‡‘æ¯”ä¾‹\n",
        "        ])\n",
        "        \n",
        "        return state\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        æ‰§è¡ŒåŠ¨ä½œ\n",
        "        \n",
        "        å‚æ•°:\n",
        "        action: 0=å–å‡º, 1=æŒæœ‰, 2=ä¹°å…¥\n",
        "        \"\"\"\n",
        "        if self.current_step >= len(self.prices) - 1:\n",
        "            return None, 0, True\n",
        "        \n",
        "        current_price = self.prices[self.current_step]\n",
        "        next_price = self.prices[self.current_step + 1]\n",
        "        \n",
        "        # æ‰§è¡ŒåŠ¨ä½œ\n",
        "        if action == 0:  # å–å‡º\n",
        "            if self.position > 0:\n",
        "                self.capital += self.position * current_price\n",
        "                self.position = 0\n",
        "        elif action == 2:  # ä¹°å…¥\n",
        "            if self.capital > 0:\n",
        "                shares = int(self.capital / current_price)\n",
        "                if shares > 0:\n",
        "                    self.position += shares\n",
        "                    self.capital -= shares * current_price\n",
        "        \n",
        "        # æ›´æ–°æ€»ä»·å€¼\n",
        "        prev_value = self.total_value\n",
        "        self.total_value = self.capital + self.position * next_price\n",
        "        \n",
        "        # è®¡ç®—å¥–åŠ±ï¼ˆæ”¶ç›Šç‡ï¼‰\n",
        "        reward = (self.total_value - prev_value) / prev_value if prev_value > 0 else 0\n",
        "        \n",
        "        # æ›´æ–°æ­¥æ•°\n",
        "        self.current_step += 1\n",
        "        \n",
        "        # æ£€æŸ¥æ˜¯å¦ç»“æŸ\n",
        "        done = self.current_step >= len(self.prices) - 1\n",
        "        \n",
        "        next_state = self.get_state()\n",
        "        \n",
        "        return next_state, reward, done\n",
        "    \n",
        "    def get_total_return(self):\n",
        "        \"\"\"è·å–æ€»æ”¶ç›Šç‡\"\"\"\n",
        "        return (self.total_value - self.initial_capital) / self.initial_capital\n",
        "\n",
        "# ç”Ÿæˆç¤ºä¾‹ä»·æ ¼æ•°æ®\n",
        "np.random.seed(42)\n",
        "n_days = 200\n",
        "prices = 10 + np.cumsum(np.random.randn(n_days) * 0.1)\n",
        "\n",
        "# åˆ›å»ºäº¤æ˜“ç¯å¢ƒ\n",
        "env = TradingEnvironment(prices, initial_capital=1000000)\n",
        "\n",
        "print(\"äº¤æ˜“ç¯å¢ƒåˆ›å»ºå®Œæˆï¼\")\n",
        "print(f\"ä»·æ ¼æ•°æ®ï¼š{len(prices)} å¤©\")\n",
        "print(f\"åˆå§‹èµ„é‡‘ï¼š{env.initial_capital:,.0f} å…ƒ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç®€åŒ–çš„Q-Learningç®—æ³•\n",
        "class QLearningAgent:\n",
        "    \"\"\"Q-Learningæ™ºèƒ½ä½“\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–Q-Learningæ™ºèƒ½ä½“\n",
        "        \n",
        "        å‚æ•°:\n",
        "        state_size: çŠ¶æ€ç©ºé—´å¤§å°\n",
        "        action_size: åŠ¨ä½œç©ºé—´å¤§å°\n",
        "        learning_rate: å­¦ä¹ ç‡\n",
        "        discount_factor: æŠ˜æ‰£å› å­\n",
        "        epsilon: æ¢ç´¢ç‡\n",
        "        epsilon_decay: æ¢ç´¢ç‡è¡°å‡\n",
        "        epsilon_min: æœ€å°æ¢ç´¢ç‡\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        # ç®€åŒ–çš„Qè¡¨ï¼ˆä½¿ç”¨çº¿æ€§å‡½æ•°è¿‘ä¼¼ï¼‰\n",
        "        self.weights = np.random.randn(state_size, action_size) * 0.01\n",
        "    \n",
        "    def get_state_index(self, state):\n",
        "        \"\"\"å°†è¿ç»­çŠ¶æ€ç¦»æ•£åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n",
        "        if state is None:\n",
        "            return None\n",
        "        # å°†çŠ¶æ€ç¦»æ•£åŒ–ä¸ºæ•´æ•°ç´¢å¼•ï¼ˆç®€åŒ–å¤„ç†ï¼‰\n",
        "        state_discrete = np.clip((state * 10).astype(int), 0, 9)\n",
        "        return tuple(state_discrete[:3])  # åªä½¿ç”¨å‰3ä¸ªç‰¹å¾\n",
        "    \n",
        "    def get_q_value(self, state, action):\n",
        "        \"\"\"è·å–Qå€¼\"\"\"\n",
        "        state_idx = self.get_state_index(state)\n",
        "        if state_idx is None:\n",
        "            return 0\n",
        "        # ç®€åŒ–ï¼šä½¿ç”¨çº¿æ€§ç»„åˆ\n",
        "        return np.dot(state[:self.state_size], self.weights[:, action])\n",
        "    \n",
        "    def choose_action(self, state):\n",
        "        \"\"\"é€‰æ‹©åŠ¨ä½œï¼ˆepsilon-greedyï¼‰\"\"\"\n",
        "        if state is None:\n",
        "            return 1  # æŒæœ‰\n",
        "        \n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.action_size)  # æ¢ç´¢\n",
        "        else:\n",
        "            q_values = [self.get_q_value(state, a) for a in range(self.action_size)]\n",
        "            return np.argmax(q_values)  # åˆ©ç”¨\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"æ›´æ–°Qå€¼\"\"\"\n",
        "        if state is None or next_state is None:\n",
        "            return\n",
        "        \n",
        "        current_q = self.get_q_value(state, action)\n",
        "        \n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            next_q_values = [self.get_q_value(next_state, a) for a in range(self.action_size)]\n",
        "            target_q = reward + self.discount_factor * max(next_q_values)\n",
        "        \n",
        "        # æ›´æ–°æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
        "        error = target_q - current_q\n",
        "        self.weights[:, action] += self.learning_rate * error * state[:self.state_size]\n",
        "        \n",
        "        # è¡°å‡æ¢ç´¢ç‡\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# è®­ç»ƒQ-Learningæ™ºèƒ½ä½“\n",
        "agent = QLearningAgent(state_size=5, action_size=3)\n",
        "\n",
        "n_episodes = 50\n",
        "episode_returns = []\n",
        "\n",
        "print(\"å¼€å§‹è®­ç»ƒQ-Learningæ™ºèƒ½ä½“...\")\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    \n",
        "    while True:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        \n",
        "        agent.update(state, action, reward, next_state, done)\n",
        "        \n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    episode_return = env.get_total_return()\n",
        "    episode_returns.append(episode_return)\n",
        "    \n",
        "    if (episode + 1) % 10 == 0:\n",
        "        print(f\"Episode {episode + 1}/{n_episodes}, æ”¶ç›Šç‡: {episode_return*100:.2f}%, æ¢ç´¢ç‡: {agent.epsilon:.3f}\")\n",
        "\n",
        "print(\"\\nè®­ç»ƒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. è®­ç»ƒç»“æœå¯è§†åŒ–\n",
        "\n",
        "è®©æˆ‘ä»¬å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­–ç•¥ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯è§†åŒ–è®­ç»ƒç»“æœ\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
        "\n",
        "# 1. è®­ç»ƒè¿‡ç¨‹\n",
        "axes[0].plot(episode_returns, linewidth=2, color='blue', alpha=0.7)\n",
        "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
        "axes[0].set_title('è®­ç»ƒè¿‡ç¨‹ï¼šæ¯è½®æ”¶ç›Šç‡', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('æ”¶ç›Šç‡')\n",
        "axes[0].set_xlabel('Episode')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# è®¡ç®—ç§»åŠ¨å¹³å‡\n",
        "if len(episode_returns) > 10:\n",
        "    moving_avg = pd.Series(episode_returns).rolling(window=10).mean()\n",
        "    axes[0].plot(moving_avg, linewidth=2, color='red', label='10è½®ç§»åŠ¨å¹³å‡', alpha=0.7)\n",
        "    axes[0].legend()\n",
        "\n",
        "# 2. æµ‹è¯•è®­ç»ƒå¥½çš„ç­–ç•¥\n",
        "test_env = TradingEnvironment(prices, initial_capital=1000000)\n",
        "state = test_env.reset()\n",
        "actions_taken = []\n",
        "portfolio_values = [test_env.total_value]\n",
        "\n",
        "while True:\n",
        "    action = agent.choose_action(state)\n",
        "    actions_taken.append(action)\n",
        "    next_state, reward, done = test_env.step(action)\n",
        "    \n",
        "    portfolio_values.append(test_env.total_value)\n",
        "    state = next_state\n",
        "    \n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# ç»˜åˆ¶ä»·æ ¼å’ŒåŠ¨ä½œ\n",
        "dates = pd.date_range(start='2024-01-01', periods=len(prices), freq='D')\n",
        "axes[1].plot(dates, prices, linewidth=2, label='ä»·æ ¼', color='black', alpha=0.7)\n",
        "\n",
        "# æ ‡æ³¨åŠ¨ä½œ\n",
        "buy_indices = [i for i, a in enumerate(actions_taken) if a == 2]\n",
        "sell_indices = [i for i, a in enumerate(actions_taken) if a == 0]\n",
        "\n",
        "if buy_indices:\n",
        "    axes[1].scatter([dates[i] for i in buy_indices], [prices[i] for i in buy_indices],\n",
        "                   marker='^', color='green', s=100, label='ä¹°å…¥', zorder=5)\n",
        "if sell_indices:\n",
        "    axes[1].scatter([dates[i] for i in sell_indices], [prices[i] for i in sell_indices],\n",
        "                   marker='v', color='red', s=100, label='å–å‡º', zorder=5)\n",
        "\n",
        "axes[1].set_title('è®­ç»ƒåçš„äº¤æ˜“ç­–ç•¥', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('ä»·æ ¼')\n",
        "axes[1].set_xlabel('æ—¥æœŸ')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "final_return = test_env.get_total_return()\n",
        "print(f\"\\næœ€ç»ˆç­–ç•¥è¡¨ç°ï¼š\")\n",
        "print(f\"æ€»æ”¶ç›Šç‡ï¼š{final_return*100:.2f}%\")\n",
        "print(f\"æœ€ç»ˆä»·å€¼ï¼š{test_env.total_value:,.0f} å…ƒ\")\n",
        "print(f\"ä¹°å…¥æ¬¡æ•°ï¼š{len(buy_indices)}\")\n",
        "print(f\"å–å‡ºæ¬¡æ•°ï¼š{len(sell_indices)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹æ¯”\n",
        "\n",
        "å¼ºåŒ–å­¦ä¹ åœ¨é‡åŒ–äº¤æ˜“ä¸­å¸¸ç”¨çš„ç®—æ³•åŒ…æ‹¬ï¼š\n",
        "\n",
        "**Q-Learning**ï¼š\n",
        "- å€¼å‡½æ•°æ–¹æ³•\n",
        "- é€‚åˆç¦»æ•£çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´\n",
        "- å®ç°ç®€å•\n",
        "\n",
        "**DQNï¼ˆDeep Q-Networkï¼‰**ï¼š\n",
        "- ä½¿ç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°\n",
        "- é€‚åˆè¿ç»­çŠ¶æ€ç©ºé—´\n",
        "- éœ€è¦è¾ƒå¤šæ•°æ®\n",
        "\n",
        "**PPOï¼ˆProximal Policy Optimizationï¼‰**ï¼š\n",
        "- ç­–ç•¥æ¢¯åº¦æ–¹æ³•\n",
        "- ç¨³å®šæ€§å¥½\n",
        "- é€‚åˆè¿ç»­åŠ¨ä½œç©ºé—´\n",
        "\n",
        "**A3Cï¼ˆAsynchronous Advantage Actor-Criticï¼‰**ï¼š\n",
        "- Actor-Criticæ–¹æ³•\n",
        "- å¯ä»¥å¹¶è¡Œè®­ç»ƒ\n",
        "- æ•ˆç‡é«˜\n",
        "\n",
        "## ğŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“\n",
        "\n",
        "1. **å¼ºåŒ–å­¦ä¹ è¦ç´ **ï¼šçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±\n",
        "2. **å¸¸ç”¨ç®—æ³•**ï¼šQ-Learningã€DQNã€PPOã€A3C\n",
        "3. **åº”ç”¨åœºæ™¯**ï¼šç­–ç•¥ä¼˜åŒ–ã€ä»“ä½ç®¡ç†ã€é£é™©æ§åˆ¶\n",
        "4. **è®­ç»ƒè¦ç‚¹**ï¼šæ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ã€å¥–åŠ±è®¾è®¡ã€ç¯å¢ƒè®¾è®¡\n",
        "\n",
        "## ğŸ› ï¸ å·¥å…·ä¸è½¯ä»¶\n",
        "\n",
        "### å¼ºåŒ–å­¦ä¹ æ¡†æ¶\n",
        "\n",
        "- **Gym**ï¼šOpenAIå¼€å‘çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒåº“\n",
        "- **Stable-Baselines3**ï¼šå¼ºåŒ–å­¦ä¹ ç®—æ³•åº“\n",
        "- **Ray RLlib**ï¼šåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ åº“\n",
        "- **å®‰è£…**ï¼š`pip install gym stable-baselines3`\n",
        "\n",
        "### æ³¨æ„äº‹é¡¹\n",
        "\n",
        "- âœ… **ç¯å¢ƒè®¾è®¡**ï¼šåˆç†è®¾è®¡çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±\n",
        "- âœ… **æ¢ç´¢ç­–ç•¥**ï¼šå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨\n",
        "- âœ… **å¥–åŠ±è®¾è®¡**ï¼šå¥–åŠ±å‡½æ•°è¦åˆç†\n",
        "- âš ï¸ **è®­ç»ƒæ—¶é—´**ï¼šå¼ºåŒ–å­¦ä¹ è®­ç»ƒéœ€è¦è¾ƒé•¿æ—¶é—´\n",
        "- âš ï¸ **ç¨³å®šæ€§**ï¼šæ³¨æ„ç®—æ³•ç¨³å®šæ€§\n",
        "\n",
        "## ğŸ”— ç›¸å…³çŸ¥è¯†ç‚¹\n",
        "\n",
        "- [ç›‘ç£å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨](./ç›‘ç£å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨.ipynb)\n",
        "- [æ·±åº¦å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨](./æ·±åº¦å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨.ipynb)\n",
        "- [ç­–ç•¥è®¾è®¡åŸç†](../02_ç­–ç•¥å¼€å‘/ç­–ç•¥è®¾è®¡åŸç†.ipynb)\n",
        "\n",
        "## ğŸ“š æ‹“å±•é˜…è¯»\n",
        "\n",
        "- ã€Šå¼ºåŒ–å­¦ä¹ ï¼šåŸç†ä¸Pythonå®ç°ã€‹- å¼ºåŒ–å­¦ä¹ ç»å…¸æ•™æ\n",
        "- ã€Šæ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‹- æ·±å…¥ç†è§£æ·±åº¦å¼ºåŒ–å­¦ä¹ \n",
        "- é‡åŒ–äº¤æ˜“ç›¸å…³è®ºæ–‡ - æœ€æ–°ç ”ç©¶æ–¹æ³•\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
