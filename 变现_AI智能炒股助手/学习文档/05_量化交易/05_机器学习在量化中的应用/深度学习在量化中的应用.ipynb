{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# æ·±åº¦å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨\n",
        "\n",
        "## ğŸ“‹ æ¦‚è¿°\n",
        "\n",
        "æ·±åº¦å­¦ä¹ æ˜¯é‡åŒ–äº¤æ˜“ä¸­åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›çš„æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œæ¥å­¦ä¹ å¤æ‚çš„å¸‚åœºæ¨¡å¼ã€‚æœ¬ç« èŠ‚å°†è¯¦ç»†ä»‹ç»æ·±åº¦å­¦ä¹ åœ¨é‡åŒ–äº¤æ˜“ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬LSTMã€CNNã€Transformerç­‰æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ã€ä»·æ ¼é¢„æµ‹ç­‰æ–¹é¢çš„åº”ç”¨ã€‚\n",
        "\n",
        "**å­¦ä¹ æ–¹å¼**ï¼šæœ¬æ–‡ä»¶æ˜¯Jupyter Notebookæ ¼å¼ï¼Œä½ å¯ä»¥è¾¹çœ‹è¾¹è¿è¡Œä»£ç ï¼Œé€šè¿‡å®é™…ä»£ç ç¤ºä¾‹ç†è§£æ·±åº¦å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨ã€‚\n",
        "\n",
        "**æ³¨æ„**ï¼šæœ¬ç¤ºä¾‹ä½¿ç”¨ç®€åŒ–çš„æ¨¡å‹ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç½‘ç»œç»“æ„å’Œæ›´å¤šçš„æ•°æ®ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…å¿…è¦çš„åº“ï¼ˆå¦‚æœè¿˜æ²¡æœ‰å®‰è£…ï¼‰\n",
        "# !pip install pandas numpy matplotlib scikit-learn tensorflow\n",
        "\n",
        "# å¯¼å…¥åº“\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# å°è¯•å¯¼å…¥TensorFlowï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "    print(\"TensorFlowå¯ç”¨ï¼Œå°†ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹\")\n",
        "except ImportError:\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    print(\"TensorFlowä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„æ¨¡æ‹Ÿæ¨¡å‹\")\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“– æ ¸å¿ƒå†…å®¹\n",
        "\n",
        "### 1. æ·±åº¦å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨åœºæ™¯\n",
        "\n",
        "#### 1.1 æ—¶é—´åºåˆ—é¢„æµ‹\n",
        "\n",
        "**ä»·æ ¼é¢„æµ‹**ï¼š\n",
        "- ä½¿ç”¨LSTMé¢„æµ‹æœªæ¥ä»·æ ¼\n",
        "- ä½¿ç”¨CNNæå–ä»·æ ¼æ¨¡å¼\n",
        "- ä½¿ç”¨Transformerå¤„ç†é•¿åºåˆ—\n",
        "\n",
        "**æ”¶ç›Šç‡é¢„æµ‹**ï¼š\n",
        "- é¢„æµ‹æœªæ¥æ”¶ç›Šç‡\n",
        "- é¢„æµ‹æ³¢åŠ¨ç‡\n",
        "\n",
        "#### 1.2 æ¨¡å¼è¯†åˆ«\n",
        "\n",
        "**Kçº¿å½¢æ€è¯†åˆ«**ï¼š\n",
        "- ä½¿ç”¨CNNè¯†åˆ«Kçº¿å½¢æ€\n",
        "- ä½¿ç”¨å›¾åƒè¯†åˆ«æŠ€æœ¯\n",
        "\n",
        "**å¸‚åœºçŠ¶æ€è¯†åˆ«**ï¼š\n",
        "- è¯†åˆ«å¸‚åœºè¶‹åŠ¿\n",
        "- è¯†åˆ«å¸‚åœºæ³¢åŠ¨çŠ¶æ€\n",
        "\n",
        "è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªLSTMæ¨¡å‹æ¥é¢„æµ‹è‚¡ç¥¨ä»·æ ¼ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç”Ÿæˆç¤ºä¾‹æ•°æ®\n",
        "np.random.seed(42)\n",
        "n_days = 1000\n",
        "dates = pd.date_range(start='2021-01-01', periods=n_days, freq='D')\n",
        "dates = dates[dates.weekday < 5]\n",
        "\n",
        "# ç”Ÿæˆä»·æ ¼æ•°æ®ï¼ˆå¸¦è¶‹åŠ¿å’Œå™ªå£°ï¼‰\n",
        "trend = np.linspace(10, 15, len(dates))\n",
        "noise = np.cumsum(np.random.randn(len(dates)) * 0.1)\n",
        "prices = trend + noise\n",
        "\n",
        "price_data = pd.DataFrame({\n",
        "    'Close': prices,\n",
        "    'Volume': np.random.randint(1000000, 10000000, len(dates))\n",
        "}, index=dates)\n",
        "\n",
        "# è®¡ç®—æŠ€æœ¯æŒ‡æ ‡\n",
        "price_data['Returns'] = price_data['Close'].pct_change()\n",
        "price_data['MA5'] = price_data['Close'].rolling(window=5).mean()\n",
        "price_data['MA20'] = price_data['Close'].rolling(window=20).mean()\n",
        "price_data['Volatility'] = price_data['Returns'].rolling(window=20).std()\n",
        "\n",
        "price_data = price_data.dropna()\n",
        "\n",
        "print(f\"æ•°æ®å‡†å¤‡å®Œæˆï¼å…± {len(price_data)} æ¡è®°å½•\")\n",
        "print(f\"ä»·æ ¼èŒƒå›´ï¼š{price_data['Close'].min():.2f} - {price_data['Close'].max():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. LSTMæ¨¡å‹ï¼šæ—¶é—´åºåˆ—é¢„æµ‹\n",
        "\n",
        "è®©æˆ‘ä»¬ä½¿ç”¨LSTMæ¥é¢„æµ‹æœªæ¥ä»·æ ¼ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å‡†å¤‡LSTMæ•°æ®\n",
        "def create_sequences(data, seq_length=60):\n",
        "    \"\"\"åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# ä½¿ç”¨æ”¶ç›˜ä»·\n",
        "values = price_data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# æ ‡å‡†åŒ–\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_values = scaler.fit_transform(values)\n",
        "\n",
        "# åˆ›å»ºåºåˆ—\n",
        "seq_length = 60\n",
        "X, y = create_sequences(scaled_values, seq_length)\n",
        "\n",
        "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "print(f\"è®­ç»ƒé›†ï¼š{len(X_train)} ä¸ªæ ·æœ¬\")\n",
        "print(f\"æµ‹è¯•é›†ï¼š{len(X_test)} ä¸ªæ ·æœ¬\")\n",
        "\n",
        "if TENSORFLOW_AVAILABLE:\n",
        "    # æ„å»ºLSTMæ¨¡å‹\n",
        "    model = Sequential([\n",
        "        LSTM(50, return_sequences=True, input_shape=(seq_length, 1)),\n",
        "        Dropout(0.2),\n",
        "        LSTM(50, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    \n",
        "    print(\"\\nLSTMæ¨¡å‹ç»“æ„ï¼š\")\n",
        "    model.summary()\n",
        "    \n",
        "    # è®­ç»ƒæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦æ›´å¤šepochsï¼‰\n",
        "    print(\"\\nå¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=32,\n",
        "        epochs=10,  # å®é™…åº”ç”¨éœ€è¦æ›´å¤šepochs\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # é¢„æµ‹\n",
        "    train_predict = model.predict(X_train)\n",
        "    test_predict = model.predict(X_test)\n",
        "    \n",
        "    # åæ ‡å‡†åŒ–\n",
        "    train_predict = scaler.inverse_transform(train_predict)\n",
        "    y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
        "    test_predict = scaler.inverse_transform(test_predict)\n",
        "    y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "    \n",
        "    # è¯„ä¼°\n",
        "    train_mse = mean_squared_error(y_train_actual, train_predict)\n",
        "    test_mse = mean_squared_error(y_test_actual, test_predict)\n",
        "    train_mae = mean_absolute_error(y_train_actual, train_predict)\n",
        "    test_mae = mean_absolute_error(y_test_actual, test_predict)\n",
        "    \n",
        "    print(f\"\\næ¨¡å‹è¯„ä¼°ï¼š\")\n",
        "    print(f\"è®­ç»ƒé›† - MSE: {train_mse:.4f}, MAE: {train_mae:.4f}\")\n",
        "    print(f\"æµ‹è¯•é›† - MSE: {test_mse:.4f}, MAE: {test_mae:.4f}\")\n",
        "    \n",
        "    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "    \n",
        "    # è®­ç»ƒæŸå¤±\n",
        "    axes[0].plot(history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)\n",
        "    axes[0].plot(history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)\n",
        "    axes[0].set_title('æ¨¡å‹è®­ç»ƒè¿‡ç¨‹', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # é¢„æµ‹ç»“æœ\n",
        "    test_dates = price_data.index[train_size + seq_length:train_size + seq_length + len(y_test)]\n",
        "    axes[1].plot(test_dates, y_test_actual.flatten(), label='å®é™…ä»·æ ¼', \n",
        "                linewidth=2, color='black', alpha=0.7)\n",
        "    axes[1].plot(test_dates, test_predict.flatten(), label='LSTMé¢„æµ‹', \n",
        "                linewidth=2, color='green', alpha=0.7)\n",
        "    axes[1].set_title('LSTMä»·æ ¼é¢„æµ‹ç»“æœ', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('æ—¥æœŸ')\n",
        "    axes[1].set_ylabel('ä»·æ ¼')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"\\næ³¨æ„ï¼šTensorFlowä¸å¯ç”¨ï¼Œä»¥ä¸‹æ˜¯ç®€åŒ–çš„æ¨¡æ‹Ÿç»“æœ\")\n",
        "    print(\"å®é™…åº”ç”¨ä¸­éœ€è¦å®‰è£…TensorFlow: pip install tensorflow\")\n",
        "    \n",
        "    # ä½¿ç”¨ç®€å•çš„ç§»åŠ¨å¹³å‡ä½œä¸ºæ¨¡æ‹Ÿ\n",
        "    test_predict = price_data['Close'].rolling(window=seq_length).mean().iloc[train_size + seq_length:].values\n",
        "    y_test_actual = price_data['Close'].iloc[train_size + seq_length:].values[:len(test_predict)]\n",
        "    \n",
        "    # å¯è§†åŒ–\n",
        "    fig, ax = plt.subplots(figsize=(15, 6))\n",
        "    test_dates = price_data.index[train_size + seq_length:train_size + seq_length + len(test_predict)]\n",
        "    ax.plot(test_dates, y_test_actual, label='å®é™…ä»·æ ¼', linewidth=2, color='black', alpha=0.7)\n",
        "    ax.plot(test_dates, test_predict, label='æ¨¡æ‹Ÿé¢„æµ‹ï¼ˆç§»åŠ¨å¹³å‡ï¼‰', linewidth=2, color='green', alpha=0.7)\n",
        "    ax.set_title('ä»·æ ¼é¢„æµ‹ï¼ˆæ¨¡æ‹Ÿï¼‰', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('æ—¥æœŸ')\n",
        "    ax.set_ylabel('ä»·æ ¼')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. CNNæ¨¡å‹ï¼šæ¨¡å¼è¯†åˆ«\n",
        "\n",
        "CNNå¯ä»¥ç”¨äºè¯†åˆ«Kçº¿å½¢æ€å’Œå¸‚åœºæ¨¡å¼ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNNæ¨¡å‹ç¤ºä¾‹ï¼ˆç”¨äºæ¨¡å¼è¯†åˆ«ï¼‰\n",
        "if TENSORFLOW_AVAILABLE:\n",
        "    # å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨å¤šä¸ªç‰¹å¾ï¼‰\n",
        "    feature_cols = ['Close', 'Volume', 'Returns', 'MA5', 'MA20', 'Volatility']\n",
        "    feature_data = price_data[feature_cols].values\n",
        "    \n",
        "    # æ ‡å‡†åŒ–\n",
        "    feature_scaler = MinMaxScaler()\n",
        "    scaled_features = feature_scaler.fit_transform(feature_data)\n",
        "    \n",
        "    # åˆ›å»ºåºåˆ—\n",
        "    X_cnn, y_cnn = create_sequences(scaled_features, seq_length)\n",
        "    \n",
        "    # åˆ’åˆ†æ•°æ®é›†\n",
        "    train_size_cnn = int(len(X_cnn) * 0.8)\n",
        "    X_train_cnn, X_test_cnn = X_cnn[:train_size_cnn], X_cnn[train_size_cnn:]\n",
        "    y_train_cnn, y_test_cnn = y_cnn[:train_size_cnn], y_cnn[train_size_cnn:]\n",
        "    \n",
        "    # æ„å»ºCNNæ¨¡å‹\n",
        "    cnn_model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(seq_length, len(feature_cols))),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(50, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(len(feature_cols))  # é¢„æµ‹æ‰€æœ‰ç‰¹å¾\n",
        "    ])\n",
        "    \n",
        "    cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    \n",
        "    print(\"CNNæ¨¡å‹ç»“æ„ï¼š\")\n",
        "    cnn_model.summary()\n",
        "    \n",
        "    print(\"\\næ³¨æ„ï¼šCNNæ¨¡å‹è®­ç»ƒéœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¿™é‡Œä»…å±•ç¤ºæ¨¡å‹ç»“æ„\")\n",
        "    print(\"å®é™…åº”ç”¨ä¸­å¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´ç½‘ç»œç»“æ„å’Œè®­ç»ƒå‚æ•°\")\n",
        "    \n",
        "else:\n",
        "    print(\"TensorFlowä¸å¯ç”¨ï¼Œæ— æ³•å±•ç¤ºCNNæ¨¡å‹\")\n",
        "    print(\"å®‰è£…TensorFlowåå¯ä»¥è¿è¡ŒCNNæ¨¡å‹ï¼špip install tensorflow\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“\n",
        "\n",
        "1. **æ·±åº¦å­¦ä¹ æ¨¡å‹**ï¼šLSTMã€CNNã€Transformerç­‰\n",
        "2. **åº”ç”¨åœºæ™¯**ï¼šæ—¶é—´åºåˆ—é¢„æµ‹ã€æ¨¡å¼è¯†åˆ«ã€ç‰¹å¾æå–\n",
        "3. **æ•°æ®å‡†å¤‡**ï¼šåºåˆ—æ•°æ®ã€ç‰¹å¾å·¥ç¨‹ã€æ•°æ®æ ‡å‡†åŒ–\n",
        "4. **æ¨¡å‹è®­ç»ƒ**ï¼šè¶…å‚æ•°è°ƒä¼˜ã€è¿‡æ‹Ÿåˆæ§åˆ¶ã€æ¨¡å‹è¯„ä¼°\n",
        "\n",
        "## ğŸ› ï¸ å·¥å…·ä¸è½¯ä»¶\n",
        "\n",
        "### æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
        "\n",
        "- **TensorFlow/Keras**ï¼šGoogleå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
        "- **PyTorch**ï¼šFacebookå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
        "- **å®‰è£…**ï¼š`pip install tensorflow` æˆ– `pip install torch`\n",
        "\n",
        "### æ³¨æ„äº‹é¡¹\n",
        "\n",
        "- âœ… **æ•°æ®è´¨é‡**ï¼šç¡®ä¿æ•°æ®å‡†ç¡®æ€§å’Œå®Œæ•´æ€§\n",
        "- âœ… **åºåˆ—é•¿åº¦**ï¼šé€‰æ‹©åˆé€‚çš„åºåˆ—é•¿åº¦\n",
        "- âœ… **æ¨¡å‹å¤æ‚åº¦**ï¼šå¹³è¡¡æ¨¡å‹å¤æ‚åº¦å’Œè¿‡æ‹Ÿåˆé£é™©\n",
        "- âš ï¸ **è®¡ç®—èµ„æº**ï¼šæ·±åº¦å­¦ä¹ éœ€è¦è¾ƒå¤šè®¡ç®—èµ„æº\n",
        "- âš ï¸ **è¿‡æ‹Ÿåˆé£é™©**ï¼šä½¿ç”¨æ­£åˆ™åŒ–ã€Dropoutç­‰æ–¹æ³•é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
        "\n",
        "## ğŸ”— ç›¸å…³çŸ¥è¯†ç‚¹\n",
        "\n",
        "- [ç›‘ç£å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨](./ç›‘ç£å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨.ipynb)\n",
        "- [å¼ºåŒ–å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨](./å¼ºåŒ–å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨.md)\n",
        "- [ç­–ç•¥è®¾è®¡åŸç†](../02_ç­–ç•¥å¼€å‘/ç­–ç•¥è®¾è®¡åŸç†.ipynb)\n",
        "\n",
        "## ğŸ“š æ‹“å±•é˜…è¯»\n",
        "\n",
        "- ã€Šæ·±åº¦å­¦ä¹ ã€‹- æ·±åº¦å­¦ä¹ ç»å…¸æ•™æ\n",
        "- ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹- ä½¿ç”¨Pythonè¿›è¡Œæ·±åº¦å­¦ä¹ \n",
        "- é‡åŒ–äº¤æ˜“ç›¸å…³è®ºæ–‡ - æœ€æ–°ç ”ç©¶æ–¹æ³•\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
