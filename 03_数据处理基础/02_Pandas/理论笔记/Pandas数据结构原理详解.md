# Pandas数据结构原理详解

## 1. Pandas的核心数据结构

### 1.1 Series：一维带标签数组

**Series的本质**：
- 类似于NumPy数组，但带有索引标签
- 可以看作是一个有序的字典
- 底层使用NumPy数组存储数据

**为什么需要Series？**⚠️【知其所以然】

1. **标签索引**：可以使用有意义的标签而不是数字索引
2. **数据对齐**：不同Series之间的运算会自动对齐标签
3. **缺失值处理**：原生支持NaN值
4. **类型推断**：自动推断数据类型

**内存结构**：
```
Series = {
    index: [标签1, 标签2, ...],
    values: [值1, 值2, ...],  # NumPy数组
    dtype: 数据类型
}
```

### 1.2 DataFrame：二维带标签数据结构

**DataFrame的本质**：
- 可以看作多个Series的集合（共享索引）
- 类似于Excel表格或SQL表
- 每列可以是不同的数据类型

**为什么需要DataFrame？**⚠️【知其所以然】

1. **异构数据**：不同列可以是不同类型
2. **关系型操作**：支持类似SQL的操作（join、groupby等）
3. **数据对齐**：自动处理索引对齐
4. **高效操作**：向量化操作，性能接近NumPy

**内存结构**：
```
DataFrame = {
    index: [行标签1, 行标签2, ...],
    columns: [列名1, 列名2, ...],
    data: {
        列名1: Series,
        列名2: Series,
        ...
    }
}
```

---

## 2. 索引机制

### 2.1 索引类型

**默认索引**：
- 整数索引：0, 1, 2, ...
- 自动生成

**自定义索引**：
- 字符串索引：'A', 'B', 'C', ...
- 时间索引：DatetimeIndex
- 层次索引：MultiIndex

**为什么需要自定义索引？**⚠️【知其所以然】

1. **语义化**：使用有意义的标签
2. **数据对齐**：不同数据源可以自动对齐
3. **时间序列**：时间索引支持时间序列操作
4. **多级索引**：处理多维数据

### 2.2 索引操作

**位置索引（iloc）**：
```python
df.iloc[0, 1]  # 第0行第1列
df.iloc[0:5, :]  # 前5行所有列
```

**标签索引（loc）**：
```python
df.loc['row1', 'col1']  # 标签'row1'的行，标签'col1'的列
df.loc['row1':'row3', 'col1']  # 切片
```

**为什么区分iloc和loc？**⚠️【知其所以然】

1. **iloc**：基于位置，类似NumPy数组索引，速度快
2. **loc**：基于标签，支持标签对齐，更灵活
3. **避免混淆**：明确区分位置和标签索引

---

## 3. 数据对齐

### 3.1 自动对齐

**Series对齐**：
```python
s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
s2 = pd.Series([4, 5, 6], index=['b', 'c', 'd'])
s1 + s2  # 自动对齐，缺失值变为NaN
```

**为什么需要自动对齐？**⚠️【知其所以然】

1. **数据整合**：不同数据源可能有不同的索引
2. **避免错误**：防止位置错位导致的错误
3. **灵活性**：不需要手动对齐索引
4. **缺失值处理**：自动处理缺失的标签

### 3.2 对齐规则

1. **相同标签**：对应位置运算
2. **不同标签**：结果包含所有标签，缺失值为NaN
3. **缺失值处理**：可以使用`fill_value`填充

---

## 4. 缺失值处理

### 4.1 缺失值表示

**NaN（Not a Number）**：
- 浮点数类型：使用`np.nan`表示
- 整数类型：转换为`float64`以支持NaN

**为什么整数不能有NaN？**⚠️【知其所以然】

- NumPy的整数类型不支持NaN
- 为了支持缺失值，Pandas将整数列转换为浮点数
- 可以使用`Int64`（可空整数）类型避免这个问题

### 4.2 缺失值检测

```python
df.isna()  # 检测缺失值
df.isnull()  # 等价于isna()
df.notna()  # 检测非缺失值
```

### 4.3 缺失值处理

**删除**：
```python
df.dropna()  # 删除包含NaN的行
df.dropna(axis=1)  # 删除包含NaN的列
```

**填充**：
```python
df.fillna(0)  # 用0填充
df.fillna(method='ffill')  # 前向填充
df.fillna(method='bfill')  # 后向填充
```

**插值**：
```python
df.interpolate()  # 线性插值
```

---

## 5. 分组操作（GroupBy）

### 5.1 GroupBy原理

**分组过程**：
1. **Split**：根据键值分组
2. **Apply**：对每组应用函数
3. **Combine**：合并结果

**为什么GroupBy高效？**⚠️【知其所以然】

1. **向量化操作**：在C语言层面执行
2. **索引优化**：使用哈希表快速分组
3. **延迟计算**：只在需要时计算
4. **内存效率**：不创建中间副本

### 5.2 分组操作示例

```python
df.groupby('key').sum()  # 按key分组求和
df.groupby('key').mean()  # 按key分组求平均
df.groupby('key').agg(['sum', 'mean', 'std'])  # 多个聚合函数
```

---

## 6. 合并操作（Merge/Join）

### 6.1 合并类型

**内连接（inner）**：只保留两个表都有的键
**外连接（outer）**：保留所有键，缺失值用NaN填充
**左连接（left）**：保留左表的所有键
**右连接（right）**：保留右表的所有键

**为什么需要不同的连接类型？**⚠️【知其所以然】

- **数据完整性**：不同场景需要保留不同的数据
- **数据分析**：需要灵活的数据整合方式
- **SQL兼容**：与SQL的JOIN操作一致

### 6.2 合并实现

```python
pd.merge(df1, df2, on='key')  # 基于键合并
df1.join(df2, on='key')  # 基于索引合并
pd.concat([df1, df2])  # 拼接
```

---

## 7. 性能优化

### 7.1 向量化操作

**避免循环**：
```python
# 慢
for i in range(len(df)):
    df.loc[i, 'new_col'] = df.loc[i, 'col1'] * 2

# 快
df['new_col'] = df['col1'] * 2  # 向量化
```

**为什么向量化快？**⚠️【知其所以然】

1. **NumPy底层**：使用NumPy的向量化操作
2. **C语言实现**：核心操作在C语言层面执行
3. **缓存友好**：连续内存访问
4. **SIMD指令**：利用CPU的向量化指令

### 7.2 数据类型优化

**选择合适的数据类型**：
```python
# 如果值在0-255之间，使用uint8而不是int64
df['col'] = df['col'].astype('uint8')  # 节省内存
```

### 7.3 使用分类类型

**分类类型（Categorical）**：
```python
df['category'] = df['category'].astype('category')
```

**优势**：
- 节省内存（特别是重复值多的列）
- 提高性能（某些操作更快）
- 保持顺序

---

## 8. 常见误区

### 误区1：Pandas总是比NumPy慢
**纠正**：对于结构化数据操作，Pandas通常更快，因为它针对这类操作优化

### 误区2：使用循环处理DataFrame
**纠正**：应该使用向量化操作，避免Python循环

### 误区3：不区分iloc和loc
**纠正**：iloc基于位置，loc基于标签，用途不同

### 误区4：忽略数据类型
**纠正**：选择合适的数据类型可以显著提高性能和节省内存

---

## 9. 总结

Pandas的核心优势：
1. **标签索引**：有意义的索引标签
2. **数据对齐**：自动处理索引对齐
3. **缺失值处理**：原生支持NaN
4. **关系型操作**：类似SQL的操作
5. **高性能**：基于NumPy，向量化操作

**继续学习**：
- 掌握高级索引技巧
- 学习时间序列处理
- 掌握数据清洗技巧

