# 数据清洗原理详解

## 1. 数据质量问题的类型

### 1.1 缺失值（Missing Values）

**缺失值的类型**：
1. **完全随机缺失（MCAR）**：缺失与任何变量无关
2. **随机缺失（MAR）**：缺失与观测到的变量有关
3. **非随机缺失（MNAR）**：缺失与未观测到的变量有关

**为什么需要处理缺失值？**⚠️【知其所以然】

1. **算法要求**：大多数机器学习算法不能直接处理缺失值
2. **统计偏差**：缺失值可能导致统计偏差
3. **信息损失**：缺失值代表信息损失
4. **模型性能**：缺失值处理不当会影响模型性能

### 1.2 异常值（Outliers）

**异常值的类型**：
1. **点异常**：单个数据点异常
2. **上下文异常**：在特定上下文中异常
3. **集体异常**：一组数据点整体异常

**为什么需要处理异常值？**⚠️【知其所以然】

1. **统计影响**：异常值会影响均值和标准差
2. **模型影响**：某些算法对异常值敏感
3. **数据质量**：异常值可能表示数据错误
4. **但要注意**：异常值也可能是真实的重要信息

### 1.3 重复值（Duplicates）

**重复值的类型**：
1. **完全重复**：所有列都相同
2. **部分重复**：某些列相同

**为什么需要处理重复值？**⚠️【知其所以然】

1. **数据冗余**：重复值浪费存储空间
2. **统计偏差**：重复值会导致统计偏差
3. **模型影响**：重复值可能影响模型训练

---

## 2. 缺失值处理

### 2.1 删除策略

**删除行**：
- 如果缺失值比例很小（<5%）
- 如果缺失是随机的
- 如果样本量足够大

**删除列**：
- 如果缺失值比例很大（>50%）
- 如果该列不重要

**为什么有时删除是合理的？**⚠️【知其所以然】

1. **简单有效**：删除是最简单的方法
2. **避免偏差**：如果缺失是随机的，删除不会引入偏差
3. **计算效率**：删除后数据更小，计算更快

### 2.2 填充策略

**统计量填充**：
- **均值填充**：适用于数值变量，假设数据对称分布
- **中位数填充**：适用于数值变量，对异常值更稳健
- **众数填充**：适用于分类变量

**为什么选择不同的填充方法？**⚠️【知其所以然】

1. **均值**：假设数据正态分布，但受异常值影响
2. **中位数**：对异常值稳健，适合偏态分布
3. **众数**：适合分类变量，保持类别分布

**前向/后向填充**：
- **前向填充（ffill）**：用前一个值填充
- **后向填充（bfill）**：用后一个值填充
- 适用于时间序列数据

**为什么时间序列用前向/后向填充？**⚠️【知其所以然】

- 时间序列数据有顺序性
- 相邻时间点的值通常相似
- 保持时间序列的连续性

**插值填充**：
- **线性插值**：假设值之间线性变化
- **样条插值**：更平滑的插值
- **多项式插值**：高阶插值

### 2.3 模型预测填充

**使用模型预测缺失值**：
- 使用其他特征预测缺失值
- 更准确，但计算成本高

**为什么模型预测更准确？**⚠️【知其所以然】

- 考虑了特征之间的关系
- 利用数据中的模式
- 比简单统计量填充更智能

---

## 3. 异常值处理

### 3.1 异常值检测方法

**Z-score方法**：
$$Z = \frac{x - \mu}{\sigma}$$

如果|Z| > 3，通常认为是异常值。

**为什么Z-score有效？**⚠️【知其所以然】

- 基于正态分布假设
- 3σ原则：99.7%的数据在3σ范围内
- 简单易用

**IQR方法**：
$$IQR = Q_3 - Q_1$$
异常值：< Q₁ - 1.5×IQR 或 > Q₃ + 1.5×IQR

**为什么IQR方法更稳健？**⚠️【知其所以然】

- 不依赖均值和标准差
- 对异常值本身不敏感
- 基于分位数，更稳健

**Isolation Forest**：
- 使用机器学习方法检测异常值
- 适合高维数据

### 3.2 异常值处理策略

**删除**：
- 如果确定是错误数据
- 如果异常值很少

**替换**：
- **截断（Clipping）**：用边界值替换
- **Winsorization**：用分位数替换

**保留**：
- 如果异常值是真实的
- 如果异常值包含重要信息

---

## 4. 数据标准化和归一化

### 4.1 为什么需要标准化？

**不同量纲**：
- 特征的单位和范围不同
- 某些算法对尺度敏感

**为什么某些算法需要标准化？**⚠️【知其所以然】

1. **距离计算**：基于距离的算法（如KNN、K-means）受尺度影响
2. **梯度下降**：不同尺度的特征导致梯度下降困难
3. **正则化**：L1/L2正则化对尺度敏感

### 4.2 标准化方法

**Z-score标准化**：
$$z = \frac{x - \mu}{\sigma}$$

**特点**：
- 均值为0，标准差为1
- 保持原始分布形状

**Min-Max归一化**：
$$x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}$$

**特点**：
- 值域[0, 1]
- 改变分布形状

**为什么选择不同的标准化方法？**⚠️【知其所以然】

- **Z-score**：保持分布形状，适合大多数情况
- **Min-Max**：值域固定，适合需要[0,1]范围的情况
- **Robust标准化**：使用中位数和IQR，对异常值稳健

---

## 5. 特征工程

### 5.1 特征创建

**特征组合**：
- 创建新特征（如年龄×收入）
- 捕获特征之间的交互

**为什么特征组合有效？**⚠️【知其所以然】

- 某些关系是交互的
- 线性模型无法自动捕获交互
- 特征组合可以显式建模交互

**特征变换**：
- **对数变换**：处理偏态分布
- **平方根变换**：处理中等偏态
- **Box-Cox变换**：通用变换

**为什么需要特征变换？**⚠️【知其所以然】

1. **正态化**：某些算法假设正态分布
2. **稳定方差**：变换可以稳定方差
3. **线性化关系**：变换可以线性化非线性关系

### 5.2 特征选择

**过滤法**：
- 基于统计特征选择
- 快速，但不考虑模型

**包装法**：
- 基于模型性能选择
- 准确，但计算成本高

**嵌入法**：
- 模型训练过程中选择
- 平衡准确性和效率

---

## 6. 数据清洗流程

### 6.1 标准流程

1. **数据探索**：了解数据质量
2. **缺失值处理**：选择合适策略
3. **异常值处理**：检测和处理
4. **数据转换**：标准化、归一化
5. **特征工程**：创建新特征
6. **数据验证**：检查处理结果

### 6.2 最佳实践

1. **保留原始数据**：始终保留原始数据副本
2. **记录处理过程**：记录所有处理步骤
3. **验证结果**：检查处理后的数据质量
4. **迭代优化**：根据模型性能调整处理策略

---

## 7. 总结

数据清洗的核心：
1. **理解数据**：了解数据质量问题
2. **选择策略**：根据数据特点选择方法
3. **验证结果**：检查处理效果
4. **迭代优化**：持续改进

**继续学习**：
- 掌握更多清洗技巧
- 学习自动化清洗流程
- 掌握特征工程方法

