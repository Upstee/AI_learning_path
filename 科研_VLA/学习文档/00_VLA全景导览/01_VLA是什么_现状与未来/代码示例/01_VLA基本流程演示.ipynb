{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VLA基本流程演示\n",
        "\n",
        "## 目标\n",
        "通过一个简化的VLA模型，理解VLA的基本工作流程。\n",
        "\n",
        "**注意**：这是教学示例，实际VLA模型会更复杂。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 导入必要的库\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch版本: 2.6.0+cu124\n",
            "CUDA可用: True\n",
            "GPU名称: NVIDIA GeForce RTX 4060 Laptop GPU\n",
            "GPU显存: 8.59 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU名称: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU显存: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 定义简化的VLA模型\n",
        "\n",
        "### 2.1 Vision编码器（简化版）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VisionEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    简化的视觉编码器\n",
        "    实际VLA会使用ResNet、ViT等更复杂的模型\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dim=128):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # 全局平均池化\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.fc = nn.Linear(128, feature_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        参数:\n",
        "            x: 图像 [batch_size, 3, H, W]\n",
        "        返回:\n",
        "            features: 视觉特征 [batch_size, feature_dim]\n",
        "        \"\"\"\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Language编码器（简化版）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LanguageEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    简化的语言编码器\n",
        "    实际VLA会使用BERT、GPT等预训练模型\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=1000, embed_dim=128, hidden_dim=128, feature_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, feature_dim)  # 双向LSTM，所以是2倍\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        参数:\n",
        "            x: 文本序列 [batch_size, seq_len]\n",
        "        返回:\n",
        "            features: 语言特征 [batch_size, feature_dim]\n",
        "        \"\"\"\n",
        "        # 词嵌入\n",
        "        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
        "        \n",
        "        # LSTM编码\n",
        "        lstm_out, (h_n, c_n) = self.lstm(embedded)  # [batch_size, seq_len, hidden_dim*2]\n",
        "        \n",
        "        # 取最后一个时间步的输出\n",
        "        last_hidden = lstm_out[:, -1, :]  # [batch_size, hidden_dim*2]\n",
        "        \n",
        "        # 全连接层\n",
        "        features = self.fc(last_hidden)  # [batch_size, feature_dim]\n",
        "        \n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 融合层\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FusionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    多模态融合层\n",
        "    将视觉和语言特征融合\n",
        "    \"\"\"\n",
        "    def __init__(self, vision_dim=128, language_dim=128, fused_dim=256):\n",
        "        super().__init__()\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(vision_dim + language_dim, fused_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(fused_dim, fused_dim // 2)\n",
        "        )\n",
        "    \n",
        "    def forward(self, vision_features, language_features):\n",
        "        \"\"\"\n",
        "        参数:\n",
        "            vision_features: 视觉特征 [batch_size, vision_dim]\n",
        "            language_features: 语言特征 [batch_size, language_dim]\n",
        "        返回:\n",
        "            fused_features: 融合特征 [batch_size, fused_dim // 2]\n",
        "        \"\"\"\n",
        "        # 拼接特征\n",
        "        combined = torch.cat([vision_features, language_features], dim=1)\n",
        "        \n",
        "        # 融合\n",
        "        fused = self.fusion(combined)\n",
        "        \n",
        "        return fused\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Action解码器（简化版）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActionDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    简化的动作解码器\n",
        "    实际VLA会使用更复杂的序列生成模型\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=128, action_dim=7):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, fused_features):\n",
        "        \"\"\"\n",
        "        参数:\n",
        "            fused_features: 融合特征 [batch_size, input_dim]\n",
        "        返回:\n",
        "            action: 动作 [batch_size, action_dim]\n",
        "        \"\"\"\n",
        "        action = self.decoder(fused_features)\n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 完整的VLA模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleVLA(nn.Module):\n",
        "    \"\"\"\n",
        "    简化的VLA模型\n",
        "    用于演示VLA的基本工作流程\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=1000, action_dim=7):\n",
        "        super().__init__()\n",
        "        self.vision_encoder = VisionEncoder(feature_dim=128)\n",
        "        self.language_encoder = LanguageEncoder(vocab_size=vocab_size, feature_dim=128)\n",
        "        self.fusion = FusionLayer(vision_dim=128, language_dim=128, fused_dim=256)\n",
        "        self.action_decoder = ActionDecoder(input_dim=128, action_dim=action_dim)\n",
        "    \n",
        "    def forward(self, image, text):\n",
        "        \"\"\"\n",
        "        完整的VLA前向传播\n",
        "        \n",
        "        参数:\n",
        "            image: 图像 [batch_size, 3, H, W]\n",
        "            text: 文本序列 [batch_size, seq_len]\n",
        "        \n",
        "        返回:\n",
        "            action: 动作 [batch_size, action_dim]\n",
        "        \"\"\"\n",
        "        # 1. 视觉编码\n",
        "        vision_features = self.vision_encoder(image)\n",
        "        \n",
        "        # 2. 语言编码\n",
        "        language_features = self.language_encoder(text)\n",
        "        \n",
        "        # 3. 融合\n",
        "        fused_features = self.fusion(vision_features, language_features)\n",
        "        \n",
        "        # 4. 动作生成\n",
        "        action = self.action_decoder(fused_features)\n",
        "        \n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 创建模型实例\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "使用设备: cuda\n",
            "\n",
            "模型结构:\n",
            "SimpleVLA(\n",
            "  (vision_encoder): VisionEncoder(\n",
            "    (conv_layers): Sequential(\n",
            "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): ReLU()\n",
            "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): ReLU()\n",
            "      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "      (9): Flatten(start_dim=1, end_dim=-1)\n",
            "    )\n",
            "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (language_encoder): LanguageEncoder(\n",
            "    (embedding): Embedding(1000, 128)\n",
            "    (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
            "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (fusion): FusionLayer(\n",
            "    (fusion): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.1, inplace=False)\n",
            "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (action_decoder): ActionDecoder(\n",
            "    (decoder): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.1, inplace=False)\n",
            "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (4): ReLU()\n",
            "      (5): Linear(in_features=64, out_features=7, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "总参数量: 658,759\n",
            "可训练参数量: 658,759\n"
          ]
        }
      ],
      "source": [
        "# 设置设备\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"使用设备: {device}\")\n",
        "\n",
        "# 创建模型\n",
        "model = SimpleVLA(vocab_size=1000, action_dim=7).to(device)\n",
        "\n",
        "# 打印模型结构\n",
        "print(\"\\n模型结构:\")\n",
        "print(model)\n",
        "\n",
        "# 计算参数量\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\n总参数量: {total_params:,}\")\n",
        "print(f\"可训练参数量: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 准备示例数据\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "图像形状: torch.Size([2, 3, 224, 224])\n",
            "文本形状: torch.Size([2, 10])\n",
            "\n",
            "图像数据范围: [-4.41, 4.50]\n",
            "文本数据范围: [9, 984]\n"
          ]
        }
      ],
      "source": [
        "# 创建示例输入\n",
        "batch_size = 2\n",
        "image_height, image_width = 224, 224\n",
        "seq_len = 10\n",
        "\n",
        "# 图像输入 (RGB图像)\n",
        "image = torch.randn(batch_size, 3, image_height, image_width).to(device)\n",
        "\n",
        "# 文本输入 (词索引)\n",
        "text = torch.randint(0, 1000, (batch_size, seq_len)).to(device)\n",
        "\n",
        "print(f\"图像形状: {image.shape}\")\n",
        "print(f\"文本形状: {text.shape}\")\n",
        "print(f\"\\n图像数据范围: [{image.min():.2f}, {image.max():.2f}]\")\n",
        "print(f\"文本数据范围: [{text.min()}, {text.max()}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 前向传播演示\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "输入图像形状: torch.Size([2, 3, 224, 224])\n",
            "输入文本形状: torch.Size([2, 10])\n",
            "输出动作形状: torch.Size([2, 7])\n",
            "\n",
            "输出动作值 (前2个样本):\n",
            "tensor([[-0.0886,  0.0920, -0.1167,  0.0010,  0.0162,  0.0272,  0.0374],\n",
            "        [-0.0884,  0.0922, -0.1163,  0.0034,  0.0161,  0.0294,  0.0386]],\n",
            "       device='cuda:0')\n",
            "\n",
            "动作值范围: [-0.12, 0.09]\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# 设置为评估模式\n",
        "model.eval()\n",
        "\n",
        "# 前向传播\n",
        "with torch.no_grad():\n",
        "    action = model(image, text)\n",
        "\n",
        "print(f\"输入图像形状: {image.shape}\")\n",
        "print(f\"输入文本形状: {text.shape}\")\n",
        "print(f\"输出动作形状: {action.shape}\")\n",
        "print(f\"\\n输出动作值 (前2个样本):\")\n",
        "print(action[:2])\n",
        "print(f\"\\n动作值范围: [{action.min():.2f}, {action.max():.2f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 可视化数据流\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 获取中间特征（用于可视化）\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # 分别获取各模块的输出\n",
        "    vision_features = model.vision_encoder(image)\n",
        "    language_features = model.language_encoder(text)\n",
        "    fused_features = model.fusion(vision_features, language_features)\n",
        "    action = model.action_decoder(fused_features)\n",
        "\n",
        "print(\"数据流可视化:\")\n",
        "print(f\"1. 视觉特征形状: {vision_features.shape}\")\n",
        "print(f\"2. 语言特征形状: {language_features.shape}\")\n",
        "print(f\"3. 融合特征形状: {fused_features.shape}\")\n",
        "print(f\"4. 动作形状: {action.shape}\")\n",
        "\n",
        "# 可视化特征分布\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# 视觉特征分布\n",
        "axes[0, 0].hist(vision_features[0].cpu().numpy(), bins=50, alpha=0.7)\n",
        "axes[0, 0].set_title('视觉特征分布')\n",
        "axes[0, 0].set_xlabel('特征值')\n",
        "axes[0, 0].set_ylabel('频数')\n",
        "\n",
        "# 语言特征分布\n",
        "axes[0, 1].hist(language_features[0].cpu().numpy(), bins=50, alpha=0.7, color='orange')\n",
        "axes[0, 1].set_title('语言特征分布')\n",
        "axes[0, 1].set_xlabel('特征值')\n",
        "axes[0, 1].set_ylabel('频数')\n",
        "\n",
        "# 融合特征分布\n",
        "axes[1, 0].hist(fused_features[0].cpu().numpy(), bins=50, alpha=0.7, color='green')\n",
        "axes[1, 0].set_title('融合特征分布')\n",
        "axes[1, 0].set_xlabel('特征值')\n",
        "axes[1, 0].set_ylabel('频数')\n",
        "\n",
        "# 动作值分布\n",
        "axes[1, 1].bar(range(action.shape[1]), action[0].cpu().numpy())\n",
        "axes[1, 1].set_title('动作值')\n",
        "axes[1, 1].set_xlabel('动作维度')\n",
        "axes[1, 1].set_ylabel('动作值')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 理解VLA的工作流程\n",
        "\n",
        "### 7.1 流程总结\n",
        "\n",
        "VLA的基本工作流程：\n",
        "\n",
        "1. **Vision编码器**：将图像编码为视觉特征\n",
        "   - 输入：图像 $I \\in \\mathbb{R}^{H \\times W \\times C}$\n",
        "   - 输出：视觉特征 $f_v \\in \\mathbb{R}^{d_v}$\n",
        "\n",
        "2. **Language编码器**：将文本编码为语言特征\n",
        "   - 输入：文本序列 $T = [t_1, t_2, ..., t_n]$\n",
        "   - 输出：语言特征 $f_l \\in \\mathbb{R}^{d_l}$\n",
        "\n",
        "3. **融合层**：将视觉和语言特征融合\n",
        "   - 输入：$f_v$ 和 $f_l$\n",
        "   - 输出：融合特征 $f_{fused} \\in \\mathbb{R}^{d}$\n",
        "\n",
        "4. **Action解码器**：根据融合特征生成动作\n",
        "   - 输入：$f_{fused}$\n",
        "   - 输出：动作 $A \\in \\mathbb{R}^{action\\_dim}$\n",
        "\n",
        "### 7.2 数学表示\n",
        "\n",
        "完整的VLA模型可以表示为：\n",
        "\n",
        "$$A = \\text{ActionDecoder}(\\text{Fusion}(\\text{VisionEncoder}(I), \\text{LanguageEncoder}(T)))$$\n",
        "\n",
        "### 7.3 关键理解\n",
        "\n",
        "- **多模态输入**：VLA同时处理视觉和语言两种模态\n",
        "- **特征提取**：每个模态都有自己的编码器\n",
        "- **信息融合**：将视觉和语言信息融合\n",
        "- **动作生成**：根据融合信息生成动作序列\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 总结\n",
        "\n",
        "### 8.1 关键要点\n",
        "\n",
        "1. **VLA是端到端的多模态学习系统**\n",
        "2. **三个核心模块：Vision、Language、Action**\n",
        "3. **通过融合多模态信息生成动作**\n",
        "4. **本示例是简化版本，实际VLA模型会更复杂**\n",
        "\n",
        "### 8.2 下一步学习\n",
        "\n",
        "- 深入学习视觉编码器的实现（ResNet、ViT等）\n",
        "- 深入学习语言编码器的实现（BERT、GPT等）\n",
        "- 深入学习动作解码器的实现（Transformer解码器等）\n",
        "- 学习多模态融合的具体方法\n",
        "- 学习如何训练VLA模型\n",
        "\n",
        "### 8.3 参考资源\n",
        "\n",
        "- 理论笔记：`理论笔记/VLA基本概念详解.md`\n",
        "- 论文：`VLA/科研论文/` 文件夹中的相关论文\n",
        "- 开源项目：openVLA、RT-1等\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
