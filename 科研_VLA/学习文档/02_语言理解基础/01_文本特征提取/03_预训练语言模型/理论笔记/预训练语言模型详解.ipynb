{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 预训练语言模型详解\n",
        "\n",
        "## 📋 文档说明\n",
        "\n",
        "本文档是预训练语言模型的详细理论讲解，比父目录的《文本特征提取详解》更加深入和详细。本文档将深入讲解预训练语言模型的原理、数学推导和实现细节。通过本文档，你将能够：\n",
        "\n",
        "1. **深入理解预训练语言模型的原理**：从无监督学习到迁移学习的完整流程\n",
        "2. **掌握预训练任务的数学原理**：理解语言建模、掩码语言模型等预训练任务的数学定义\n",
        "3. **理解预训练的优势**：理解为什么预训练能够提高模型性能\n",
        "4. **掌握预训练语言模型在VLA中的应用**：理解预训练语言模型在VLA模型中的具体应用和优势\n",
        "\n",
        "**学习方式**：本文件是Jupyter Notebook格式，你可以边看边运行代码，通过可视化图表和数学推导更好地理解预训练语言模型的原理和过程。\n",
        "\n",
        "**文档结构**：\n",
        "- 父目录：文本特征提取详解（见../文本特征提取详解.ipynb）\n",
        "- 本文档：预训练语言模型详解（本文档）\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 术语表（按出现顺序）\n",
        "\n",
        "### 1. 预训练语言模型 (Pre-trained Language Model)\n",
        "- **中文名称**：预训练语言模型\n",
        "- **英文全称**：Pre-trained Language Model\n",
        "- **定义**：预训练语言模型是指在大规模文本数据上预先训练的语言模型，能够学习到丰富的语言表示，然后在特定任务上进行微调。预训练语言模型的核心思想是\"预训练+微调\"（Pre-training + Fine-tuning），即先在大规模无标注文本数据上预训练模型，学习通用的语言表示，然后在特定任务的有标注数据上进行微调，适应特定任务。预训练语言模型的优势在于：1）通用语言表示：通过大规模文本数据学习到通用的语言表示，能够捕获语言的语义、语法、上下文等信息；2）迁移学习：将预训练的知识迁移到下游任务，提高模型性能；3）数据效率：只需要少量标注数据就能在特定任务上取得好效果；4）可扩展性：通过增加模型大小和数据量，能够持续提升性能。预训练语言模型是现代自然语言处理和VLA的核心技术，BERT、GPT、T5等模型都是预训练语言模型。预训练语言模型通过大规模无监督学习学习语言的内在规律，这使得它们能够理解语言的语义、语法、上下文等信息，为下游任务提供强大的特征表示。\n",
        "- **核心组成**：预训练语言模型的核心组成包括：1）预训练数据：使用大规模无标注文本数据（如维基百科、新闻语料、书籍等）进行预训练；2）预训练任务：使用无监督学习任务（如语言建模、掩码语言模型、下一句预测等）进行预训练；3）模型架构：使用Transformer编码器或解码器作为模型架构；4）预训练目标：最大化预训练任务的似然函数，学习通用的语言表示；5）微调：在特定任务的有标注数据上进行微调，适应特定任务；6）迁移学习：将预训练的知识迁移到下游任务。预训练语言模型通常使用Transformer架构，通过多层堆叠的Transformer编码器或解码器学习语言的层次化表示，从低层的词语特征到高层的语义特征。\n",
        "- **在VLA中的应用**：在VLA中，预训练语言模型是语言编码器的核心。VLA模型使用预训练语言模型（如BERT、GPT）从输入文本中提取语言特征，这些特征将被用于理解语言指令的意图、对象、动作等。预训练语言模型的优势在于能够理解语言的语义、语法、上下文等信息，这对于理解复杂的语言指令非常重要。在VLA训练过程中，预训练语言模型通常使用预训练的权重初始化，然后在VLA任务上进行微调，以提高特征提取的质量和效率。预训练语言模型的输出特征将与视觉编码器的输出特征进行融合，生成多模态表示，最终用于动作生成。预训练语言模型的通用语言表示使得VLA模型能够更好地理解语言指令，从而生成更准确的动作序列。\n",
        "- **相关概念**：语言建模、掩码语言模型、下一句预测、BERT、GPT、Transformer、迁移学习、微调\n",
        "- **首次出现位置**：本文档标题\n",
        "- **深入学习**：参考父目录的[文本特征提取详解](../文本特征提取详解.ipynb)和[BERT详解](../../02_语言编码器/01_BERT/理论笔记/BERT详解.ipynb)\n",
        "- **直观理解**：想象预训练语言模型就像一位\"语言专家\"，他通过阅读大量文本（预训练）学会了语言的规律，然后能够理解各种语言任务（微调）。例如，预训练语言模型通过阅读大量文本学会了\"拿起\"是动作，\"杯子\"是物体，\"桌子上\"是位置等语言知识，然后在VLA任务中，它能够理解\"拿起桌子上的杯子\"这个指令的含义。在VLA中，预训练语言模型就是这样的\"语言专家\"，它帮助模型理解语言指令，从而生成相应的动作。预训练语言模型就像语言的\"知识库\"，能够为VLA模型提供丰富的语言理解能力。\n",
        "\n",
        "### 2. 语言建模 (Language Modeling)\n",
        "- **中文名称**：语言建模\n",
        "- **英文全称**：Language Modeling\n",
        "- **定义**：语言建模是指预测序列中下一个词语的概率分布的任务，是预训练语言模型的基础任务。语言建模的目标是学习语言的概率分布，即给定前面的词语，预测下一个词语的概率。语言建模的数学表示为：$P(w_t | w_{<t})$，其中$w_t$是当前位置的词语，$w_{<t}$是前面的词语序列。语言建模的优势在于：1）无监督学习：不需要标注数据，可以从大规模文本数据中学习；2）通用性：能够学习语言的通用规律，适用于各种下游任务；3）自回归：使用自回归方式生成文本，能够生成连贯的文本；4）可扩展性：通过增加模型大小和数据量，能够持续提升性能。语言建模是GPT等生成式预训练语言模型的基础任务，通过语言建模，模型能够学习语言的概率分布，理解语言的语法、语义、上下文等信息。\n",
        "- **核心组成**：语言建模的核心组成包括：1）序列建模：使用序列模型（如RNN、LSTM、Transformer）建模序列的概率分布；2）条件概率：计算给定前面词语的条件下，下一个词语的概率；3）最大似然估计：最大化训练数据的似然函数，学习语言的概率分布；4）自回归生成：使用自回归方式生成文本，即根据前面的词语预测下一个词语；5）损失函数：使用交叉熵损失函数，最大化下一个词语的条件概率；6）评估指标：使用困惑度（Perplexity）等指标评估模型性能。语言建模通常使用Transformer解码器作为模型架构，通过掩码自注意力机制实现自回归生成，即只能看到前面的词语，不能看到后面的词语。\n",
        "- **在VLA中的应用**：在VLA中，语言建模用于预训练语言模型，学习通用的语言表示。VLA模型使用预训练的语言模型（如GPT）从输入文本中提取语言特征，这些特征将被用于理解语言指令的意图、对象、动作等。语言建模的通用语言表示使得VLA模型能够更好地理解语言指令，从而生成更准确的动作序列。在某些VLA应用中，语言建模还可以用于生成动作序列的描述，例如生成\"拿起杯子\"、\"放下盘子\"等动作描述。理解语言建模有助于理解GPT等生成式预训练语言模型的工作原理，如何学习语言的概率分布，如何生成连贯的文本。\n",
        "- **相关概念**：预训练语言模型、GPT、自回归、条件概率、最大似然估计、困惑度、Transformer解码器\n",
        "- **首次出现位置**：本文档第2.1节\n",
        "- **深入学习**：参考本文档的语言建模详细讲解部分和[GPT详解](../../02_语言编码器/02_GPT/理论笔记/GPT详解.ipynb)\n",
        "- **直观理解**：想象语言建模就像一位\"续写专家\"，他能够根据前面的文字预测后面的文字。例如，看到\"拿起\"，语言建模模型会预测下一个词语可能是\"杯子\"、\"盘子\"、\"书\"等。通过这种\"续写\"的方式，语言建模模型学会了语言的规律，理解了词语之间的关系。在VLA中，语言建模帮助模型理解语言指令的语义，从而生成相应的动作。语言建模就像语言的\"预测器\"，能够预测下一个词语，理解语言的概率分布。\n",
        "\n",
        "### 3. 掩码语言模型 (Masked Language Model, MLM)\n",
        "- **中文名称**：掩码语言模型\n",
        "- **英文全称**：Masked Language Model\n",
        "- **定义**：掩码语言模型是指预测被掩码（mask）的词语的任务，是BERT等双向预训练语言模型的基础任务。掩码语言模型的目标是学习双向语言表示，即能够同时利用词语的前后文信息。掩码语言模型的数学表示为：$P(w_t | w_{\\\\backslash t})$，其中$w_t$是被掩码的词语，$w_{\\\\backslash t}$是除了$w_t$之外的所有词语。掩码语言模型的优势在于：1）双向编码：能够同时利用词语的前后文信息，提取更丰富的语言表示；2）无监督学习：不需要标注数据，可以从大规模文本数据中学习；3）通用性：能够学习语言的通用规律，适用于各种下游任务；4）上下文理解：能够理解词语在上下文中的含义，例如\"银行\"在\"金融机构\"和\"河岸\"两种上下文中的不同含义。掩码语言模型是BERT等双向预训练语言模型的基础任务，通过掩码语言模型，模型能够学习双向语言表示，理解语言的语义、语法、上下文等信息。\n",
        "- **核心组成**：掩码语言模型的核心组成包括：1）掩码策略：随机选择一定比例的词语进行掩码（通常为15%），其中80%替换为[MASK]标记，10%替换为随机词语，10%保持不变；2）双向编码：使用Transformer编码器同时编码被掩码词语的前后文信息；3）预测任务：预测被掩码的词语，最大化被掩码词语的条件概率；4）损失函数：使用交叉熵损失函数，最大化被掩码词语的条件概率；5）评估指标：使用准确率等指标评估模型性能；6）微调：在特定任务上进行微调，适应特定任务。掩码语言模型通常使用Transformer编码器作为模型架构，通过自注意力机制实现双向编码，即能够同时看到被掩码词语的前后文信息。\n",
        "- **在VLA中的应用**：在VLA中，掩码语言模型用于预训练语言模型，学习双向语言表示。VLA模型使用预训练的语言模型（如BERT）从输入文本中提取语言特征，这些特征将被用于理解语言指令的意图、对象、动作等。掩码语言模型的双向编码使得VLA模型能够更好地理解语言指令的完整语义，例如理解\"拿起桌子上的杯子\"中\"拿起\"、\"桌子\"、\"杯子\"之间的关系。在VLA训练过程中，掩码语言模型的预训练权重通常用于初始化语言编码器，然后在VLA任务上进行微调。理解掩码语言模型有助于理解BERT等双向预训练语言模型的工作原理，如何学习双向语言表示，如何理解语言的上下文信息。\n",
        "- **相关概念**：预训练语言模型、BERT、双向编码、掩码策略、Transformer编码器、自注意力机制\n",
        "- **首次出现位置**：本文档第2.2节\n",
        "- **深入学习**：参考本文档的掩码语言模型详细讲解部分和[BERT详解](../../02_语言编码器/01_BERT/理论笔记/BERT详解.ipynb)\n",
        "- **直观理解**：想象掩码语言模型就像一位\"填空专家\"，他能够根据上下文预测被掩码的词语。例如，看到\"拿起[MASK]上的杯子\"，掩码语言模型会预测被掩码的词语可能是\"桌子\"、\"架子\"等。通过这种\"填空\"的方式，掩码语言模型学会了语言的规律，理解了词语在上下文中的含义。在VLA中，掩码语言模型帮助模型理解语言指令的完整语义，从而生成相应的动作。掩码语言模型就像语言的\"填空器\"，能够预测被掩码的词语，理解语言的上下文信息。\n",
        "\n",
        "### 4. 迁移学习 (Transfer Learning)\n",
        "- **中文名称**：迁移学习\n",
        "- **英文全称**：Transfer Learning\n",
        "- **定义**：迁移学习是指将在一个任务上学到的知识迁移到另一个相关任务上的学习方法，是预训练语言模型的核心思想。迁移学习的核心思想是\"预训练+微调\"，即先在大规模无标注数据上预训练模型，学习通用的特征表示，然后在特定任务的有标注数据上进行微调，适应特定任务。迁移学习的优势在于：1）数据效率：只需要少量标注数据就能在特定任务上取得好效果；2）性能提升：预训练的知识能够显著提高模型在下游任务上的性能；3）通用性：预训练的特征表示适用于各种相关任务；4）可扩展性：通过增加预训练数据量和模型大小，能够持续提升性能。迁移学习是预训练语言模型的核心思想，通过迁移学习，模型能够将在大规模文本数据上学到的语言知识迁移到各种下游任务，提高模型性能。\n",
        "- **核心组成**：迁移学习的核心组成包括：1）预训练阶段：在大规模无标注数据上预训练模型，学习通用的特征表示；2）特征提取：从预训练模型中提取特征，用于下游任务；3）微调阶段：在特定任务的有标注数据上进行微调，适应特定任务；4）知识迁移：将预训练的知识迁移到下游任务，提高模型性能；5）任务适应：通过微调使模型适应特定任务的特点；6）性能评估：评估模型在下游任务上的性能。迁移学习通常使用预训练的权重初始化模型，然后在特定任务上进行微调，即同时更新预训练权重和任务特定权重，使模型适应特定任务。\n",
        "- **在VLA中的应用**：在VLA中，迁移学习用于将预训练语言模型的知识迁移到VLA任务。VLA模型使用预训练的语言模型（如BERT、GPT）作为语言编码器，通过微调使模型适应VLA任务的特点。迁移学习的优势在于能够利用预训练语言模型的通用语言表示，提高VLA模型的语言理解能力。在VLA训练过程中，通常使用预训练语言模型的权重初始化语言编码器，然后在VLA任务上进行微调，即同时更新语言编码器、视觉编码器、多模态融合模块和动作生成模块的权重，使整个模型适应VLA任务。理解迁移学习有助于理解预训练语言模型如何提高VLA模型的性能，如何将通用的语言知识迁移到VLA任务。\n",
        "- **相关概念**：预训练语言模型、微调、特征提取、知识迁移、BERT、GPT、VLA\n",
        "- **首次出现位置**：本文档第1.2节\n",
        "- **深入学习**：参考本文档的迁移学习详细讲解部分\n",
        "- **直观理解**：想象迁移学习就像一位\"多才多艺的专家\"，他先通过大量学习（预训练）掌握了通用技能，然后能够快速适应新任务（微调）。例如，预训练语言模型通过阅读大量文本学会了语言的规律，然后在VLA任务中，它能够快速理解\"拿起桌子上的杯子\"这个指令的含义。在VLA中，迁移学习帮助模型利用预训练语言模型的知识，提高语言理解能力，从而生成更准确的动作。迁移学习就像知识的\"传递器\"，能够将预训练的知识传递到下游任务。\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 概述\n",
        "\n",
        "### 什么是预训练语言模型\n",
        "\n",
        "预训练语言模型是指在大规模文本数据上预先训练的语言模型，能够学习到丰富的语言表示，然后在特定任务上进行微调。预训练语言模型的核心思想是\"预训练+微调\"，即先在大规模无标注文本数据上预训练模型，学习通用的语言表示，然后在特定任务的有标注数据上进行微调，适应特定任务。\n",
        "\n",
        "### 为什么重要\n",
        "\n",
        "预训练语言模型对于VLA学习非常重要，原因包括：\n",
        "\n",
        "1. **通用语言表示**：通过大规模文本数据学习到通用的语言表示，能够捕获语言的语义、语法、上下文等信息\n",
        "2. **迁移学习**：将预训练的知识迁移到下游任务，提高模型性能\n",
        "3. **数据效率**：只需要少量标注数据就能在特定任务上取得好效果\n",
        "4. **预训练模型**：BERT、GPT等预训练语言模型是VLA语言编码器的核心\n",
        "\n",
        "### 学习目标\n",
        "\n",
        "通过本文档的学习，你将能够：\n",
        "\n",
        "1. **深入理解预训练语言模型**：理解预训练语言模型的原理和方法\n",
        "2. **掌握语言建模**：理解语言建模的数学定义和计算方法\n",
        "3. **理解掩码语言模型**：理解掩码语言模型的原理和优势\n",
        "4. **掌握预训练语言模型在VLA中的应用**：理解预训练语言模型在VLA模型中的具体应用\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 预训练语言模型的基本原理\n",
        "\n",
        "### 1.1 什么是预训练语言模型\n",
        "\n",
        "**直观理解**：想象预训练语言模型就像一位\"语言专家\"，他通过阅读大量文本（预训练）学会了语言的规律，然后能够理解各种语言任务（微调）。\n",
        "\n",
        "### 1.2 为什么需要预训练语言模型\n",
        "\n",
        "1. **通用语言表示**：通过大规模文本数据学习到通用的语言表示，能够捕获语言的语义、语法、上下文等信息\n",
        "2. **迁移学习**：将预训练的知识迁移到下游任务，提高模型性能\n",
        "3. **数据效率**：只需要少量标注数据就能在特定任务上取得好效果\n",
        "4. **可扩展性**：通过增加模型大小和数据量，能够持续提升性能\n",
        "\n",
        "### 1.3 预训练语言模型的训练流程\n",
        "\n",
        "预训练语言模型的训练流程包括：\n",
        "\n",
        "1. **预训练阶段**：在大规模无标注文本数据上预训练模型，学习通用的语言表示\n",
        "2. **微调阶段**：在特定任务的有标注数据上进行微调，适应特定任务\n",
        "\n",
        "### 1.4 预训练语言模型的数学表示详解\n",
        "\n",
        "#### 1.4.1 从基础数学开始\n",
        "\n",
        "**步骤1：理解预训练目标**\n",
        "\n",
        "预训练语言模型的目标是最大化训练数据的似然函数：\n",
        "\n",
        "$$\\max \\prod_{i=1}^{N} P(x_i | \\theta)$$\n",
        "\n",
        "其中：\n",
        "- $N$ 是训练样本数量\n",
        "- $x_i$ 是第$i$个训练样本\n",
        "- $\\theta$ 是模型参数\n",
        "\n",
        "**步骤2：理解语言建模**\n",
        "\n",
        "对于语言建模任务，目标函数为：\n",
        "\n",
        "$$\\max \\prod_{t=1}^{T} P(w_t | w_{<t}, \\theta)$$\n",
        "\n",
        "其中：\n",
        "- $w_t$ 是当前位置的词语\n",
        "- $w_{<t}$ 是前面的词语序列\n",
        "- $T$ 是序列长度\n",
        "\n",
        "**步骤3：理解掩码语言模型**\n",
        "\n",
        "对于掩码语言模型任务，目标函数为：\n",
        "\n",
        "$$\\max \\prod_{t \\in M} P(w_t | w_{\\backslash t}, \\theta)$$\n",
        "\n",
        "其中：\n",
        "- $M$ 是被掩码的词语集合\n",
        "- $w_t$ 是被掩码的词语\n",
        "- $w_{\\backslash t}$ 是除了$w_t$之外的所有词语\n",
        "\n",
        "### 1.5 预训练语言模型的可视化\n",
        "\n",
        "下面我们通过代码可视化预训练语言模型的概念：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 导入必要的库\n",
        "# ============================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Microsoft YaHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "print(\"环境准备完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 预训练语言模型训练流程可视化\n",
        "# ============================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# 模拟预训练和微调的过程\n",
        "epochs_pretrain = 10\n",
        "epochs_finetune = 5\n",
        "\n",
        "# 模拟预训练损失（逐渐下降）\n",
        "pretrain_losses = np.exp(-np.linspace(0, 2, epochs_pretrain)) + np.random.randn(epochs_pretrain) * 0.05\n",
        "pretrain_losses = np.maximum(pretrain_losses, 0.1)  # 确保损失为正\n",
        "\n",
        "# 模拟微调损失（从预训练损失开始，继续下降）\n",
        "finetune_losses = pretrain_losses[-1] * np.exp(-np.linspace(0, 1, epochs_finetune)) + np.random.randn(epochs_finetune) * 0.02\n",
        "finetune_losses = np.maximum(finetune_losses, 0.05)\n",
        "\n",
        "# 可视化\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 左图：预训练阶段\n",
        "axes[0].plot(range(1, epochs_pretrain + 1), pretrain_losses, 'o-', linewidth=2, markersize=8, color='steelblue', label='预训练损失')\n",
        "axes[0].set_title('预训练阶段：在大规模无标注数据上学习通用语言表示', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('损失值')\n",
        "axes[0].set_xticks(range(1, epochs_pretrain + 1))\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].legend()\n",
        "axes[0].set_ylim(0, max(pretrain_losses) * 1.2)\n",
        "\n",
        "# 右图：微调阶段\n",
        "axes[1].plot(range(epochs_pretrain, epochs_pretrain + epochs_finetune + 1), \n",
        "             [pretrain_losses[-1]] + list(finetune_losses), 'o-', linewidth=2, markersize=8, color='lightcoral', label='微调损失')\n",
        "axes[1].axvline(epochs_pretrain, color='gray', linestyle='--', linewidth=2, alpha=0.5, label='预训练结束')\n",
        "axes[1].set_title('微调阶段：在特定任务数据上适应任务', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('损失值')\n",
        "axes[1].set_xticks(range(epochs_pretrain, epochs_pretrain + epochs_finetune + 1))\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].legend()\n",
        "axes[1].set_ylim(0, max(finetune_losses) * 1.2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"预训练语言模型训练流程可视化说明：\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. 左图：预训练阶段，在大规模无标注数据上学习通用语言表示，损失逐渐下降\")\n",
        "print(\"2. 右图：微调阶段，在特定任务数据上适应任务，损失继续下降\")\n",
        "print(\"3. 预训练语言模型通过'预训练+微调'的方式，将通用语言知识迁移到特定任务\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 语言建模详解\n",
        "\n",
        "### 2.1 什么是语言建模\n",
        "\n",
        "**直观理解**：想象语言建模就像一位\"续写专家\"，他能够根据前面的文字预测后面的文字。\n",
        "\n",
        "### 2.2 为什么需要语言建模\n",
        "\n",
        "1. **无监督学习**：不需要标注数据，可以从大规模文本数据中学习\n",
        "2. **通用性**：能够学习语言的通用规律，适用于各种下游任务\n",
        "3. **自回归生成**：使用自回归方式生成文本，能够生成连贯的文本\n",
        "4. **可扩展性**：通过增加模型大小和数据量，能够持续提升性能\n",
        "\n",
        "### 2.3 语言建模的数学推导详解\n",
        "\n",
        "#### 2.3.1 从基础数学开始\n",
        "\n",
        "**步骤1：理解条件概率**\n",
        "\n",
        "条件概率$P(A|B)$表示在事件$B$发生的条件下，事件$A$发生的概率。\n",
        "\n",
        "**步骤2：理解语言建模的目标**\n",
        "\n",
        "语言建模的目标是最大化序列的似然函数：\n",
        "\n",
        "$$\\max \\prod_{t=1}^{T} P(w_t | w_{<t}, \\theta)$$\n",
        "\n",
        "其中：\n",
        "- $w_t$ 是当前位置的词语\n",
        "- $w_{<t} = w_1, w_2, \\ldots, w_{t-1}$ 是前面的词语序列\n",
        "- $T$ 是序列长度\n",
        "- $\\theta$ 是模型参数\n",
        "\n",
        "**步骤3：理解对数似然**\n",
        "\n",
        "为了便于计算，通常使用对数似然：\n",
        "\n",
        "$$\\max \\sum_{t=1}^{T} \\log P(w_t | w_{<t}, \\theta)$$\n",
        "\n",
        "**步骤4：理解损失函数**\n",
        "\n",
        "损失函数是负对数似然：\n",
        "\n",
        "$$\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(w_t | w_{<t}, \\theta)$$\n",
        "\n",
        "#### 2.3.2 语言建模的具体计算示例\n",
        "\n",
        "**示例：计算语言建模的概率**\n",
        "\n",
        "假设我们有一个句子：\"拿起 桌子 上 的 杯子\"。\n",
        "\n",
        "**步骤1：计算每个位置的条件概率**\n",
        "\n",
        "对于位置1（\"拿起\"）：\n",
        "- $P(拿起 | \\emptyset) = P(拿起)$（没有前面的词语）\n",
        "\n",
        "对于位置2（\"桌子\"）：\n",
        "- $P(桌子 | 拿起)$\n",
        "\n",
        "对于位置3（\"上\"）：\n",
        "- $P(上 | 拿起, 桌子)$\n",
        "\n",
        "对于位置4（\"的\"）：\n",
        "- $P(的 | 拿起, 桌子, 上)$\n",
        "\n",
        "对于位置5（\"杯子\"）：\n",
        "- $P(杯子 | 拿起, 桌子, 上, 的)$\n",
        "\n",
        "**步骤2：计算序列的概率**\n",
        "\n",
        "序列的概率为：\n",
        "\n",
        "$$P(拿起, 桌子, 上, 的, 杯子) = P(拿起) \\times P(桌子 | 拿起) \\times P(上 | 拿起, 桌子) \\times P(的 | 拿起, 桌子, 上) \\times P(杯子 | 拿起, 桌子, 上, 的)$$\n",
        "\n",
        "**步骤3：计算对数似然**\n",
        "\n",
        "对数似然为：\n",
        "\n",
        "$$\\log P(拿起, 桌子, 上, 的, 杯子) = \\log P(拿起) + \\log P(桌子 | 拿起) + \\log P(上 | 拿起, 桌子) + \\log P(的 | 拿起, 桌子, 上) + \\log P(杯子 | 拿起, 桌子, 上, 的)$$\n",
        "\n",
        "**步骤4：理解概率的含义**\n",
        "\n",
        "概率值越大，说明序列越符合语言的规律，模型预测越准确。\n",
        "\n",
        "### 2.4 语言建模的可视化\n",
        "\n",
        "下面我们通过代码可视化语言建模的过程：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 语言建模可视化（示例：预测下一个词语）\n",
        "# ============================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# 模拟一个句子：\"拿起 桌子 上 的 杯子\"\n",
        "sentence = ['拿起', '桌子', '上', '的', '杯子']\n",
        "vocab = ['拿起', '桌子', '上', '的', '杯子', '盘子', '书', '放下', '移动', '其他']\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 模拟语言建模模型预测下一个词语的概率\n",
        "# 对于每个位置，模型预测下一个词语的概率分布\n",
        "next_word_probs = []\n",
        "\n",
        "# 位置0：给定\"拿起\"，预测下一个词语\n",
        "probs_0 = np.array([0.05, 0.60, 0.10, 0.05, 0.15, 0.02, 0.01, 0.01, 0.01, 0.00])\n",
        "next_word_probs.append(probs_0)\n",
        "\n",
        "# 位置1：给定\"拿起 桌子\"，预测下一个词语\n",
        "probs_1 = np.array([0.02, 0.05, 0.70, 0.15, 0.05, 0.01, 0.01, 0.00, 0.01, 0.00])\n",
        "next_word_probs.append(probs_1)\n",
        "\n",
        "# 位置2：给定\"拿起 桌子 上\"，预测下一个词语\n",
        "probs_2 = np.array([0.01, 0.02, 0.05, 0.80, 0.10, 0.01, 0.00, 0.00, 0.01, 0.00])\n",
        "next_word_probs.append(probs_2)\n",
        "\n",
        "# 位置3：给定\"拿起 桌子 上 的\"，预测下一个词语\n",
        "probs_3 = np.array([0.01, 0.01, 0.02, 0.05, 0.85, 0.03, 0.02, 0.00, 0.01, 0.00])\n",
        "next_word_probs.append(probs_3)\n",
        "\n",
        "# 可视化\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 左上：位置0的预测概率\n",
        "pred_word_0 = vocab[np.argmax(probs_0)]\n",
        "colors_0 = ['red' if i == np.argmax(probs_0) else 'steelblue' for i in range(vocab_size)]\n",
        "axes[0, 0].bar(range(vocab_size), probs_0, color=colors_0, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_title(f'给定\"拿起\"，预测下一个词语\\\\n预测: {pred_word_0} (概率: {probs_0[np.argmax(probs_0)]:.2f})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('词语')\n",
        "axes[0, 0].set_ylabel('概率')\n",
        "axes[0, 0].set_xticks(range(vocab_size))\n",
        "axes[0, 0].set_xticklabels(vocab, rotation=45, ha='right')\n",
        "axes[0, 0].set_ylim(0, 1.1)\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, prob in enumerate(probs_0):\n",
        "    if prob > 0.05:  # 只标注概率大于0.05的\n",
        "        axes[0, 0].text(i, prob + 0.02, f'{prob:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 右上：位置1的预测概率\n",
        "pred_word_1 = vocab[np.argmax(probs_1)]\n",
        "colors_1 = ['red' if i == np.argmax(probs_1) else 'steelblue' for i in range(vocab_size)]\n",
        "axes[0, 1].bar(range(vocab_size), probs_1, color=colors_1, edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].set_title(f'给定\"拿起 桌子\"，预测下一个词语\\\\n预测: {pred_word_1} (概率: {probs_1[np.argmax(probs_1)]:.2f})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('词语')\n",
        "axes[0, 1].set_ylabel('概率')\n",
        "axes[0, 1].set_xticks(range(vocab_size))\n",
        "axes[0, 1].set_xticklabels(vocab, rotation=45, ha='right')\n",
        "axes[0, 1].set_ylim(0, 1.1)\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "for i, prob in enumerate(probs_1):\n",
        "    if prob > 0.05:\n",
        "        axes[0, 1].text(i, prob + 0.02, f'{prob:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 左下：位置2的预测概率\n",
        "pred_word_2 = vocab[np.argmax(probs_2)]\n",
        "colors_2 = ['red' if i == np.argmax(probs_2) else 'steelblue' for i in range(vocab_size)]\n",
        "axes[1, 0].bar(range(vocab_size), probs_2, color=colors_2, edgecolor='black', alpha=0.7)\n",
        "axes[1, 0].set_title(f'给定\"拿起 桌子 上\"，预测下一个词语\\\\n预测: {pred_word_2} (概率: {probs_2[np.argmax(probs_2)]:.2f})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('词语')\n",
        "axes[1, 0].set_ylabel('概率')\n",
        "axes[1, 0].set_xticks(range(vocab_size))\n",
        "axes[1, 0].set_xticklabels(vocab, rotation=45, ha='right')\n",
        "axes[1, 0].set_ylim(0, 1.1)\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, prob in enumerate(probs_2):\n",
        "    if prob > 0.05:\n",
        "        axes[1, 0].text(i, prob + 0.02, f'{prob:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 右下：位置3的预测概率\n",
        "pred_word_3 = vocab[np.argmax(probs_3)]\n",
        "colors_3 = ['red' if i == np.argmax(probs_3) else 'steelblue' for i in range(vocab_size)]\n",
        "axes[1, 1].bar(range(vocab_size), probs_3, color=colors_3, edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].set_title(f'给定\"拿起 桌子 上 的\"，预测下一个词语\\\\n预测: {pred_word_3} (概率: {probs_3[np.argmax(probs_3)]:.2f})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('词语')\n",
        "axes[1, 1].set_ylabel('概率')\n",
        "axes[1, 1].set_xticks(range(vocab_size))\n",
        "axes[1, 1].set_xticklabels(vocab, rotation=45, ha='right')\n",
        "axes[1, 1].set_ylim(0, 1.1)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "for i, prob in enumerate(probs_3):\n",
        "    if prob > 0.05:\n",
        "        axes[1, 1].text(i, prob + 0.02, f'{prob:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"语言建模可视化说明：\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. 左上：给定\\\"拿起\\\"，预测下一个词语，模型预测\\\"桌子\\\"的概率最高\")\n",
        "print(\"2. 右上：给定\\\"拿起 桌子\\\"，预测下一个词语，模型预测\\\"上\\\"的概率最高\")\n",
        "print(\"3. 左下：给定\\\"拿起 桌子 上\\\"，预测下一个词语，模型预测\\\"的\\\"的概率最高\")\n",
        "print(\"4. 右下：给定\\\"拿起 桌子 上 的\\\"，预测下一个词语，模型预测\\\"杯子\\\"的概率最高\")\n",
        "print(\"5. 语言建模通过预测下一个词语，学习语言的概率分布，理解语言的规律\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 掩码语言模型可视化（示例：预测被掩码的词语）\n",
        "# ============================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# 模拟一个句子：\"拿起 桌子 上 的 杯子\"\n",
        "sentence = ['拿起', '桌子', '上', '的', '杯子']\n",
        "vocab = ['拿起', '桌子', '椅子', '盘子', '杯子', '书', '放下', '移动', '其他']\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 模拟掩码语言模型预测被掩码词语的概率\n",
        "# 假设掩码\"桌子\"\n",
        "masked_sentence = ['拿起', '[MASK]', '上', '的', '杯子']\n",
        "masked_idx = 1  # \"桌子\"的位置\n",
        "\n",
        "# 模拟模型预测被掩码词语的概率分布\n",
        "mask_probs = np.array([0.05, 0.75, 0.08, 0.05, 0.03, 0.02, 0.01, 0.01, 0.00])\n",
        "mask_probs = mask_probs / np.sum(mask_probs)  # 归一化\n",
        "\n",
        "# 可视化\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 左图：掩码后的句子\n",
        "ax = axes[0]\n",
        "ax.axis('off')\n",
        "ax.text(0.5, 0.5, f'原句: {\" \".join(sentence)}\\\\n\\\\n掩码后: {\" \".join(masked_sentence)}', \n",
        "       ha='center', va='center', fontsize=14, fontweight='bold',\n",
        "       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "ax.set_title('掩码语言模型示例', fontsize=12, fontweight='bold')\n",
        "\n",
        "# 右图：预测被掩码词语的概率分布\n",
        "pred_word = vocab[np.argmax(mask_probs)]\n",
        "colors = ['red' if i == np.argmax(mask_probs) else 'steelblue' for i in range(vocab_size)]\n",
        "bars = axes[1].bar(range(vocab_size), mask_probs, color=colors, edgecolor='black', alpha=0.7)\n",
        "axes[1].set_title(f'预测被掩码词语的概率分布\\\\n预测: {pred_word} (概率: {mask_probs[np.argmax(mask_probs)]:.2f})', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('词语')\n",
        "axes[1].set_ylabel('概率')\n",
        "axes[1].set_xticks(range(vocab_size))\n",
        "axes[1].set_xticklabels(vocab, rotation=45, ha='right')\n",
        "axes[1].set_ylim(0, 1.1)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "for i, (bar, prob) in enumerate(zip(bars, mask_probs)):\n",
        "    if prob > 0.05:  # 只标注概率大于0.05的\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2., prob + 0.02,\n",
        "                    f'{prob:.2f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"掩码语言模型可视化说明：\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"1. 左图：原句和掩码后的句子，将\\\"桌子\\\"掩码为[MASK]\")\n",
        "print(f\"2. 右图：模型预测被掩码词语的概率分布，预测\\\"桌子\\\"的概率最高\")\n",
        "print(\"3. 掩码语言模型通过预测被掩码的词语，学习双向语言表示，理解上下文信息\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 迁移学习详解\n",
        "\n",
        "### 4.1 什么是迁移学习\n",
        "\n",
        "**直观理解**：想象迁移学习就像一位\"多才多艺的专家\"，他先通过大量学习（预训练）掌握了通用技能，然后能够快速适应新任务（微调）。\n",
        "\n",
        "### 4.2 为什么需要迁移学习\n",
        "\n",
        "1. **数据效率**：只需要少量标注数据就能在特定任务上取得好效果\n",
        "2. **性能提升**：预训练的知识能够显著提高模型在下游任务上的性能\n",
        "3. **通用性**：预训练的特征表示适用于各种相关任务\n",
        "4. **可扩展性**：通过增加预训练数据量和模型大小，能够持续提升性能\n",
        "\n",
        "### 4.3 迁移学习的数学表示详解\n",
        "\n",
        "#### 4.3.1 从基础数学开始\n",
        "\n",
        "**步骤1：理解预训练阶段**\n",
        "\n",
        "预训练阶段的目标是最大化预训练数据的似然函数：\n",
        "\n",
        "$$\\max_{\\theta} \\prod_{i=1}^{N_{pre}} P(x_i^{pre} | \\theta)$$\n",
        "\n",
        "其中：\n",
        "- $N_{pre}$ 是预训练样本数量\n",
        "- $x_i^{pre}$ 是第$i$个预训练样本\n",
        "- $\\theta$ 是模型参数\n",
        "\n",
        "**步骤2：理解微调阶段**\n",
        "\n",
        "微调阶段的目标是最大化下游任务数据的似然函数：\n",
        "\n",
        "$$\\max_{\\theta} \\prod_{i=1}^{N_{fine}} P(y_i | x_i^{fine}, \\theta)$$\n",
        "\n",
        "其中：\n",
        "- $N_{fine}$ 是微调样本数量\n",
        "- $x_i^{fine}$ 是第$i$个微调样本的输入\n",
        "- $y_i$ 是第$i$个微调样本的标签\n",
        "- $\\theta$ 是模型参数（使用预训练的权重初始化）\n",
        "\n",
        "**步骤3：理解知识迁移**\n",
        "\n",
        "知识迁移是指将预训练阶段学到的特征表示$\\theta_{pre}$迁移到下游任务，通过微调使模型适应特定任务：\n",
        "\n",
        "$$\\theta_{fine} = \\arg\\max_{\\theta} \\prod_{i=1}^{N_{fine}} P(y_i | x_i^{fine}, \\theta)$$\n",
        "\n",
        "其中$\\theta$的初始值为$\\theta_{pre}$。\n",
        "\n",
        "#### 4.3.2 迁移学习的具体示例\n",
        "\n",
        "**示例：从预训练到VLA微调**\n",
        "\n",
        "假设：\n",
        "- 预训练阶段：在大规模文本数据上预训练BERT模型，学习通用语言表示\n",
        "- 微调阶段：在VLA任务数据上微调BERT模型，适应VLA任务\n",
        "\n",
        "**步骤1：预训练阶段**\n",
        "\n",
        "预训练BERT模型，学习通用语言表示：\n",
        "- 输入：大规模无标注文本数据\n",
        "- 输出：预训练的BERT模型参数$\\theta_{pre}$\n",
        "\n",
        "**步骤2：微调阶段**\n",
        "\n",
        "在VLA任务上微调BERT模型：\n",
        "- 输入：VLA任务的标注数据（语言指令和对应的动作序列）\n",
        "- 初始化：使用$\\theta_{pre}$初始化模型参数\n",
        "- 输出：微调后的模型参数$\\theta_{fine}$\n",
        "\n",
        "**步骤3：知识迁移**\n",
        "\n",
        "预训练的知识（通用语言表示）被迁移到VLA任务，帮助模型理解语言指令。\n",
        "\n",
        "### 4.4 迁移学习的优势详解\n",
        "\n",
        "#### 4.4.1 数据效率\n",
        "\n",
        "迁移学习只需要少量标注数据就能在特定任务上取得好效果，因为预训练阶段已经学习了通用的语言表示。\n",
        "\n",
        "#### 4.4.2 性能提升\n",
        "\n",
        "预训练的知识能够显著提高模型在下游任务上的性能，因为预训练阶段学习了丰富的语言知识。\n",
        "\n",
        "#### 4.4.3 通用性\n",
        "\n",
        "预训练的特征表示适用于各种相关任务，因为预训练阶段学习了通用的语言规律。\n",
        "\n",
        "---\n",
        "\n",
        "## 5. 预训练语言模型在VLA中的应用\n",
        "\n",
        "### 5.1 语言指令编码\n",
        "\n",
        "在VLA中，预训练语言模型用于将语言指令编码为固定维度的特征向量，例如：\n",
        "- 将\"拿起桌子上的杯子\"编码为特征向量\n",
        "- 理解指令的意图、对象、位置等信息\n",
        "\n",
        "### 5.2 语义理解\n",
        "\n",
        "预训练语言模型的通用语言表示使得VLA模型能够理解语言指令中的全局关系，例如：\n",
        "- 理解\"拿起\"和\"杯子\"之间的关系\n",
        "- 理解\"桌子\"和\"上\"之间的关系\n",
        "\n",
        "### 5.3 预训练方法\n",
        "\n",
        "预训练语言模型可以用于VLA的预训练，通过大规模文本数据预训练语言模型（如BERT、GPT），然后在VLA任务上进行微调。\n",
        "\n",
        "### 5.4 在VLA中的具体应用示例\n",
        "\n",
        "**示例：理解\"拿起桌子上的杯子\"**\n",
        "\n",
        "如果语言指令是\"拿起桌子上的杯子\"，VLA模型需要：\n",
        "1. 使用预训练语言模型（如BERT）将每个词语转换为向量表示\n",
        "2. 通过自注意力机制理解词语之间的关系\n",
        "3. 理解指令的意图（\"拿起\"）、对象（\"杯子\"）、位置（\"桌子上\"）\n",
        "4. 结合视觉理解结果生成相应的动作序列\n",
        "\n",
        "---\n",
        "\n",
        "## 6. 总结\n",
        "\n",
        "### 6.1 预训练语言模型的核心思想\n",
        "\n",
        "1. **预训练+微调**：先在大规模无标注数据上预训练，然后在特定任务上微调\n",
        "2. **通用语言表示**：通过大规模文本数据学习到通用的语言表示\n",
        "3. **迁移学习**：将预训练的知识迁移到下游任务，提高模型性能\n",
        "4. **无监督学习**：预训练阶段不需要标注数据，可以从大规模文本数据中学习\n",
        "\n",
        "### 6.2 预训练语言模型的优势\n",
        "\n",
        "1. **通用语言表示**：能够捕获语言的语义、语法、上下文等信息\n",
        "2. **迁移学习**：将预训练的知识迁移到下游任务，提高模型性能\n",
        "3. **数据效率**：只需要少量标注数据就能在特定任务上取得好效果\n",
        "4. **可扩展性**：通过增加模型大小和数据量，能够持续提升性能\n",
        "\n",
        "### 6.3 在VLA中的意义\n",
        "\n",
        "预训练语言模型是VLA语言理解的核心技术，它提供了强大的语言特征提取能力，使得VLA模型能够理解复杂的语言指令。预训练语言模型的通用语言表示和迁移学习能力使得VLA模型能够更好地理解语言指令，从而生成更准确的动作序列。\n",
        "\n",
        "---\n",
        "\n",
        "**文档完成时间**：2025-01-27  \n",
        "**文档版本**：v1.0  \n",
        "**维护者**：AI助手\n",
        "\n",
        "**相关文档**：\n",
        "- 父目录：文本特征提取详解（见../文本特征提取详解.ipynb）\n",
        "- BERT详解（见../../02_语言编码器/01_BERT/理论笔记/BERT详解.ipynb）\n",
        "- GPT详解（见../../02_语言编码器/02_GPT/理论笔记/GPT详解.ipynb）\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
