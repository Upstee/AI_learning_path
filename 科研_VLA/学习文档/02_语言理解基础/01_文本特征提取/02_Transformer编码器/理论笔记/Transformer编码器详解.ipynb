{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer编码器详解\n",
        "\n",
        "## 📋 文档说明\n",
        "\n",
        "本文档是Transformer编码器的详细理论讲解，比父目录的《文本特征提取详解》更加深入和详细。本文档将深入讲解Transformer编码器的原理、数学推导和实现细节。通过本文档，你将能够：\n",
        "\n",
        "1. **深入理解Transformer编码器的原理**：从自注意力机制到位置编码的完整流程\n",
        "2. **掌握自注意力机制的数学原理**：理解Query、Key、Value的数学定义、为什么有效、如何计算\n",
        "3. **理解多头注意力机制**：理解多头注意力的原理和优势\n",
        "4. **掌握位置编码**：理解位置编码如何保留序列的位置信息\n",
        "5. **掌握Transformer编码器在VLA中的应用**：理解Transformer编码器在VLA模型中的具体应用和优势\n",
        "\n",
        "**学习方式**：本文件是Jupyter Notebook格式，你可以边看边运行代码，通过可视化图表和数学推导更好地理解Transformer编码器的原理和过程。\n",
        "\n",
        "**文档结构**：\n",
        "- 父目录：文本特征提取详解（见../文本特征提取详解.ipynb）\n",
        "- 本文档：Transformer编码器详解（本文档）\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 术语表（按出现顺序）\n",
        "\n",
        "### 1. Transformer编码器 (Transformer Encoder)\n",
        "- **中文名称**：Transformer编码器\n",
        "- **英文全称**：Transformer Encoder\n",
        "- **定义**：Transformer编码器是指使用Transformer架构将输入序列编码为固定维度的特征向量的神经网络模块，是自然语言处理的基础架构。Transformer编码器通过自注意力机制和位置编码处理输入序列，能够捕获序列中每个位置与其他位置之间的关系，提取序列的语义信息。Transformer编码器的优势在于：1）全局感受野：自注意力机制能够捕获序列中任意两个位置之间的关系，具有全局感受野；2）并行计算：能够并行计算所有位置之间的关系，提高训练和推理效率；3）长距离依赖：能够捕获长距离依赖关系，不受距离限制；4）可扩展性：通过增加模型大小和数据量，能够持续提升性能。Transformer编码器是现代自然语言处理和VLA的核心架构，BERT、GPT等预训练语言模型都基于Transformer编码器。Transformer编码器通过自注意力机制实现全局感受野，这是RNN无法做到的，这使得Transformer能够更好地理解文本中的长距离依赖关系。\n",
        "- **核心组成**：Transformer编码器的核心组成包括：1）词嵌入层：将词语转换为词向量；2）位置编码：为每个位置添加位置编码，保留序列的位置信息；3）自注意力机制：使用自注意力机制捕获序列中每个位置与其他位置之间的关系；4）多头注意力：使用多个注意力头，从不同的角度关注输入序列；5）前馈神经网络：使用前馈神经网络处理注意力输出；6）残差连接和层归一化：使用残差连接和层归一化加速训练，提高稳定性。Transformer编码器通常由多个编码器层堆叠而成，每个编码器层包含自注意力机制和前馈神经网络。Transformer编码器通过多层堆叠，能够提取不同层次的特征，从低层的词语特征到高层的语义特征。\n",
        "- **在VLA中的应用**：在VLA中，Transformer编码器是语言编码器的核心架构。VLA模型使用Transformer编码器从输入文本中提取语言特征，这些特征将被用于理解语言指令的意图、对象、动作等。Transformer编码器的优势在于能够捕获文本中的长距离依赖关系，这对于理解复杂的语言指令非常重要。在VLA训练过程中，Transformer编码器通常使用预训练的权重初始化（如BERT、GPT），然后在VLA任务上进行微调，以提高特征提取的质量和效率。Transformer编码器的输出特征将与视觉编码器的输出特征进行融合，生成多模态表示，最终用于动作生成。Transformer编码器的全局感受野使得VLA模型能够更好地理解语言指令中的全局关系，这对于理解复杂的语言指令非常重要。\n",
        "- **相关概念**：自注意力机制、多头注意力、位置编码、BERT、GPT、语言编码器、文本特征提取\n",
        "- **首次出现位置**：本文档标题\n",
        "- **深入学习**：参考父目录的[文本特征提取详解](../文本特征提取详解.ipynb)\n",
        "- **直观理解**：想象Transformer编码器就像一位\"语言分析师\"，他能够从文本中提取关键信息，理解每个词语与其他词语之间的关系。例如，看到\"拿起桌子上的杯子\"这个句子，Transformer编码器能够理解\"拿起\"是动作，\"杯子\"是对象，\"桌子上\"是位置，并且理解它们之间的关系。在VLA中，Transformer编码器就是这样的\"分析师\"，它帮助模型理解语言指令，从而生成相应的动作。Transformer编码器就像文本的\"语义地图\"，能够将语义相关的词语映射到相近的位置。\n",
        "\n",
        "### 2. 自注意力机制 (Self-Attention Mechanism)\n",
        "- **中文名称**：自注意力机制\n",
        "- **英文全称**：Self-Attention Mechanism\n",
        "- **定义**：自注意力机制是Transformer的核心机制，用于计算序列中每个位置与其他位置之间的关系。自注意力机制允许模型关注输入序列中的不同位置，计算每个位置与其他位置之间的相关性。自注意力机制的数学定义为：$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$，其中 $Q$（Query）、$K$（Key）、$V$（Value）是通过线性变换得到的。自注意力机制的优势在于：1）全局感受野：能够捕获序列中任意两个位置之间的关系；2）并行计算：能够并行计算所有位置之间的关系；3）长距离依赖：能够捕获长距离依赖关系，不受距离限制；4）可解释性：注意力权重可以解释模型关注的位置。自注意力机制是Transformer编码器的核心，它使Transformer能够捕获文本中的长距离依赖关系，这是RNN无法做到的。\n",
        "- **核心组成**：自注意力机制的核心组成包括：1）Query（Q）：查询向量，用于查询其他位置的信息；2）Key（K）：键向量，用于匹配查询；3）Value（V）：值向量，包含实际的信息；4）注意力分数：计算Query和Key之间的相似度，数学表示为 $S = \\frac{QK^T}{\\sqrt{d_k}}$；5）注意力权重：通过softmax归一化注意力分数，数学表示为 $A = \\text{softmax}(S)$；6）加权求和：使用注意力权重对Value进行加权求和，得到每个位置的输出，数学表示为 $O = AV$。自注意力机制通过Query、Key、Value三个向量计算注意力权重，然后使用注意力权重对Value进行加权求和，得到每个位置的输出。多头注意力机制使用多个注意力头，从不同的角度关注输入序列，提高模型的表达能力。\n",
        "- **在VLA中的应用**：在VLA中，自注意力机制是Transformer编码器的核心。VLA模型使用自注意力机制捕获文本中的长距离依赖关系，这对于理解复杂的语言指令非常重要。自注意力机制的全局感受野使得VLA模型能够理解文本中的全局关系，例如词语之间的关系、句子的语义等。在VLA训练过程中，自注意力机制的参数通过端到端训练学习，能够自动学习最适合VLA任务的注意力模式。理解自注意力机制有助于理解Transformer如何捕获文本中的长距离依赖关系，如何理解复杂的语言指令。\n",
        "- **相关概念**：Transformer、多头注意力、位置编码、Query、Key、Value、注意力权重\n",
        "- **首次出现位置**：本文档第2.1节\n",
        "- **深入学习**：参考本文档的自注意力机制详细讲解部分\n",
        "- **直观理解**：想象自注意力机制就像让序列中的每个位置\"互相交流\"，每个位置可以\"询问\"其他位置的信息，然后根据\"询问\"的结果决定关注哪些位置。自注意力机制就是这样的过程，它通过Query、Key、Value三个向量计算注意力权重，然后使用注意力权重对Value进行加权求和，得到每个位置的输出。通过自注意力机制，Transformer能够捕获文本中的长距离依赖关系，理解复杂的语言指令。\n",
        "\n",
        "### 3. 多头注意力 (Multi-Head Attention)\n",
        "- **中文名称**：多头注意力\n",
        "- **英文全称**：Multi-Head Attention\n",
        "- **定义**：多头注意力是指使用多个注意力头并行计算注意力，从不同的角度关注输入序列。多头注意力的数学表示为：$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$，其中每个注意力头 $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$。多头注意力的优势在于：1）多角度关注：从不同的角度关注输入序列，提高模型的表达能力；2）并行计算：多个注意力头可以并行计算，提高计算效率；3）特征多样性：不同的注意力头关注不同的特征，形成更丰富的特征表示；4）可解释性：可以分析不同注意力头关注的内容，理解模型的行为。多头注意力是Transformer编码器的核心组件，它通过多个注意力头从不同的角度关注输入序列，提高模型的表达能力。\n",
        "- **核心组成**：多头注意力的核心组成包括：1）多个注意力头：使用多个独立的注意力头，每个注意力头有自己的Query、Key、Value权重矩阵；2）并行计算：多个注意力头可以并行计算，提高计算效率；3）特征拼接：将多个注意力头的输出拼接在一起，形成更丰富的特征表示；4）线性投影：使用线性投影将拼接后的特征映射到输出维度。多头注意力通常使用8个或16个注意力头，每个注意力头关注不同的特征，形成更丰富的特征表示。多头注意力的输出维度通常等于输入维度，这使得多头注意力可以堆叠多个层。\n",
        "- **在VLA中的应用**：在VLA中，多头注意力是Transformer编码器的核心组件。VLA模型使用多头注意力从不同的角度关注输入文本，理解语言指令的不同方面，例如动作、对象、位置等。多头注意力的多角度关注使得VLA模型能够更好地理解复杂的语言指令，提取更丰富的语言特征。在VLA训练过程中，多头注意力的参数通过端到端训练学习，能够自动学习最适合VLA任务的注意力模式。理解多头注意力有助于理解Transformer如何从不同的角度关注输入序列，如何提取更丰富的特征表示。\n",
        "- **相关概念**：自注意力机制、Transformer、注意力头、Query、Key、Value、特征拼接\n",
        "- **首次出现位置**：本文档第2.3节\n",
        "- **深入学习**：参考本文档的多头注意力详细讲解部分\n",
        "- **直观理解**：想象多头注意力就像让多个\"分析师\"同时分析文本，每个\"分析师\"关注不同的方面。例如，一个\"分析师\"关注动作（\"拿起\"、\"放下\"），另一个\"分析师\"关注对象（\"杯子\"、\"盘子\"），还有一个\"分析师\"关注位置（\"桌子上\"、\"架子上\"）。通过多个\"分析师\"的分析，我们能够更全面地理解文本的内容。在VLA中，多头注意力帮助模型从不同的角度理解语言指令，从而生成更准确的动作。\n",
        "\n",
        "### 4. 位置编码 (Positional Encoding)\n",
        "- **中文名称**：位置编码\n",
        "- **英文全称**：Positional Encoding\n",
        "- **定义**：位置编码是指为序列中的每个位置添加位置信息的方法，用于保留序列的位置信息。由于Transformer的自注意力机制是位置无关的（permutation-invariant），它无法区分不同位置的词语，因此需要位置编码来保留序列的位置信息。位置编码的数学表示为：$PE(pos, 2i) = \\sin(\\frac{pos}{10000^{2i/d_{model}}})$ 和 $PE(pos, 2i+1) = \\cos(\\frac{pos}{10000^{2i/d_{model}}})$，其中 $pos$ 是位置，$i$ 是维度索引，$d_{model}$ 是模型维度。位置编码的优势在于：1）保留位置信息：能够保留序列中每个位置的位置信息；2）可学习：位置编码可以是固定的（正弦位置编码）或可学习的（学习位置编码）；3）可扩展性：能够处理不同长度的序列；4）相对位置：能够捕获相对位置信息。位置编码是Transformer编码器的重要组成部分，它使Transformer能够处理序列的顺序信息。\n",
        "- **核心组成**：位置编码的核心组成包括：1）位置索引：为序列中的每个位置分配一个索引（0, 1, 2, ...）；2）位置编码函数：使用正弦和余弦函数计算位置编码；3）位置编码向量：为每个位置生成一个位置编码向量，维度与词向量相同；4）位置编码融合：将位置编码与词向量相加，得到包含位置信息的词向量。位置编码可以是固定的（正弦位置编码）或可学习的（学习位置编码）。固定的位置编码使用正弦和余弦函数，能够处理任意长度的序列；可学习的位置编码通过训练学习，能够学习最适合任务的位置表示。\n",
        "- **在VLA中的应用**：在VLA中，位置编码用于保留语言指令中词语的位置信息。例如，在\"拿起桌子上的杯子\"这个句子中，位置编码帮助模型理解\"拿起\"在句首，\"杯子\"在句末，以及它们之间的相对位置。位置编码的位置信息对于理解语言指令的顺序非常重要，例如理解\"先拿起杯子，再放下\"这样的顺序指令。在VLA训练过程中，位置编码通常是固定的（正弦位置编码），但也可以是可学习的（学习位置编码）。理解位置编码有助于理解Transformer如何保留序列的位置信息，如何理解语言指令的顺序。\n",
        "- **相关概念**：Transformer、自注意力机制、词向量、序列位置、正弦位置编码、学习位置编码\n",
        "- **首次出现位置**：本文档第1.3节\n",
        "- **深入学习**：参考本文档的位置编码详细讲解部分\n",
        "- **直观理解**：想象位置编码就像给序列中的每个位置贴上\"标签\"，告诉模型这个位置在序列中的位置。例如，在\"拿起桌子上的杯子\"这个句子中，位置编码会给\"拿起\"贴上\"位置0\"的标签，给\"桌子\"贴上\"位置1\"的标签，给\"上\"贴上\"位置2\"的标签，等等。通过位置编码，模型能够理解词语在序列中的位置，理解语言指令的顺序。在VLA中，位置编码帮助模型理解语言指令的顺序，从而生成正确的动作序列。\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 概述\n",
        "\n",
        "### 什么是Transformer编码器\n",
        "\n",
        "Transformer编码器是指使用Transformer架构将输入序列编码为固定维度的特征向量的神经网络模块，是自然语言处理的基础架构。Transformer编码器通过自注意力机制和位置编码处理输入序列，能够捕获序列中每个位置与其他位置之间的关系，提取序列的语义信息。\n",
        "\n",
        "### 为什么重要\n",
        "\n",
        "Transformer编码器对于VLA学习非常重要，原因包括：\n",
        "\n",
        "1. **全局感受野**：自注意力机制能够捕获序列中任意两个位置之间的关系\n",
        "2. **长距离依赖**：能够捕获长距离依赖关系，不受距离限制\n",
        "3. **并行计算**：能够并行计算所有位置之间的关系，提高训练和推理效率\n",
        "4. **预训练模型**：BERT、GPT等预训练语言模型都基于Transformer编码器\n",
        "\n",
        "### 学习目标\n",
        "\n",
        "通过本文档的学习，你将能够：\n",
        "\n",
        "1. **深入理解自注意力机制**：理解Query、Key、Value的数学定义和计算方法\n",
        "2. **掌握多头注意力**：理解多头注意力的原理和优势\n",
        "3. **理解位置编码**：理解位置编码如何保留序列的位置信息\n",
        "4. **掌握Transformer编码器在VLA中的应用**：理解Transformer编码器在VLA模型中的具体应用\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Transformer编码器的基本原理\n",
        "\n",
        "### 1.1 什么是Transformer编码器\n",
        "\n",
        "**直观理解**：想象Transformer编码器就像一位\"语言分析师\"，他能够从文本中提取关键信息，理解每个词语与其他词语之间的关系。\n",
        "\n",
        "### 1.2 为什么需要Transformer编码器\n",
        "\n",
        "1. **全局感受野**：自注意力机制能够捕获序列中任意两个位置之间的关系\n",
        "2. **长距离依赖**：能够捕获长距离依赖关系，不受距离限制\n",
        "3. **并行计算**：能够并行计算所有位置之间的关系，提高训练和推理效率\n",
        "4. **预训练模型**：BERT、GPT等预训练语言模型都基于Transformer编码器\n",
        "\n",
        "### 1.3 位置编码详解\n",
        "\n",
        "#### 1.3.1 什么是位置编码\n",
        "\n",
        "**直观理解**：想象位置编码就像给序列中的每个位置贴上\"标签\"，告诉模型这个位置在序列中的位置。\n",
        "\n",
        "#### 1.3.2 为什么需要位置编码\n",
        "\n",
        "Transformer的自注意力机制是位置无关的（permutation-invariant），它无法区分不同位置的词语。例如，对于序列[\"拿起\", \"杯子\"]和[\"杯子\", \"拿起\"]，如果没有位置编码，Transformer会认为它们是相同的。\n",
        "\n",
        "#### 1.3.3 位置编码的数学推导详解\n",
        "\n",
        "#### 1.3.3.1 从基础数学开始\n",
        "\n",
        "**步骤1：理解正弦和余弦函数**\n",
        "\n",
        "正弦函数 $\\sin(x)$ 和余弦函数 $\\cos(x)$ 是周期函数，能够表示位置信息。\n",
        "\n",
        "**步骤2：理解位置编码的公式**\n",
        "\n",
        "位置编码使用正弦和余弦函数计算：\n",
        "\n",
        "$$PE(pos, 2i) = \\sin(\\frac{pos}{10000^{2i/d_{model}}})$$\n",
        "\n",
        "$$PE(pos, 2i+1) = \\cos(\\frac{pos}{10000^{2i/d_{model}}})$$\n",
        "\n",
        "其中：\n",
        "- $pos$ 是位置（0, 1, 2, ...）\n",
        "- $i$ 是维度索引（0, 1, 2, ..., $d_{model}/2-1$）\n",
        "- $d_{model}$ 是模型维度（通常为512或768）\n",
        "\n",
        "**为什么使用不同的频率？**\n",
        "\n",
        "使用不同的频率（$10000^{2i/d_{model}}$）能够使不同维度的位置编码具有不同的周期，从而能够表示不同范围的位置信息。\n",
        "\n",
        "#### 1.3.3.2 位置编码的具体计算示例\n",
        "\n",
        "**示例：计算位置0和位置1的位置编码**\n",
        "\n",
        "假设：\n",
        "- 模型维度：$d_{model} = 8$\n",
        "- 位置0和位置1\n",
        "\n",
        "**步骤1：计算位置0的位置编码**\n",
        "\n",
        "对于维度 $i = 0$：\n",
        "- $PE(0, 0) = \\sin(\\frac{0}{10000^{0/8}}) = \\sin(0) = 0$\n",
        "- $PE(0, 1) = \\cos(\\frac{0}{10000^{0/8}}) = \\cos(0) = 1$\n",
        "\n",
        "对于维度 $i = 1$：\n",
        "- $PE(0, 2) = \\sin(\\frac{0}{10000^{2/8}}) = \\sin(0) = 0$\n",
        "- $PE(0, 3) = \\cos(\\frac{0}{10000^{2/8}}) = \\cos(0) = 1$\n",
        "\n",
        "所以位置0的位置编码为：$[0, 1, 0, 1, 0, 1, 0, 1]$\n",
        "\n",
        "**步骤2：计算位置1的位置编码**\n",
        "\n",
        "对于维度 $i = 0$：\n",
        "- $PE(1, 0) = \\sin(\\frac{1}{10000^{0/8}}) = \\sin(1) \\approx 0.84$\n",
        "- $PE(1, 1) = \\cos(\\frac{1}{10000^{0/8}}) = \\cos(1) \\approx 0.54$\n",
        "\n",
        "对于维度 $i = 1$：\n",
        "- $PE(1, 2) = \\sin(\\frac{1}{10000^{2/8}}) = \\sin(\\frac{1}{10000^{0.25}}) \\approx 0.00$\n",
        "- $PE(1, 3) = \\cos(\\frac{1}{10000^{2/8}}) = \\cos(\\frac{1}{10000^{0.25}}) \\approx 1.00$\n",
        "\n",
        "所以位置1的位置编码为：$[0.84, 0.54, 0.00, 1.00, \\ldots]$\n",
        "\n",
        "### 1.4 位置编码的可视化\n",
        "\n",
        "下面我们通过代码可视化位置编码：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 导入必要的库\n",
        "# ============================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Microsoft YaHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "print(\"环境准备完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 位置编码可视化（正弦位置编码）\n",
        "# ============================================\n",
        "def get_positional_encoding(max_len, d_model):\n",
        "    \"\"\"计算位置编码\"\"\"\n",
        "    pe = np.zeros((max_len, d_model))\n",
        "    position = np.arange(0, max_len).reshape(-1, 1)\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    \n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    \n",
        "    return pe\n",
        "\n",
        "# 计算位置编码\n",
        "max_len = 20  # 最大序列长度\n",
        "d_model = 64  # 模型维度\n",
        "pe = get_positional_encoding(max_len, d_model)\n",
        "\n",
        "# 可视化\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 左上：位置编码热图\n",
        "im1 = axes[0, 0].imshow(pe.T, cmap='RdBu', aspect='auto')\n",
        "axes[0, 0].set_title('位置编码热图（每个位置的位置编码向量）', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('位置索引')\n",
        "axes[0, 0].set_ylabel('维度索引')\n",
        "plt.colorbar(im1, ax=axes[0, 0])\n",
        "\n",
        "# 右上：不同位置的编码（前8个维度）\n",
        "for pos in [0, 5, 10, 15]:\n",
        "    axes[0, 1].plot(pe[pos, :8], label=f'位置{pos}', marker='o', linewidth=2, markersize=6)\n",
        "axes[0, 1].set_title('不同位置的位置编码（前8个维度）', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('维度索引')\n",
        "axes[0, 1].set_ylabel('编码值')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 左下：不同维度的编码（前10个位置）\n",
        "for dim in [0, 2, 4, 6]:\n",
        "    axes[1, 0].plot(pe[:10, dim], label=f'维度{dim}', marker='o', linewidth=2, markersize=6)\n",
        "axes[1, 0].set_title('不同维度的位置编码（前10个位置）', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('位置索引')\n",
        "axes[1, 0].set_ylabel('编码值')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 右下：位置编码的周期性\n",
        "axes[1, 1].plot(pe[:, 0], label='维度0（sin）', linewidth=2)\n",
        "axes[1, 1].plot(pe[:, 1], label='维度1（cos）', linewidth=2)\n",
        "axes[1, 1].set_title('位置编码的周期性（维度0和1）', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('位置索引')\n",
        "axes[1, 1].set_ylabel('编码值')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"位置编码可视化说明：\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. 左上：位置编码热图，展示每个位置的位置编码向量\")\n",
        "print(\"2. 右上：不同位置的编码，展示不同位置的位置编码差异\")\n",
        "print(\"3. 左下：不同维度的编码，展示不同维度的位置编码模式\")\n",
        "print(\"4. 右下：位置编码的周期性，展示正弦和余弦函数的周期性\")\n",
        "print(\"5. 位置编码能够保留序列中每个位置的位置信息\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 自注意力机制详解\n",
        "\n",
        "### 2.1 什么是自注意力机制\n",
        "\n",
        "**直观理解**：想象自注意力机制就像让序列中的每个位置\"互相交流\"，每个位置可以\"询问\"其他位置的信息，然后根据\"询问\"的结果决定关注哪些位置。\n",
        "\n",
        "### 2.2 为什么需要自注意力机制\n",
        "\n",
        "1. **全局感受野**：能够捕获序列中任意两个位置之间的关系\n",
        "2. **长距离依赖**：能够捕获长距离依赖关系，不受距离限制\n",
        "3. **并行计算**：能够并行计算所有位置之间的关系，提高计算效率\n",
        "4. **可解释性**：注意力权重可以解释模型关注的位置\n",
        "\n",
        "### 2.3 自注意力机制的数学推导详解\n",
        "\n",
        "#### 2.3.1 从基础数学开始\n",
        "\n",
        "**步骤1：理解Query、Key、Value**\n",
        "\n",
        "- **Query（Q）**：查询向量，用于查询其他位置的信息\n",
        "- **Key（K）**：键向量，用于匹配查询\n",
        "- **Value（V）**：值向量，包含实际的信息\n",
        "\n",
        "**步骤2：理解注意力分数**\n",
        "\n",
        "注意力分数衡量Query和Key之间的相似度：\n",
        "\n",
        "$$S = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
        "\n",
        "其中：\n",
        "- $Q \\in \\mathbb{R}^{n \\times d_k}$ 是Query矩阵（$n$是序列长度）\n",
        "- $K \\in \\mathbb{R}^{n \\times d_k}$ 是Key矩阵\n",
        "- $d_k$ 是Key的维度\n",
        "- $\\sqrt{d_k}$ 是缩放因子，防止点积过大\n",
        "\n",
        "**为什么需要缩放因子？**\n",
        "\n",
        "当$d_k$很大时，点积$QK^T$的值会很大，导致softmax的梯度很小。使用缩放因子$\\sqrt{d_k}$可以防止这个问题。\n",
        "\n",
        "**步骤3：理解注意力权重**\n",
        "\n",
        "通过softmax归一化注意力分数，得到注意力权重：\n",
        "\n",
        "$$A = \\text{softmax}(S) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})$$\n",
        "\n",
        "**步骤4：理解加权求和**\n",
        "\n",
        "使用注意力权重对Value进行加权求和，得到每个位置的输出：\n",
        "\n",
        "$$O = AV$$\n",
        "\n",
        "其中 $V \\in \\mathbb{R}^{n \\times d_v}$ 是Value矩阵。\n",
        "\n",
        "#### 2.3.2 自注意力机制的具体计算示例\n",
        "\n",
        "**示例：计算3个词语的自注意力**\n",
        "\n",
        "假设我们有一个句子：\"拿起 杯子\"，序列长度为2（简化示例）。\n",
        "\n",
        "**步骤1：获取词向量**\n",
        "\n",
        "假设词向量维度$d_{model} = 4$：\n",
        "- $\\mathbf{v}_{拿起} = [0.2, -0.1, 0.5, 0.3]$\n",
        "- $\\mathbf{v}_{杯子} = [0.1, 0.4, -0.2, 0.6]$\n",
        "\n",
        "**步骤2：计算Query、Key、Value**\n",
        "\n",
        "假设权重矩阵：\n",
        "- $W^Q = \\begin{bmatrix} 0.5 & 0.2 & -0.1 & 0.3 \\\\ 0.1 & 0.4 & 0.2 & -0.1 \\\\ -0.2 & 0.3 & 0.5 & 0.1 \\\\ 0.3 & -0.1 & 0.2 & 0.4 \\end{bmatrix}$\n",
        "- $W^K = \\begin{bmatrix} 0.3 & 0.1 & 0.4 & -0.2 \\\\ 0.2 & 0.5 & -0.1 & 0.3 \\\\ 0.1 & -0.2 & 0.3 & 0.5 \\\\ -0.1 & 0.3 & 0.2 & 0.4 \\end{bmatrix}$\n",
        "- $W^V = \\begin{bmatrix} 0.4 & 0.2 & -0.1 & 0.3 \\\\ 0.2 & 0.3 & 0.4 & -0.1 \\\\ -0.1 & 0.4 & 0.2 & 0.5 \\\\ 0.3 & -0.1 & 0.3 & 0.4 \\end{bmatrix}$\n",
        "\n",
        "对于\"拿起\"：\n",
        "- $Q_1 = \\mathbf{v}_{拿起} W^Q = [0.2, -0.1, 0.5, 0.3] W^Q = [0.15, 0.05, 0.20, 0.25]$\n",
        "- $K_1 = \\mathbf{v}_{拿起} W^K = [0.2, -0.1, 0.5, 0.3] W^K = [0.12, 0.08, 0.25, 0.30]$\n",
        "- $V_1 = \\mathbf{v}_{拿起} W^V = [0.2, -0.1, 0.5, 0.3] W^V = [0.18, 0.10, 0.22, 0.28]$\n",
        "\n",
        "对于\"杯子\"：\n",
        "- $Q_2 = \\mathbf{v}_{杯子} W^Q = [0.1, 0.4, -0.2, 0.6] W^Q = [0.10, 0.15, -0.05, 0.20]$\n",
        "- $K_2 = \\mathbf{v}_{杯子} W^K = [0.1, 0.4, -0.2, 0.6] W^K = [0.08, 0.12, -0.10, 0.25]$\n",
        "- $V_2 = \\mathbf{v}_{杯子} W^V = [0.1, 0.4, -0.2, 0.6] W^V = [0.12, 0.18, -0.08, 0.30]$\n",
        "\n",
        "**步骤3：计算注意力分数**\n",
        "\n",
        "对于位置1（\"拿起\"）查询位置2（\"杯子\"）：\n",
        "\n",
        "$$S_{1,2} = \\frac{Q_1 \\cdot K_2}{\\sqrt{d_k}} = \\frac{[0.15, 0.05, 0.20, 0.25] \\cdot [0.08, 0.12, -0.10, 0.25]}{\\sqrt{4}} = \\frac{0.012 + 0.006 - 0.020 + 0.063}{2} = \\frac{0.061}{2} = 0.031$$\n",
        "\n",
        "**步骤4：计算注意力权重**\n",
        "\n",
        "假设所有注意力分数为：\n",
        "- $S_{1,1} = 0.5$（\"拿起\"关注\"拿起\"）\n",
        "- $S_{1,2} = 0.3$（\"拿起\"关注\"杯子\"）\n",
        "- $S_{2,1} = 0.4$（\"杯子\"关注\"拿起\"）\n",
        "- $S_{2,2} = 0.6$（\"杯子\"关注\"杯子\"）\n",
        "\n",
        "对于位置1（\"拿起\"）：\n",
        "- $\\exp(S_{1,1}) = \\exp(0.5) = 1.65$\n",
        "- $\\exp(S_{1,2}) = \\exp(0.3) = 1.35$\n",
        "- 总和：$1.65 + 1.35 = 3.00$\n",
        "- $A_{1,1} = \\frac{1.65}{3.00} = 0.55$\n",
        "- $A_{1,2} = \\frac{1.35}{3.00} = 0.45$\n",
        "\n",
        "**步骤5：计算输出**\n",
        "\n",
        "对于位置1（\"拿起\"）的输出：\n",
        "\n",
        "$$O_1 = A_{1,1} V_1 + A_{1,2} V_2 = 0.55 \\times [0.18, 0.10, 0.22, 0.28] + 0.45 \\times [0.12, 0.18, -0.08, 0.30] = [0.153, 0.136, 0.085, 0.289]$$\n",
        "\n",
        "### 2.4 自注意力机制的可视化\n",
        "\n",
        "下面我们通过代码可视化自注意力机制：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 自注意力机制可视化（示例：5个词语的序列）\n",
        "# ============================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# 模拟一个句子：\"拿起 桌子 上 的 杯子\"\n",
        "sentence = ['拿起', '桌子', '上', '的', '杯子']\n",
        "seq_len = len(sentence)\n",
        "d_model = 8\n",
        "d_k = d_model\n",
        "\n",
        "# 模拟词向量（随机初始化）\n",
        "word_vectors = np.random.randn(seq_len, d_model) * 0.5\n",
        "\n",
        "# 模拟Query、Key、Value权重矩阵\n",
        "W_Q = np.random.randn(d_model, d_k) * 0.1\n",
        "W_K = np.random.randn(d_model, d_k) * 0.1\n",
        "W_V = np.random.randn(d_model, d_model) * 0.1\n",
        "\n",
        "# 计算Query、Key、Value\n",
        "Q = word_vectors @ W_Q\n",
        "K = word_vectors @ W_K\n",
        "V = word_vectors @ W_V\n",
        "\n",
        "# 计算注意力分数\n",
        "S = Q @ K.T / np.sqrt(d_k)\n",
        "\n",
        "# 计算注意力权重\n",
        "A = np.exp(S) / np.sum(np.exp(S), axis=1, keepdims=True)\n",
        "\n",
        "# 计算输出\n",
        "O = A @ V\n",
        "\n",
        "# 可视化\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 左上：注意力分数矩阵\n",
        "im1 = axes[0, 0].imshow(S, cmap='RdBu', aspect='auto')\n",
        "axes[0, 0].set_title('注意力分数矩阵 S = QK^T / √d_k', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Key位置（被关注的位置）')\n",
        "axes[0, 0].set_ylabel('Query位置（关注的位置）')\n",
        "axes[0, 0].set_xticks(range(seq_len))\n",
        "axes[0, 0].set_yticks(range(seq_len))\n",
        "axes[0, 0].set_xticklabels(sentence)\n",
        "axes[0, 0].set_yticklabels(sentence)\n",
        "# 添加数值标注\n",
        "for i in range(seq_len):\n",
        "    for j in range(seq_len):\n",
        "        axes[0, 0].text(j, i, f'{S[i, j]:.2f}', ha='center', va='center', \n",
        "                       color='white' if abs(S[i, j]) < np.max(np.abs(S)) * 0.5 else 'black', \n",
        "                       fontsize=9)\n",
        "plt.colorbar(im1, ax=axes[0, 0])\n",
        "\n",
        "# 右上：注意力权重矩阵\n",
        "im2 = axes[0, 1].imshow(A, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
        "axes[0, 1].set_title('注意力权重矩阵 A = softmax(S)', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Key位置（被关注的位置）')\n",
        "axes[0, 1].set_ylabel('Query位置（关注的位置）')\n",
        "axes[0, 1].set_xticks(range(seq_len))\n",
        "axes[0, 1].set_yticks(range(seq_len))\n",
        "axes[0, 1].set_xticklabels(sentence)\n",
        "axes[0, 1].set_yticklabels(sentence)\n",
        "# 添加数值标注\n",
        "for i in range(seq_len):\n",
        "    for j in range(seq_len):\n",
        "        axes[0, 1].text(j, i, f'{A[i, j]:.2f}', ha='center', va='center', \n",
        "                       color='white' if A[i, j] < 0.5 else 'black', fontsize=9)\n",
        "plt.colorbar(im2, ax=axes[0, 1])\n",
        "\n",
        "# 左下：位置\"拿起\"的注意力权重\n",
        "axes[1, 0].bar(range(seq_len), A[0, :], color='steelblue', edgecolor='black', alpha=0.7)\n",
        "axes[1, 0].set_title(f'位置\"拿起\"的注意力权重\\n关注哪些位置？', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('位置')\n",
        "axes[1, 0].set_ylabel('注意力权重')\n",
        "axes[1, 0].set_xticks(range(seq_len))\n",
        "axes[1, 0].set_xticklabels(sentence, rotation=45, ha='right')\n",
        "axes[1, 0].set_ylim(0, 1.1)\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "# 标注权重值\n",
        "for i, weight in enumerate(A[0, :]):\n",
        "    axes[1, 0].text(i, weight + 0.02, f'{weight:.2f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# 右下：位置\"杯子\"的注意力权重\n",
        "axes[1, 1].bar(range(seq_len), A[-1, :], color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].set_title(f'位置\"杯子\"的注意力权重\\n关注哪些位置？', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('位置')\n",
        "axes[1, 1].set_ylabel('注意力权重')\n",
        "axes[1, 1].set_xticks(range(seq_len))\n",
        "axes[1, 1].set_xticklabels(sentence, rotation=45, ha='right')\n",
        "axes[1, 1].set_ylim(0, 1.1)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "# 标注权重值\n",
        "for i, weight in enumerate(A[-1, :]):\n",
        "    axes[1, 1].text(i, weight + 0.02, f'{weight:.2f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"自注意力机制可视化说明：\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. 左上：注意力分数矩阵，展示每个位置对其他位置的关注程度（未归一化）\")\n",
        "print(\"2. 右上：注意力权重矩阵，展示每个位置对其他位置的关注程度（归一化后）\")\n",
        "print(\"3. 左下：位置\\\"拿起\\\"的注意力权重，展示\\\"拿起\\\"关注哪些位置\")\n",
        "print(\"4. 右下：位置\\\"杯子\\\"的注意力权重，展示\\\"杯子\\\"关注哪些位置\")\n",
        "print(\"5. 自注意力机制使每个位置能够关注序列中的所有位置，捕获长距离依赖关系\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 缩放因子的作用详解\n",
        "\n",
        "#### 2.5.1 为什么需要缩放因子\n",
        "\n",
        "当$d_k$很大时，点积$QK^T$的值会很大，导致softmax的梯度很小。使用缩放因子$\\sqrt{d_k}$可以防止这个问题。\n",
        "\n",
        "#### 2.5.2 缩放因子的数学证明\n",
        "\n",
        "**步骤1：理解点积的方差**\n",
        "\n",
        "假设$Q$和$K$的元素独立同分布，均值为0，方差为1，则点积$QK^T$的方差为$d_k$。\n",
        "\n",
        "**步骤2：理解softmax的梯度**\n",
        "\n",
        "当输入值很大时，softmax的输出会接近one-hot分布，梯度接近0。\n",
        "\n",
        "**步骤3：理解缩放因子的作用**\n",
        "\n",
        "使用缩放因子$\\sqrt{d_k}$后，点积的方差变为1，使得softmax的梯度保持在合理范围内。\n",
        "\n",
        "### 2.6 Query、Key、Value的作用详解\n",
        "\n",
        "#### 2.6.1 Query的作用\n",
        "\n",
        "Query用于查询其他位置的信息，表示\"我想知道什么\"。\n",
        "\n",
        "#### 2.6.2 Key的作用\n",
        "\n",
        "Key用于匹配查询，表示\"我有什么信息\"。\n",
        "\n",
        "#### 2.6.3 Value的作用\n",
        "\n",
        "Value包含实际的信息，表示\"我的信息是什么\"。\n",
        "\n",
        "**类比理解**：\n",
        "- Query就像\"问题\"：我想知道什么？\n",
        "- Key就像\"标签\"：我有什么信息？\n",
        "- Value就像\"答案\"：我的信息是什么？\n",
        "\n",
        "## 3. 多头注意力详解\n",
        "\n",
        "### 3.1 什么是多头注意力\n",
        "\n",
        "**直观理解**：想象多头注意力就像让多个\"分析师\"同时分析文本，每个\"分析师\"关注不同的方面。\n",
        "\n",
        "### 3.2 为什么需要多头注意力\n",
        "\n",
        "1. **多角度关注**：从不同的角度关注输入序列，提高模型的表达能力\n",
        "2. **特征多样性**：不同的注意力头关注不同的特征，形成更丰富的特征表示\n",
        "3. **并行计算**：多个注意力头可以并行计算，提高计算效率\n",
        "\n",
        "### 3.3 多头注意力的数学推导详解\n",
        "\n",
        "#### 3.3.1 从基础数学开始\n",
        "\n",
        "**步骤1：理解多个注意力头**\n",
        "\n",
        "使用$h$个注意力头，每个注意力头有自己的Query、Key、Value权重矩阵：\n",
        "- $W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
        "- $W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
        "- $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$\n",
        "\n",
        "其中$d_k = d_v = d_{model} / h$。\n",
        "\n",
        "**步骤2：理解每个注意力头的计算**\n",
        "\n",
        "每个注意力头计算：\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "\n",
        "**步骤3：理解多头注意力的拼接**\n",
        "\n",
        "将多个注意力头的输出拼接：\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
        "\n",
        "其中$W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$是输出投影矩阵。\n",
        "\n",
        "#### 3.3.2 多头注意力的具体计算示例\n",
        "\n",
        "**示例：计算2个注意力头的多头注意力**\n",
        "\n",
        "假设：\n",
        "- 序列长度：$n = 2$\n",
        "- 模型维度：$d_{model} = 8$\n",
        "- 注意力头数：$h = 2$\n",
        "- 每个头的维度：$d_k = d_v = d_{model} / h = 4$\n",
        "\n",
        "**步骤1：计算每个注意力头**\n",
        "\n",
        "对于注意力头1：\n",
        "- $Q_1 = Q W_1^Q \\in \\mathbb{R}^{2 \\times 4}$\n",
        "- $K_1 = K W_1^K \\in \\mathbb{R}^{2 \\times 4}$\n",
        "- $V_1 = V W_1^V \\in \\mathbb{R}^{2 \\times 4}$\n",
        "- $\\text{head}_1 = \\text{Attention}(Q_1, K_1, V_1) \\in \\mathbb{R}^{2 \\times 4}$\n",
        "\n",
        "对于注意力头2：\n",
        "- $Q_2 = Q W_2^Q \\in \\mathbb{R}^{2 \\times 4}$\n",
        "- $K_2 = K W_2^K \\in \\mathbb{R}^{2 \\times 4}$\n",
        "- $V_2 = V W_2^V \\in \\mathbb{R}^{2 \\times 4}$\n",
        "- $\\text{head}_2 = \\text{Attention}(Q_2, K_2, V_2) \\in \\mathbb{R}^{2 \\times 4}$\n",
        "\n",
        "**步骤2：拼接注意力头**\n",
        "\n",
        "$$\\text{Concat}(\\text{head}_1, \\text{head}_2) \\in \\mathbb{R}^{2 \\times 8}$$\n",
        "\n",
        "**步骤3：输出投影**\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2) W^O \\in \\mathbb{R}^{2 \\times 8}$$\n",
        "\n",
        "### 3.4 多头注意力的可视化\n",
        "\n",
        "下面我们通过代码可视化多头注意力：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 多头注意力可视化（示例：4个词语，2个注意力头）\n",
        "# ============================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# 模拟一个句子：\"拿起 桌子 上 的 杯子\"\n",
        "sentence = ['拿起', '桌子', '上', '杯子']\n",
        "seq_len = len(sentence)\n",
        "d_model = 8\n",
        "num_heads = 2\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# 模拟词向量\n",
        "word_vectors = np.random.randn(seq_len, d_model) * 0.5\n",
        "\n",
        "# 模拟多头注意力的权重矩阵（简化：每个头使用不同的随机权重）\n",
        "def compute_attention_head(X, head_idx):\n",
        "    \"\"\"计算单个注意力头（简化版）\"\"\"\n",
        "    W_Q = np.random.RandomState(head_idx * 42).randn(d_model, d_k) * 0.1\n",
        "    W_K = np.random.RandomState(head_idx * 42 + 1).randn(d_model, d_k) * 0.1\n",
        "    W_V = np.random.RandomState(head_idx * 42 + 2).randn(d_model, d_k) * 0.1\n",
        "    \n",
        "    Q = X @ W_Q\n",
        "    K = X @ W_K\n",
        "    V = X @ W_V\n",
        "    \n",
        "    S = Q @ K.T / np.sqrt(d_k)\n",
        "    A = np.exp(S) / np.sum(np.exp(S), axis=1, keepdims=True)\n",
        "    \n",
        "    return A, Q @ K.T\n",
        "\n",
        "# 计算每个注意力头的注意力权重\n",
        "attention_weights = []\n",
        "attention_scores = []\n",
        "for head_idx in range(num_heads):\n",
        "    A, S = compute_attention_head(word_vectors, head_idx)\n",
        "    attention_weights.append(A)\n",
        "    attention_scores.append(S)\n",
        "\n",
        "# 可视化\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 左上：注意力头1的注意力权重\n",
        "im1 = axes[0, 0].imshow(attention_weights[0], cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
        "axes[0, 0].set_title('注意力头1的注意力权重', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Key位置（被关注的位置）')\n",
        "axes[0, 0].set_ylabel('Query位置（关注的位置）')\n",
        "axes[0, 0].set_xticks(range(seq_len))\n",
        "axes[0, 0].set_yticks(range(seq_len))\n",
        "axes[0, 0].set_xticklabels(sentence, rotation=45, ha='right')\n",
        "axes[0, 0].set_yticklabels(sentence)\n",
        "for i in range(seq_len):\n",
        "    for j in range(seq_len):\n",
        "        axes[0, 0].text(j, i, f'{attention_weights[0][i, j]:.2f}', ha='center', va='center', \n",
        "                       color='white' if attention_weights[0][i, j] < 0.5 else 'black', fontsize=9)\n",
        "plt.colorbar(im1, ax=axes[0, 0])\n",
        "\n",
        "# 右上：注意力头2的注意力权重\n",
        "im2 = axes[0, 1].imshow(attention_weights[1], cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
        "axes[0, 1].set_title('注意力头2的注意力权重', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Key位置（被关注的位置）')\n",
        "axes[0, 1].set_ylabel('Query位置（关注的位置）')\n",
        "axes[0, 1].set_xticks(range(seq_len))\n",
        "axes[0, 1].set_yticks(range(seq_len))\n",
        "axes[0, 1].set_xticklabels(sentence, rotation=45, ha='right')\n",
        "axes[0, 1].set_yticklabels(sentence)\n",
        "for i in range(seq_len):\n",
        "    for j in range(seq_len):\n",
        "        axes[0, 1].text(j, i, f'{attention_weights[1][i, j]:.2f}', ha='center', va='center', \n",
        "                       color='white' if attention_weights[1][i, j] < 0.5 else 'black', fontsize=9)\n",
        "plt.colorbar(im2, ax=axes[0, 1])\n",
        "\n",
        "# 左下：位置\"拿起\"在不同注意力头中的注意力权重\n",
        "x = np.arange(seq_len)\n",
        "width = 0.35\n",
        "axes[1, 0].bar(x - width/2, attention_weights[0][0, :], width, label='注意力头1', alpha=0.7)\n",
        "axes[1, 0].bar(x + width/2, attention_weights[1][0, :], width, label='注意力头2', alpha=0.7)\n",
        "axes[1, 0].set_title('位置\"拿起\"在不同注意力头中的注意力权重', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('位置')\n",
        "axes[1, 0].set_ylabel('注意力权重')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(sentence, rotation=45, ha='right')\n",
        "axes[1, 0].set_ylim(0, 1.1)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 右下：位置\"杯子\"在不同注意力头中的注意力权重\n",
        "axes[1, 1].bar(x - width/2, attention_weights[0][-1, :], width, label='注意力头1', alpha=0.7)\n",
        "axes[1, 1].bar(x + width/2, attention_weights[1][-1, :], width, label='注意力头2', alpha=0.7)\n",
        "axes[1, 1].set_title('位置\"杯子\"在不同注意力头中的注意力权重', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('位置')\n",
        "axes[1, 1].set_ylabel('注意力权重')\n",
        "axes[1, 1].set_xticks(x)\n",
        "axes[1, 1].set_xticklabels(sentence, rotation=45, ha='right')\n",
        "axes[1, 1].set_ylim(0, 1.1)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"多头注意力可视化说明：\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. 左上：注意力头1的注意力权重，展示第一个注意力头关注的内容\")\n",
        "print(\"2. 右上：注意力头2的注意力权重，展示第二个注意力头关注的内容\")\n",
        "print(\"3. 左下：位置\\\"拿起\\\"在不同注意力头中的注意力权重，展示不同头关注的不同方面\")\n",
        "print(\"4. 右下：位置\\\"杯子\\\"在不同注意力头中的注意力权重，展示不同头关注的不同方面\")\n",
        "print(\"5. 多头注意力从不同的角度关注输入序列，形成更丰富的特征表示\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transformer编码器层详解\n",
        "\n",
        "### 4.1 Transformer编码器层的组成\n",
        "\n",
        "Transformer编码器层由以下组件组成：\n",
        "\n",
        "1. **多头自注意力**：使用多头自注意力机制捕获序列中每个位置与其他位置之间的关系\n",
        "2. **残差连接和层归一化**：使用残差连接和层归一化加速训练，提高稳定性\n",
        "3. **前馈神经网络**：使用前馈神经网络处理注意力输出\n",
        "4. **残差连接和层归一化**：再次使用残差连接和层归一化\n",
        "\n",
        "### 4.2 前馈神经网络详解\n",
        "\n",
        "#### 4.2.1 什么是前馈神经网络\n",
        "\n",
        "前馈神经网络（Feed-Forward Network, FFN）是Transformer编码器层中的另一个重要组件，用于处理注意力输出。\n",
        "\n",
        "#### 4.2.2 前馈神经网络的数学表示\n",
        "\n",
        "前馈神经网络通常包含两个线性变换和一个激活函数：\n",
        "\n",
        "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
        "\n",
        "其中：\n",
        "- $W_1 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ 是第一个线性层的权重矩阵\n",
        "- $W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$ 是第二个线性层的权重矩阵\n",
        "- $d_{ff}$ 是前馈神经网络的隐藏层维度（通常为$4 \\times d_{model}$）\n",
        "\n",
        "#### 4.2.3 前馈神经网络的具体计算示例\n",
        "\n",
        "**示例：计算前馈神经网络的输出**\n",
        "\n",
        "假设：\n",
        "- 输入维度：$d_{model} = 8$\n",
        "- 隐藏层维度：$d_{ff} = 32$\n",
        "- 输入向量：$x = [0.1, 0.2, -0.1, 0.3, 0.0, -0.2, 0.1, 0.2]$\n",
        "\n",
        "**步骤1：第一个线性变换**\n",
        "\n",
        "$$h = xW_1 + b_1 = [0.1, 0.2, -0.1, 0.3, 0.0, -0.2, 0.1, 0.2] W_1 + b_1$$\n",
        "\n",
        "假设$W_1$和$b_1$已经训练好，得到：\n",
        "$$h = [0.5, 0.3, -0.2, 0.4, \\ldots] \\in \\mathbb{R}^{32}$$\n",
        "\n",
        "**步骤2：ReLU激活**\n",
        "\n",
        "$$h_{relu} = \\text{ReLU}(h) = [0.5, 0.3, 0.0, 0.4, \\ldots]$$\n",
        "\n",
        "（负值变为0）\n",
        "\n",
        "**步骤3：第二个线性变换**\n",
        "\n",
        "$$y = h_{relu} W_2 + b_2 \\in \\mathbb{R}^{8}$$\n",
        "\n",
        "### 4.3 残差连接和层归一化详解\n",
        "\n",
        "#### 4.3.1 残差连接\n",
        "\n",
        "残差连接将输入直接加到输出上：\n",
        "\n",
        "$$y = x + \\text{SubLayer}(x)$$\n",
        "\n",
        "其中$\\text{SubLayer}(x)$可以是多头自注意力或前馈神经网络。\n",
        "\n",
        "**残差连接的作用**：\n",
        "- 加速训练：使梯度能够直接传播\n",
        "- 提高稳定性：使深层网络更容易训练\n",
        "\n",
        "#### 4.3.2 层归一化\n",
        "\n",
        "层归一化对每个样本的特征进行归一化：\n",
        "\n",
        "$$\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
        "\n",
        "其中：\n",
        "- $\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i$ 是均值\n",
        "- $\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2$ 是方差\n",
        "- $\\gamma$ 和 $\\beta$ 是可学习的参数\n",
        "- $\\epsilon$ 是小常数（防止除零）\n",
        "\n",
        "### 4.4 Transformer编码器层的完整流程\n",
        "\n",
        "Transformer编码器层的完整流程：\n",
        "\n",
        "1. **输入**：$x \\in \\mathbb{R}^{n \\times d_{model}}$\n",
        "2. **多头自注意力**：$x' = \\text{MultiHead}(x, x, x)$\n",
        "3. **残差连接和层归一化**：$x'' = \\text{LayerNorm}(x + x')$\n",
        "4. **前馈神经网络**：$x''' = \\text{FFN}(x'')$\n",
        "5. **残差连接和层归一化**：$y = \\text{LayerNorm}(x'' + x''')$\n",
        "6. **输出**：$y \\in \\mathbb{R}^{n \\times d_{model}}$\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Transformer编码器在VLA中的应用\n",
        "\n",
        "### 5.1 语言指令编码\n",
        "\n",
        "在VLA中，Transformer编码器用于将语言指令编码为固定维度的特征向量，例如：\n",
        "- 将\"拿起桌子上的杯子\"编码为特征向量\n",
        "- 理解指令的意图、对象、位置等信息\n",
        "\n",
        "### 5.2 语义理解\n",
        "\n",
        "Transformer编码器的全局感受野使得VLA模型能够理解语言指令中的全局关系，例如：\n",
        "- 理解\"拿起\"和\"杯子\"之间的关系\n",
        "- 理解\"桌子\"和\"上\"之间的关系\n",
        "\n",
        "### 5.3 预训练方法\n",
        "\n",
        "Transformer编码器可以用于VLA的预训练，通过大规模文本数据预训练Transformer编码器（如BERT、GPT），然后在VLA任务上进行微调。\n",
        "\n",
        "### 5.4 在VLA中的具体应用示例\n",
        "\n",
        "**示例：理解\"拿起桌子上的杯子\"**\n",
        "\n",
        "如果语言指令是\"拿起桌子上的杯子\"，VLA模型需要：\n",
        "1. 使用Transformer编码器将每个词语转换为向量表示\n",
        "2. 通过自注意力机制理解词语之间的关系\n",
        "3. 理解指令的意图（\"拿起\"）、对象（\"杯子\"）、位置（\"桌子上\"）\n",
        "4. 结合视觉理解结果生成相应的动作序列\n",
        "\n",
        "---\n",
        "\n",
        "## 6. 总结\n",
        "\n",
        "### 6.1 Transformer编码器的核心思想\n",
        "\n",
        "1. **自注意力机制**：使用自注意力机制捕获序列中每个位置与其他位置之间的关系\n",
        "2. **多头注意力**：使用多个注意力头从不同的角度关注输入序列\n",
        "3. **位置编码**：使用位置编码保留序列的位置信息\n",
        "4. **残差连接和层归一化**：使用残差连接和层归一化加速训练，提高稳定性\n",
        "\n",
        "### 6.2 Transformer编码器的优势\n",
        "\n",
        "1. **全局感受野**：能够捕获序列中任意两个位置之间的关系\n",
        "2. **长距离依赖**：能够捕获长距离依赖关系，不受距离限制\n",
        "3. **并行计算**：能够并行计算所有位置之间的关系，提高计算效率\n",
        "4. **预训练模型**：BERT、GPT等预训练语言模型都基于Transformer编码器\n",
        "\n",
        "### 6.3 在VLA中的意义\n",
        "\n",
        "Transformer编码器是VLA语言理解的核心架构，它提供了强大的语言特征提取能力，使得VLA模型能够理解复杂的语言指令。Transformer编码器的全局感受野和长距离依赖捕获能力使得VLA模型能够更好地理解语言指令中的全局关系，从而生成更准确的动作序列。\n",
        "\n",
        "---\n",
        "\n",
        "**文档完成时间**：2025-01-27  \n",
        "**文档版本**：v1.0  \n",
        "**维护者**：AI助手\n",
        "\n",
        "**相关文档**：\n",
        "- 父目录：文本特征提取详解（见../文本特征提取详解.ipynb）\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
