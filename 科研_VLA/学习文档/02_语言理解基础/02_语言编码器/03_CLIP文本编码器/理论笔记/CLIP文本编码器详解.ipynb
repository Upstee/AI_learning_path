{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLIP文本编码器详解\n",
        "\n",
        "## 📋 文档说明\n",
        "\n",
        "本文档是CLIP（Contrastive Language-Image Pre-training）文本编码器的详细理论讲解，比父目录的《语言编码器详解》更加深入和详细。本文档将深入讲解CLIP文本编码器的原理、对比学习的数学推导和实现细节。通过本文档，你将能够：\n",
        "\n",
        "1. **深入理解CLIP文本编码器的原理**：从对比学习、视觉-语言对齐到InfoNCE损失的完整流程\n",
        "2. **掌握对比学习的数学原理**：理解对比学习的数学定义、为什么有效、如何实现视觉-语言对齐\n",
        "3. **理解CLIP的架构设计**：理解CLIP的双编码器架构、特征归一化、相似度计算等关键组件\n",
        "4. **掌握CLIP文本编码器在VLA中的应用**：理解CLIP文本编码器在VLA模型中的具体应用和优势\n",
        "5. **了解CLIP的变体和改进**：了解ALIGN、BLIP等CLIP的变体和改进\n",
        "\n",
        "**学习方式**：本文件是Jupyter Notebook格式，你可以边看边运行代码，通过可视化图表和数学推导更好地理解CLIP文本编码器的原理和过程。\n",
        "\n",
        "**文档结构**：\n",
        "- 父目录：语言编码器详解（见../语言编码器详解.ipynb）\n",
        "- 本文档：CLIP文本编码器详解（本文档）\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 术语表（按出现顺序）\n",
        "\n",
        "### 1. CLIP文本编码器 (CLIP Text Encoder)\n",
        "- **中文名称**：CLIP文本编码器\n",
        "- **英文全称**：Contrastive Language-Image Pre-training Text Encoder\n",
        "- **定义**：CLIP文本编码器是指CLIP（Contrastive Language-Image Pre-training）模型中的文本编码器部分，通过对比学习实现视觉-语言对齐，将图像和文本映射到同一个嵌入空间。CLIP文本编码器的优势在于：1）多模态对齐：通过对比学习实现视觉-语言对齐，使视觉特征和语言特征在同一个嵌入空间中；2）预训练模型：使用在大规模图像-文本对数据上预训练的模型，具有强大的特征提取能力；3）零样本能力：能够处理训练时未见过的图像和文本，具有零样本学习能力；4）通用性：可以用于各种视觉-语言任务，包括VLA任务。CLIP文本编码器是VLA中的重要方法，许多VLA模型都使用CLIP作为语言编码器，以实现视觉-语言对齐。CLIP通过在大规模图像-文本对数据上进行对比学习，学习视觉和语言之间的对应关系，使得视觉特征和语言特征在同一个嵌入空间中具有相似的语义，这样就能够进行跨模态的匹配和理解。\n",
        "- **核心组成**：CLIP文本编码器的核心组成包括：1）文本编码器：使用Transformer作为文本编码器，从文本中提取文本特征；2）视觉编码器：使用ResNet或ViT作为视觉编码器，从图像中提取视觉特征；3）对比学习：通过对比学习训练文本编码器和视觉编码器，使图像和文本在同一个嵌入空间中；4）特征归一化：对文本特征和视觉特征进行归一化，便于对比学习；5）相似度计算：计算文本和图像之间的相似度，用于对比学习；6）损失函数：使用对比损失函数（如InfoNCE损失）训练模型。CLIP文本编码器通常使用预训练的权重初始化，然后在VLA任务上进行微调。CLIP的双编码器架构使得视觉和语言特征能够独立提取，然后通过对比学习实现对齐，这种设计使得CLIP能够处理各种视觉-语言任务。\n",
        "- **在VLA中的应用**：在VLA中，CLIP文本编码器是实现视觉-语言对齐的重要方法。VLA模型使用CLIP文本编码器从输入文本中提取语言特征，这些特征已经与视觉特征对齐，可以直接用于多模态融合。CLIP文本编码器的优势在于能够实现视觉-语言对齐，这对于VLA任务非常重要，因为VLA需要理解视觉场景和语言指令之间的关系。在VLA训练过程中，CLIP文本编码器通常使用预训练的权重初始化，然后在VLA任务上进行微调，以提高特征提取的质量和效率。CLIP文本编码器的输出特征将与视觉编码器的输出特征进行融合，生成多模态表示，最终用于动作生成。CLIP的零样本能力使得VLA模型能够处理训练时未见过的视觉场景和语言指令，这对于VLA的泛化能力非常重要。\n",
        "- **相关概念**：对比学习、视觉-语言对齐、多模态融合、CLIP、嵌入空间、InfoNCE损失、零样本学习\n",
        "- **首次出现位置**：本文档标题\n",
        "- **深入学习**：参考父目录的[语言编码器详解](../语言编码器详解.ipynb)和[CLIP视觉编码器详解](../../../01_视觉理解基础/02_视觉编码器/03_CLIP视觉编码器/理论笔记/CLIP视觉编码器详解.ipynb)\n",
        "- **直观理解**：想象CLIP文本编码器就像一位\"翻译官\"，它能够将图像和文本\"翻译\"成同一种\"语言\"（统一的特征表示），使它们能够相互理解。CLIP通过对比学习训练文本编码器和视觉编码器，使图像和文本在同一个嵌入空间中，这样就能够实现视觉-语言对齐。在VLA中，CLIP文本编码器帮助模型理解视觉场景和语言指令之间的关系，从而生成相应的动作。\n",
        "\n",
        "### 2. 对比学习 (Contrastive Learning)\n",
        "- **中文名称**：对比学习\n",
        "- **英文全称**：Contrastive Learning\n",
        "- **定义**：对比学习是一种自监督学习方法，通过比较正样本对和负样本对来学习特征表示。在CLIP中，对比学习用于学习视觉-语言对齐，通过最大化匹配的图像-文本对的相似度，最小化不匹配的图像-文本对的相似度，使图像和文本在同一个嵌入空间中。对比学习的优势在于：1）不需要显式标注：只需要图像-文本对，不需要详细的标注；2）学习通用特征：能够学习通用的特征表示，适用于各种任务；3）零样本能力：能够处理训练时未见过的数据，具有零样本学习能力；4）可扩展性：通过增加数据量，能够持续提升性能。对比学习是CLIP的核心，也是现代多模态学习的重要方法。\n",
        "- **核心组成**：对比学习的核心组成包括：1）正样本对：匹配的图像-文本对，应该具有高相似度；2）负样本对：不匹配的图像-文本对，应该具有低相似度；3）相似度计算：计算图像和文本之间的相似度，通常使用余弦相似度或点积；4）对比损失函数：使用对比损失函数（如InfoNCE损失）最大化正样本对的相似度，最小化负样本对的相似度；5）温度参数：使用温度参数控制softmax的锐度，影响对比学习的效果；6）批次采样：使用大批次采样，增加负样本的数量，提高对比学习的效果。对比学习通过比较正样本对和负样本对，学习特征表示，使得相似的数据在嵌入空间中靠近，不相似的数据在嵌入空间中远离。\n",
        "- **在VLA中的应用**：在VLA中，对比学习用于CLIP文本编码器中，实现视觉-语言对齐。VLA模型使用对比学习将视觉特征和语言特征对齐到同一个嵌入空间，这样就能够理解视觉场景和语言指令之间的关系。对比学习的优势在于能够学习通用的特征表示，适用于各种VLA任务，并且具有零样本学习能力，能够处理训练时未见过的视觉场景和语言指令。在VLA训练过程中，对比学习通常是端到端训练的，即与视觉编码器、语言编码器、多模态融合模块一起训练，以学习最适合VLA任务的对齐方法。\n",
        "- **相关概念**：视觉-语言对齐、InfoNCE损失、嵌入空间、正样本对、负样本对、温度参数\n",
        "- **首次出现位置**：本文档第1.2节\n",
        "- **深入学习**：参考本文档的对比学习详细讲解部分和[CLIP视觉编码器详解](../../../01_视觉理解基础/02_视觉编码器/03_CLIP视觉编码器/理论笔记/CLIP视觉编码器详解.ipynb)\n",
        "- **直观理解**：想象对比学习就像让模型学习\"配对游戏\"，匹配的图像-文本对应该\"靠近\"，不匹配的应该\"远离\"。通过大量的\"配对游戏\"，模型学会了如何将图像和文本映射到同一个空间，使得相似的图像和文本在空间中靠近，不相似的远离。在VLA中，对比学习帮助模型理解视觉场景和语言指令之间的关系，从而生成相应的动作。\n",
        "\n",
        "### 3. InfoNCE损失 (InfoNCE Loss)\n",
        "- **中文名称**：InfoNCE损失\n",
        "- **英文全称**：Information Noise Contrastive Estimation Loss\n",
        "- **定义**：InfoNCE损失是对比学习中常用的损失函数，通过最大化正样本对的相似度，最小化负样本对的相似度，学习特征表示。InfoNCE损失的数学表示为：$\\mathcal{L} = -\\log \\frac{\\exp(s(I, T) / \\tau)}{\\sum_{T' \\in \\mathcal{T}} \\exp(s(I, T') / \\tau)}$，其中 $s(I, T)$ 是图像 $I$ 和文本 $T$ 之间的相似度，$\\tau$ 是温度参数，$\\mathcal{T}$ 是批次中的所有文本。InfoNCE损失的优势在于：1）简单有效：损失函数简单，易于优化；2）批次内对比：利用批次内的其他样本作为负样本，不需要额外的负样本；3）温度参数：通过温度参数控制softmax的锐度，影响对比学习的效果；4）可扩展性：通过增加批次大小，能够增加负样本的数量，提高对比学习的效果。InfoNCE损失是CLIP的核心损失函数，也是现代对比学习的重要方法。\n",
        "- **核心组成**：InfoNCE损失的核心组成包括：1）相似度计算：计算图像和文本之间的相似度，通常使用余弦相似度或点积；2）温度参数：使用温度参数 $\\tau$ 控制softmax的锐度，温度参数越小，softmax越锐利，对比学习的效果越好；3）正样本对：匹配的图像-文本对，应该具有高相似度；4）负样本对：批次内的其他图像-文本对，作为负样本；5）softmax归一化：对相似度进行softmax归一化，得到概率分布；6）负对数似然：使用负对数似然作为损失函数，最大化正样本对的概率。InfoNCE损失通过最大化正样本对的相似度，最小化负样本对的相似度，学习特征表示，使得相似的数据在嵌入空间中靠近，不相似的数据在嵌入空间中远离。\n",
        "- **在VLA中的应用**：在VLA中，InfoNCE损失用于CLIP文本编码器中，实现视觉-语言对齐。VLA模型使用InfoNCE损失将视觉特征和语言特征对齐到同一个嵌入空间，这样就能够理解视觉场景和语言指令之间的关系。InfoNCE损失的优势在于能够学习通用的特征表示，适用于各种VLA任务，并且具有零样本学习能力，能够处理训练时未见过的视觉场景和语言指令。在VLA训练过程中，InfoNCE损失通常是端到端训练的，即与视觉编码器、语言编码器、多模态融合模块一起训练，以学习最适合VLA任务的对齐方法。\n",
        "- **相关概念**：对比学习、视觉-语言对齐、嵌入空间、温度参数、余弦相似度、负样本对\n",
        "- **首次出现位置**：本文档第1.3节\n",
        "- **深入学习**：参考本文档的InfoNCE损失详细讲解部分和[CLIP视觉编码器详解](../../../01_视觉理解基础/02_视觉编码器/03_CLIP视觉编码器/理论笔记/CLIP视觉编码器详解.ipynb)\n",
        "- **直观理解**：想象InfoNCE损失就像让模型学习\"找朋友\"的游戏，匹配的图像-文本对是\"好朋友\"，应该\"靠近\"，不匹配的是\"陌生人\"，应该\"远离\"。通过大量的\"找朋友\"游戏，模型学会了如何将图像和文本映射到同一个空间，使得\"好朋友\"在空间中靠近，\"陌生人\"远离。在VLA中，InfoNCE损失帮助模型理解视觉场景和语言指令之间的关系，从而生成相应的动作。\n",
        "\n",
        "### 4. 视觉-语言对齐 (Vision-Language Alignment)\n",
        "- **中文名称**：视觉-语言对齐\n",
        "- **英文全称**：Vision-Language Alignment\n",
        "- **定义**：视觉-语言对齐是指将视觉特征和语言特征对齐到同一个嵌入空间的方法，是多模态对齐的核心。视觉-语言对齐的目标是使视觉特征和语言特征在同一个嵌入空间中具有相似的语义，这样就能够进行跨模态的匹配和理解。视觉-语言对齐的方法包括对比学习（使用对比损失函数学习对齐）、预训练（使用大规模图像-文本对数据预训练）、联合嵌入（将视觉和语言特征映射到同一个嵌入空间）等。视觉-语言对齐的质量直接影响VLA模型的性能，好的对齐能够帮助模型更好地理解视觉场景和语言指令之间的关系，提高动作预测的准确性。在VLA中，视觉-语言对齐通常使用CLIP、ALIGN等预训练模型实现，这些模型能够学习视觉和语言之间的对应关系。\n",
        "- **核心组成**：视觉-语言对齐的核心组成包括：1）文本特征提取：使用文本编码器从文本中提取文本特征；2）视觉特征提取：使用视觉编码器从图像中提取视觉特征；3）特征编码：将文本特征和视觉特征编码为相同维度的向量；4）对齐学习：使用对比学习、预训练等方法学习文本和视觉之间的对应关系；5）嵌入空间：将文本特征和视觉特征映射到同一个嵌入空间；6）对齐评估：评估对齐的质量，如使用图像-文本检索准确率等指标。视觉-语言对齐通常使用预训练的方法，如CLIP、ALIGN等，这些方法能够学习文本和视觉之间的对应关系。\n",
        "- **在VLA中的应用**：在VLA中，视觉-语言对齐是将文本特征和视觉特征对齐到同一个嵌入空间的关键。VLA模型使用视觉-语言对齐将文本编码器提取的文本特征和视觉编码器提取的视觉特征对齐到同一个嵌入空间，这样就能够理解视觉场景和语言指令之间的关系。例如，如果语言指令是\"拿起桌子上的杯子\"，VLA模型需要先通过视觉理解识别\"桌子\"和\"杯子\"的位置，通过语言理解理解指令的意图，然后通过视觉-语言对齐将文本特征（\"拿起\"、\"桌子上\"、\"杯子\"）和视觉特征（\"桌子\"和\"杯子\"的位置）对齐到同一个嵌入空间，这样就能够理解视觉场景和语言指令之间的关系。在VLA训练过程中，视觉-语言对齐通常是端到端训练的，即与视觉编码器、文本编码器、多模态融合模块一起训练，以学习最适合VLA任务的对齐方法。\n",
        "- **相关概念**：对比学习、CLIP、嵌入空间、多模态融合、跨模态匹配、零样本学习\n",
        "- **首次出现位置**：本文档第1.1节\n",
        "- **深入学习**：参考本文档的视觉-语言对齐详细讲解部分和[多模态融合基础详解](../../../04_多模态融合基础/理论笔记/多模态融合基础详解.ipynb)\n",
        "- **直观理解**：想象视觉-语言对齐就像将图像和文本\"翻译\"成同一种\"语言\"（统一的特征表示），使它们能够相互理解。通过对比学习，模型学会了如何将图像和文本映射到同一个空间，使得相似的图像和文本在空间中靠近，不相似的远离。在VLA中，视觉-语言对齐帮助模型理解视觉场景和语言指令之间的关系，从而生成相应的动作。\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 概述\n",
        "\n",
        "### 什么是CLIP文本编码器\n",
        "\n",
        "CLIP文本编码器是CLIP模型中的文本编码器部分，通过对比学习实现视觉-语言对齐，将图像和文本映射到同一个嵌入空间。\n",
        "\n",
        "### 为什么重要\n",
        "\n",
        "CLIP文本编码器对于VLA学习非常重要，原因包括：\n",
        "\n",
        "1. **视觉-语言对齐**：CLIP能够实现视觉-语言对齐，这对于VLA任务非常重要\n",
        "2. **零样本能力**：CLIP具有零样本学习能力，能够处理训练时未见过的数据\n",
        "3. **预训练模型**：可以使用在大规模数据上预训练的CLIP模型，提高特征提取质量\n",
        "4. **通用性**：CLIP可以用于各种视觉-语言任务，包括VLA任务\n",
        "\n",
        "### 学习目标\n",
        "\n",
        "通过本文档的学习，你将能够：\n",
        "\n",
        "1. **深入理解对比学习**：理解对比学习的数学原理和为什么有效\n",
        "2. **理解InfoNCE损失**：理解InfoNCE损失的数学定义和计算方法\n",
        "3. **掌握CLIP文本编码器的架构**：理解CLIP的双编码器架构和特征归一化\n",
        "4. **掌握CLIP文本编码器在VLA中的应用**：理解CLIP文本编码器在VLA模型中的具体应用\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 导入必要的库\n",
        "# ============================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Microsoft YaHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 设置图表样式\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "print(\"环境准备完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. CLIP文本编码器的架构详解\n",
        "\n",
        "### 1.1 什么是CLIP文本编码器\n",
        "\n",
        "CLIP文本编码器是CLIP模型中的文本编码器部分，使用Transformer架构从文本中提取文本特征，然后通过对比学习与视觉编码器实现视觉-语言对齐。\n",
        "\n",
        "### 1.2 CLIP的双编码器架构\n",
        "\n",
        "CLIP采用双编码器架构：\n",
        "1. **文本编码器**：使用Transformer从文本中提取文本特征\n",
        "2. **视觉编码器**：使用ResNet或ViT从图像中提取视觉特征\n",
        "3. **对比学习**：通过对比学习使文本特征和视觉特征在同一个嵌入空间中\n",
        "\n",
        "### 1.3 CLIP文本编码器的架构\n",
        "\n",
        "CLIP文本编码器使用Transformer架构，包括：\n",
        "\n",
        "1. **词嵌入层**：将词语转换为词向量\n",
        "2. **位置编码**：为每个位置添加位置信息\n",
        "3. **Transformer编码器层**：多层Transformer编码器\n",
        "4. **特征归一化**：对文本特征进行归一化，便于对比学习\n",
        "\n",
        "**数学表示**：\n",
        "\n",
        "对于输入文本 $T$，CLIP文本编码器输出文本特征：\n",
        "\n",
        "$$f_T = \\text{CLIP\\_TextEncoder}(T) \\in \\mathbb{R}^{d}$$\n",
        "\n",
        "其中 $d$ 是特征维度（通常为512或768）。\n",
        "\n",
        "### 1.4 特征归一化\n",
        "\n",
        "CLIP对文本特征和视觉特征进行归一化：\n",
        "\n",
        "$$f_T^{norm} = \\frac{f_T}{\\|f_T\\|}$$\n",
        "\n",
        "$$f_I^{norm} = \\frac{f_I}{\\|f_I\\|}$$\n",
        "\n",
        "归一化后的特征用于计算余弦相似度。\n",
        "\n",
        "### 1.5 CLIP文本编码器架构的可视化\n",
        "\n",
        "下面我们通过代码可视化CLIP文本编码器的架构：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CLIP文本编码器架构的可视化\n",
        "# ============================================\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 14))\n",
        "ax.axis('off')\n",
        "ax.set_xlim([0, 10])\n",
        "ax.set_ylim([0, 16])\n",
        "ax.set_title('CLIP文本编码器架构示意图', fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "# 颜色定义\n",
        "colors = {\n",
        "    'embedding': 'lightblue',\n",
        "    'encoder': 'lightgreen',\n",
        "    'attention': 'lightcoral',\n",
        "    'ffn': 'lightyellow',\n",
        "    'normalize': 'wheat',\n",
        "    'output': 'lightpink'\n",
        "}\n",
        "\n",
        "# 1. 输入层\n",
        "y_pos = 15\n",
        "ax.text(5, y_pos + 0.3, '输入文本', fontsize=12, fontweight='bold', ha='center')\n",
        "input_box = FancyBboxPatch((3, y_pos - 0.3), 4, 0.6,\n",
        "                           boxstyle=\"round,pad=0.05\", facecolor='lightgray', edgecolor='black', linewidth=2)\n",
        "ax.add_patch(input_box)\n",
        "ax.text(5, y_pos, '拿起桌子上的杯子', fontsize=10, ha='center', va='center')\n",
        "\n",
        "# 2. 嵌入层\n",
        "y_pos = 13\n",
        "ax.text(5, y_pos + 0.5, '嵌入层', fontsize=12, fontweight='bold', ha='center')\n",
        "embed_box = FancyBboxPatch((3, y_pos - 0.4), 4, 0.8,\n",
        "                            boxstyle=\"round,pad=0.05\", facecolor=colors['embedding'], edgecolor='black', linewidth=2)\n",
        "ax.add_patch(embed_box)\n",
        "ax.text(5, y_pos + 0.1, '词嵌入 + 位置编码', fontsize=9, ha='center', va='center')\n",
        "\n",
        "# 箭头\n",
        "arrow1 = FancyArrowPatch((5, y_pos - 0.4), (5, y_pos + 0.4), \n",
        "                         arrowstyle='<->', mutation_scale=20, color='black', linewidth=1.5)\n",
        "ax.add_patch(arrow1)\n",
        "\n",
        "# 3. Transformer编码器层（多层）\n",
        "num_layers = 3  # 简化显示，只显示3层\n",
        "layer_height = 1.2\n",
        "y_start = 11\n",
        "\n",
        "for i in range(num_layers):\n",
        "    y_pos = y_start - i * layer_height\n",
        "    \n",
        "    # 编码器层框\n",
        "    encoder_box = FancyBboxPatch((2, y_pos - 0.5), 6, 1.0,\n",
        "                                 boxstyle=\"round,pad=0.05\", facecolor=colors['encoder'], \n",
        "                                 edgecolor='black', linewidth=1.5)\n",
        "    ax.add_patch(encoder_box)\n",
        "    ax.text(5, y_pos + 0.2, f'Transformer编码器层 {i+1}', fontsize=10, fontweight='bold', ha='center', va='center')\n",
        "    \n",
        "    # 多头自注意力\n",
        "    attn_box = FancyBboxPatch((2.2, y_pos - 0.1), 2.6, 0.4,\n",
        "                              boxstyle=\"round,pad=0.02\", facecolor=colors['attention'], \n",
        "                              edgecolor='black', linewidth=1)\n",
        "    ax.add_patch(attn_box)\n",
        "    ax.text(3.5, y_pos + 0.1, '多头自注意力', fontsize=8, ha='center', va='center')\n",
        "    \n",
        "    # 前馈神经网络\n",
        "    ffn_box = FancyBboxPatch((5.2, y_pos - 0.1), 2.6, 0.4,\n",
        "                             boxstyle=\"round,pad=0.02\", facecolor=colors['ffn'], \n",
        "                             edgecolor='black', linewidth=1)\n",
        "    ax.add_patch(ffn_box)\n",
        "    ax.text(6.5, y_pos + 0.1, '前馈神经网络', fontsize=8, ha='center', va='center')\n",
        "    \n",
        "    # 残差连接和层归一化（用虚线表示）\n",
        "    if i < num_layers - 1:\n",
        "        arrow = FancyArrowPatch((5, y_pos - 0.5), (5, y_pos - 0.7), \n",
        "                               arrowstyle='->', mutation_scale=15, color='gray', \n",
        "                               linewidth=1, linestyle='--', alpha=0.5)\n",
        "        ax.add_patch(arrow)\n",
        "\n",
        "# 4. 特征归一化\n",
        "y_pos = 6.5\n",
        "ax.text(5, y_pos + 0.3, '特征归一化', fontsize=12, fontweight='bold', ha='center')\n",
        "normalize_box = FancyBboxPatch((3, y_pos - 0.3), 4, 0.6,\n",
        "                             boxstyle=\"round,pad=0.05\", facecolor=colors['normalize'], edgecolor='black', linewidth=2)\n",
        "ax.add_patch(normalize_box)\n",
        "ax.text(5, y_pos, 'L2归一化', fontsize=10, ha='center', va='center')\n",
        "\n",
        "# 箭头\n",
        "arrow2 = FancyArrowPatch((5, y_pos - 0.3), (5, y_pos + 0.3), \n",
        "                         arrowstyle='<->', mutation_scale=20, color='black', linewidth=1.5)\n",
        "ax.add_patch(arrow2)\n",
        "\n",
        "# 5. 输出层\n",
        "y_pos = 5\n",
        "ax.text(5, y_pos + 0.3, '输出层', fontsize=12, fontweight='bold', ha='center')\n",
        "output_box = FancyBboxPatch((3, y_pos - 0.3), 4, 0.6,\n",
        "                             boxstyle=\"round,pad=0.05\", facecolor=colors['output'], edgecolor='black', linewidth=2)\n",
        "ax.add_patch(output_box)\n",
        "ax.text(5, y_pos, '文本特征向量', fontsize=10, ha='center', va='center')\n",
        "\n",
        "# 箭头\n",
        "arrow3 = FancyArrowPatch((5, y_pos - 0.3), (5, y_pos + 0.3), \n",
        "                         arrowstyle='<->', mutation_scale=20, color='black', linewidth=1.5)\n",
        "ax.add_patch(arrow3)\n",
        "\n",
        "# 6. 对比学习\n",
        "ax.text(8.5, 13, '对比学习', fontsize=11, fontweight='bold', ha='center')\n",
        "contrast_box = FancyBboxPatch((7.5, 11.5), 2, 1.5,\n",
        "                         boxstyle=\"round,pad=0.05\", facecolor='lightcyan', edgecolor='black', linewidth=1.5)\n",
        "ax.add_patch(contrast_box)\n",
        "ax.text(8.5, 12.5, '视觉-语言对齐', fontsize=9, ha='center', va='center', fontweight='bold')\n",
        "ax.text(8.5, 12, 'InfoNCE损失', fontsize=9, ha='center', va='center')\n",
        "ax.text(8.5, 11.5, '余弦相似度', fontsize=9, ha='center', va='center')\n",
        "\n",
        "# 连接线（从输出层到对比学习）\n",
        "arrow4 = FancyArrowPatch((7, 5), (7.5, 12), \n",
        "                         arrowstyle='->', mutation_scale=15, color='red', linewidth=1.5, alpha=0.7)\n",
        "ax.add_patch(arrow4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCLIP文本编码器架构可视化完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 对比学习可视化（示例：4个图像-文本对）\n",
        "# ============================================\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "N = 4  # 批次大小\n",
        "D = 8  # 特征维度\n",
        "tau = 0.07  # 温度参数\n",
        "\n",
        "# 模拟视觉特征和文本特征（归一化后）\n",
        "visual_features = np.random.randn(N, D)\n",
        "visual_features = visual_features / np.linalg.norm(visual_features, axis=1, keepdims=True)\n",
        "\n",
        "text_features = np.random.randn(N, D)\n",
        "text_features = text_features / np.linalg.norm(text_features, axis=1, keepdims=True)\n",
        "\n",
        "# 计算相似度矩阵\n",
        "similarity_matrix = visual_features @ text_features.T\n",
        "\n",
        "# 计算InfoNCE损失（图像到文本）\n",
        "def compute_infonce_loss(similarity_matrix, tau):\n",
        "    N = similarity_matrix.shape[0]\n",
        "    losses = []\n",
        "    for i in range(N):\n",
        "        # 对第i行进行softmax\n",
        "        logits = similarity_matrix[i, :] / tau\n",
        "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "        # 正样本对的概率（对角线元素）\n",
        "        pos_prob = probs[i]\n",
        "        loss = -np.log(pos_prob + 1e-8)\n",
        "        losses.append(loss)\n",
        "    return np.array(losses)\n",
        "\n",
        "losses_img = compute_infonce_loss(similarity_matrix, tau)\n",
        "losses_txt = compute_infonce_loss(similarity_matrix.T, tau)\n",
        "total_loss = (losses_img.mean() + losses_txt.mean()) / 2\n",
        "\n",
        "# 可视化\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# 左图：相似度矩阵\n",
        "im1 = axes[0].imshow(similarity_matrix, cmap='viridis', vmin=-1, vmax=1)\n",
        "axes[0].set_title('相似度矩阵 (4x4)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('文本索引')\n",
        "axes[0].set_ylabel('图像索引')\n",
        "for i in range(N):\n",
        "    for j in range(N):\n",
        "        color = 'white' if similarity_matrix[i, j] < 0 else 'black'\n",
        "        axes[0].text(j, i, f'{similarity_matrix[i, j]:.2f}', \n",
        "                    ha='center', va='center', color=color, fontsize=10)\n",
        "        if i == j:\n",
        "            # 标记正样本对\n",
        "            rect = plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=False, \n",
        "                               edgecolor='red', linewidth=3)\n",
        "            axes[0].add_patch(rect)\n",
        "plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "# 中图：特征空间可视化（PCA到2D）\n",
        "pca = PCA(n_components=2)\n",
        "all_features = np.vstack([visual_features, text_features])\n",
        "all_features_2d = pca.fit_transform(all_features)\n",
        "\n",
        "visual_2d = all_features_2d[:N]\n",
        "text_2d = all_features_2d[N:]\n",
        "\n",
        "axes[1].scatter(visual_2d[:, 0], visual_2d[:, 1], c=['r','g','b','y'], \n",
        "               s=150, marker='o', label='视觉特征', edgecolors='black', linewidths=2)\n",
        "axes[1].scatter(text_2d[:, 0], text_2d[:, 1], c=['r','g','b','y'], \n",
        "               s=150, marker='s', label='文本特征', edgecolors='black', linewidths=2)\n",
        "\n",
        "# 连接正样本对\n",
        "for i in range(N):\n",
        "    axes[1].plot([visual_2d[i, 0], text_2d[i, 0]], \n",
        "                [visual_2d[i, 1], text_2d[i, 1]], \n",
        "                'r--', alpha=0.5, linewidth=2)\n",
        "\n",
        "axes[1].set_title('特征空间可视化 (PCA到2D)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('PC1')\n",
        "axes[1].set_ylabel('PC2')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# 右图：InfoNCE损失\n",
        "axes[2].bar(range(N), losses_img, color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "axes[2].set_title(f'InfoNCE损失（图像到文本）\\\\n总损失: {total_loss:.4f}', fontsize=12, fontweight='bold')\n",
        "axes[2].set_xlabel('图像索引')\n",
        "axes[2].set_ylabel('损失值')\n",
        "axes[2].set_xticks(range(N))\n",
        "axes[2].grid(True, alpha=0.3, axis='y')\n",
        "for i, loss in enumerate(losses_img):\n",
        "    axes[2].text(i, loss + 0.01, f'{loss:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('对比学习可视化', fontsize=14, fontweight='bold', y=1.05)\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"对比学习可视化说明：\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. 左图：相似度矩阵，对角线元素（红色框）是正样本对的相似度\")\n",
        "print(\"2. 中图：特征空间可视化，红色虚线连接正样本对\")\n",
        "print(\"3. 右图：InfoNCE损失，每个图像对应一个损失值\")\n",
        "print(\"4. 对比学习的目标是最大化正样本对的相似度，最小化负样本对的相似度\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. InfoNCE损失详解\n",
        "\n",
        "### 3.1 什么是InfoNCE损失\n",
        "\n",
        "**直观理解**：想象InfoNCE损失就像让模型学习\"找朋友\"的游戏，匹配的图像-文本对是\"好朋友\"，应该\"靠近\"，不匹配的是\"陌生人\"，应该\"远离\"。\n",
        "\n",
        "**定义**：InfoNCE损失是对比学习中常用的损失函数，通过最大化正样本对的相似度，最小化负样本对的相似度，学习特征表示。\n",
        "\n",
        "### 3.2 为什么需要InfoNCE损失\n",
        "\n",
        "InfoNCE损失的优势在于：\n",
        "\n",
        "1. **简单有效**：损失函数简单，易于优化\n",
        "2. **批次内对比**：利用批次内的其他样本作为负样本，不需要额外的负样本\n",
        "3. **温度参数**：通过温度参数控制softmax的锐度，影响对比学习的效果\n",
        "4. **可扩展性**：通过增加批次大小，能够增加负样本的数量，提高对比学习的效果\n",
        "\n",
        "### 3.3 InfoNCE损失的数学推导详解\n",
        "\n",
        "#### 3.3.1 从基础数学开始\n",
        "\n",
        "**步骤1：理解softmax函数**\n",
        "\n",
        "softmax函数将一组数值转换为概率分布：\n",
        "\n",
        "$$\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_{j=1}^{N} \\exp(x_j)}$$\n",
        "\n",
        "**步骤2：理解温度参数**\n",
        "\n",
        "温度参数 $\\tau$ 控制softmax的锐度：\n",
        "\n",
        "$$\\text{softmax}(x_i / \\tau) = \\frac{\\exp(x_i / \\tau)}{\\sum_{j=1}^{N} \\exp(x_j / \\tau)}$$\n",
        "\n",
        "温度参数越小，softmax越锐利，对比学习的效果越好。\n",
        "\n",
        "**步骤3：理解InfoNCE损失**\n",
        "\n",
        "对于图像 $I_i$ 和文本 $T_j$，InfoNCE损失为：\n",
        "\n",
        "$$\\mathcal{L}_{I_i} = -\\log \\frac{\\exp(s(I_i, T_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(s(I_i, T_j) / \\tau)}$$\n",
        "\n",
        "其中：\n",
        "- $s(I_i, T_j)$ 是图像 $I_i$ 和文本 $T_j$ 之间的相似度\n",
        "- $\\tau$ 是温度参数（通常为0.07）\n",
        "- $N$ 是批次大小\n",
        "\n",
        "#### 3.3.2 InfoNCE损失的具体计算示例\n",
        "\n",
        "**示例：计算图像1的InfoNCE损失**\n",
        "\n",
        "假设：\n",
        "- 批次大小：$N = 4$\n",
        "- 温度参数：$\\tau = 0.07$\n",
        "- 相似度：$s(I_1, T_1) = 0.8$（正样本对），$s(I_1, T_2) = 0.2$，$s(I_1, T_3) = 0.1$，$s(I_1, T_4) = 0.3$\n",
        "\n",
        "**步骤1：计算logits**\n",
        "\n",
        "$$\\text{logits} = [s(I_1, T_1) / \\tau, s(I_1, T_2) / \\tau, s(I_1, T_3) / \\tau, s(I_1, T_4) / \\tau] = [11.43, 2.86, 1.43, 4.29]$$\n",
        "\n",
        "**步骤2：计算softmax概率**\n",
        "\n",
        "$$P(T_j | I_1) = \\frac{\\exp(s(I_1, T_j) / \\tau)}{\\sum_{k=1}^{4} \\exp(s(I_1, T_k) / \\tau)}$$\n",
        "\n",
        "$$P(T_1 | I_1) = \\frac{\\exp(11.43)}{\\exp(11.43) + \\exp(2.86) + \\exp(1.43) + \\exp(4.29)} \\approx 0.999$$\n",
        "\n",
        "**步骤3：计算InfoNCE损失**\n",
        "\n",
        "$$\\mathcal{L}_{I_1} = -\\log P(T_1 | I_1) = -\\log(0.999) \\approx 0.001$$\n",
        "\n",
        "### 3.4 InfoNCE损失的可视化\n",
        "\n",
        "下面我们通过代码可视化InfoNCE损失：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# InfoNCE损失可视化（温度参数的影响）\n",
        "# ============================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# 模拟相似度\n",
        "pos_sim = 0.8  # 正样本对的相似度\n",
        "neg_sims = np.array([0.2, 0.1, 0.3, 0.15, 0.25])  # 负样本对的相似度\n",
        "all_sims = np.concatenate([[pos_sim], neg_sims])\n",
        "\n",
        "# 不同的温度参数\n",
        "taus = [0.01, 0.07, 0.1, 0.5, 1.0]\n",
        "\n",
        "# 计算不同温度参数下的InfoNCE损失\n",
        "losses = []\n",
        "probs = []\n",
        "\n",
        "for tau in taus:\n",
        "    logits = all_sims / tau\n",
        "    exp_logits = np.exp(logits)\n",
        "    prob = exp_logits[0] / np.sum(exp_logits)\n",
        "    loss = -np.log(prob + 1e-8)\n",
        "    losses.append(loss)\n",
        "    probs.append(prob)\n",
        "\n",
        "# 可视化\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 左图：温度参数对InfoNCE损失的影响\n",
        "axes[0].plot(taus, losses, 'o-', linewidth=2, markersize=10, color='steelblue', label='InfoNCE损失')\n",
        "axes[0].set_title('温度参数对InfoNCE损失的影响', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('温度参数 τ')\n",
        "axes[0].set_ylabel('InfoNCE损失')\n",
        "axes[0].set_xscale('log')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].legend()\n",
        "for tau, loss in zip(taus, losses):\n",
        "    axes[0].text(tau, loss + 0.05, f'{loss:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 右图：温度参数对正样本对概率的影响\n",
        "axes[1].plot(taus, probs, 'o-', linewidth=2, markersize=10, color='lightcoral', label='正样本对概率')\n",
        "axes[1].set_title('温度参数对正样本对概率的影响', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('温度参数 τ')\n",
        "axes[1].set_ylabel('正样本对概率')\n",
        "axes[1].set_xscale('log')\n",
        "axes[1].set_ylim([0, 1.1])\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].legend()\n",
        "for tau, prob in zip(taus, probs):\n",
        "    axes[1].text(tau, prob + 0.02, f'{prob:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"InfoNCE损失可视化说明：\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. 左图：温度参数越小，InfoNCE损失越大（因为softmax更锐利）\")\n",
        "print(\"2. 右图：温度参数越小，正样本对的概率越高（因为softmax更锐利）\")\n",
        "print(\"3. 温度参数通常设置为0.07，这是一个经验值，能够平衡损失和概率\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. CLIP文本编码器在VLA中的应用\n",
        "\n",
        "### 4.1 CLIP文本编码器在VLA中的角色\n",
        "\n",
        "在VLA中，CLIP文本编码器作为语言编码器，从输入文本中提取语言特征，这些特征已经与视觉特征对齐，可以直接用于多模态融合。\n",
        "\n",
        "### 4.2 CLIP文本编码器在VLA中的优势\n",
        "\n",
        "CLIP文本编码器在VLA中的优势包括：\n",
        "\n",
        "1. **视觉-语言对齐**：能够实现视觉-语言对齐，这对于VLA任务非常重要\n",
        "2. **零样本能力**：具有零样本学习能力，能够处理训练时未见过的数据\n",
        "3. **预训练模型**：可以使用在大规模数据上预训练的CLIP模型，提高特征提取质量\n",
        "4. **通用性**：可以用于各种VLA任务，如指令理解、场景理解等\n",
        "\n",
        "### 4.3 CLIP文本编码器在VLA中的使用流程\n",
        "\n",
        "**步骤1：文本预处理**\n",
        "\n",
        "将语言指令转换为CLIP文本编码器的输入格式：\n",
        "\n",
        "```python\n",
        "# 示例：语言指令\n",
        "instruction = \"拿起桌子上的红色杯子\"\n",
        "\n",
        "# 分词和编码\n",
        "tokens = tokenizer.tokenize(instruction)  # [\"拿起\", \"桌子\", \"上\", \"的\", \"红色\", \"杯子\"]\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)  # [10, 20, 30, 40, 50, 60]\n",
        "```\n",
        "\n",
        "**步骤2：CLIP文本编码**\n",
        "\n",
        "使用CLIP文本编码器对输入文本进行编码：\n",
        "\n",
        "```python\n",
        "# CLIP文本编码\n",
        "with torch.no_grad():\n",
        "    text_features = clip_text_encoder(token_ids)  # [d]\n",
        "    text_features = F.normalize(text_features, p=2, dim=-1)  # L2归一化\n",
        "```\n",
        "\n",
        "**步骤3：特征融合**\n",
        "\n",
        "将CLIP文本编码器的输出特征与视觉编码器的输出特征进行融合：\n",
        "\n",
        "```python\n",
        "# 特征融合（CLIP特征已经对齐，可以直接使用）\n",
        "multimodal_features = fusion_module(visual_features, text_features)\n",
        "```\n",
        "\n",
        "**步骤4：动作生成**\n",
        "\n",
        "使用融合后的多模态特征生成动作：\n",
        "\n",
        "```python\n",
        "# 动作生成\n",
        "actions = action_decoder(multimodal_features)\n",
        "```\n",
        "\n",
        "### 4.4 CLIP文本编码器在VLA中的微调策略\n",
        "\n",
        "CLIP文本编码器在VLA中的微调策略包括：\n",
        "\n",
        "1. **冻结CLIP**：冻结CLIP文本编码器的参数，只训练后续的融合和动作生成模块\n",
        "2. **部分微调**：只微调CLIP文本编码器的最后几层，保持底层参数不变\n",
        "3. **端到端训练**：同时训练CLIP文本编码器和后续模块，实现端到端优化\n",
        "\n",
        "### 4.5 CLIP文本编码器在VLA中的应用示例\n",
        "\n",
        "下面我们通过代码演示CLIP文本编码器在VLA中的应用：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CLIP文本编码器在VLA中的应用示例\n",
        "# ============================================\n",
        "\n",
        "# 模拟CLIP文本编码器在VLA中的使用流程\n",
        "\n",
        "# 1. 语言指令\n",
        "instructions = [\n",
        "    \"拿起桌子上的红色杯子\",\n",
        "    \"将杯子放在架子上\",\n",
        "    \"打开门并走进房间\",\n",
        "]\n",
        "\n",
        "# 2. 模拟CLIP文本编码（简化版）\n",
        "d_model = 512  # CLIP文本编码器的特征维度\n",
        "\n",
        "# 模拟CLIP文本编码器的输出特征（归一化后）\n",
        "np.random.seed(42)\n",
        "clip_text_features = []\n",
        "for i, instruction in enumerate(instructions):\n",
        "    # 模拟CLIP文本编码后的特征\n",
        "    features = np.random.randn(d_model)\n",
        "    # L2归一化\n",
        "    features = features / np.linalg.norm(features)\n",
        "    clip_text_features.append(features)\n",
        "\n",
        "# 3. 模拟视觉特征（归一化后）\n",
        "visual_features = np.random.randn(len(instructions), d_model)\n",
        "visual_features = visual_features / np.linalg.norm(visual_features, axis=1, keepdims=True)\n",
        "\n",
        "# 4. 计算相似度（用于对比学习）\n",
        "similarities = []\n",
        "for i in range(len(instructions)):\n",
        "    sim = np.dot(clip_text_features[i], visual_features[i])\n",
        "    similarities.append(sim)\n",
        "\n",
        "# 5. 可视化\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 左上：文本特征可视化（前10个维度）\n",
        "ax1 = axes[0, 0]\n",
        "text_feat_sample = np.array([f[:10] for f in clip_text_features])\n",
        "im1 = ax1.imshow(text_feat_sample, aspect='auto', cmap='viridis', interpolation='nearest')\n",
        "ax1.set_title('CLIP文本特征（前10个维度）', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('特征维度')\n",
        "ax1.set_ylabel('指令索引')\n",
        "ax1.set_yticks(range(len(instructions)))\n",
        "ax1.set_yticklabels([f'指令{i+1}' for i in range(len(instructions))])\n",
        "plt.colorbar(im1, ax=ax1)\n",
        "\n",
        "# 右上：视觉特征可视化（前10个维度）\n",
        "ax2 = axes[0, 1]\n",
        "visual_feat_sample = visual_features[:, :10]\n",
        "im2 = ax2.imshow(visual_feat_sample, aspect='auto', cmap='plasma', interpolation='nearest')\n",
        "ax2.set_title('视觉特征（前10个维度）', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('特征维度')\n",
        "ax2.set_ylabel('图像索引')\n",
        "ax2.set_yticks(range(len(instructions)))\n",
        "ax2.set_yticklabels([f'图像{i+1}' for i in range(len(instructions))])\n",
        "plt.colorbar(im2, ax=ax2)\n",
        "\n",
        "# 左下：相似度可视化\n",
        "ax3 = axes[1, 0]\n",
        "colors_sim = ['green' if s > 0.5 else 'red' for s in similarities]\n",
        "bars = ax3.bar(range(len(instructions)), similarities, color=colors_sim, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax3.set_title('文本-图像相似度（视觉-语言对齐）', fontsize=12, fontweight='bold')\n",
        "ax3.set_xlabel('指令索引')\n",
        "ax3.set_ylabel('相似度')\n",
        "ax3.set_xticks(range(len(instructions)))\n",
        "ax3.set_xticklabels([f'指令{i+1}' for i in range(len(instructions))])\n",
        "ax3.set_ylim([0, 1.1])\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "ax3.axhline(y=0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='阈值 (0.5)')\n",
        "for i, (bar, sim) in enumerate(zip(bars, similarities)):\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., sim + 0.02,\n",
        "             f'{sim:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "ax3.legend()\n",
        "\n",
        "# 右下：特征空间可视化（PCA到2D）\n",
        "ax4 = axes[1, 1]\n",
        "pca = PCA(n_components=2)\n",
        "all_features = np.vstack([clip_text_features, visual_features])\n",
        "all_features_2d = pca.fit_transform(all_features)\n",
        "\n",
        "text_2d = all_features_2d[:len(instructions)]\n",
        "visual_2d = all_features_2d[len(instructions):]\n",
        "\n",
        "ax4.scatter(text_2d[:, 0], text_2d[:, 1], c=['r','g','b'], \n",
        "           s=200, marker='o', label='文本特征', edgecolors='black', linewidths=2)\n",
        "ax4.scatter(visual_2d[:, 0], visual_2d[:, 1], c=['r','g','b'], \n",
        "           s=200, marker='s', label='视觉特征', edgecolors='black', linewidths=2)\n",
        "\n",
        "# 连接正样本对\n",
        "for i in range(len(instructions)):\n",
        "    ax4.plot([text_2d[i, 0], visual_2d[i, 0]], \n",
        "            [text_2d[i, 1], visual_2d[i, 1]], \n",
        "            'r--', alpha=0.5, linewidth=2)\n",
        "\n",
        "ax4.set_title('特征空间可视化（PCA到2D）', fontsize=12, fontweight='bold')\n",
        "ax4.set_xlabel('PC1')\n",
        "ax4.set_ylabel('PC2')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('CLIP文本编码器在VLA中的应用：视觉-语言对齐', fontsize=14, fontweight='bold', y=1.0)\n",
        "plt.show()\n",
        "\n",
        "# 6. 特征统计\n",
        "print(\"\\nCLIP文本编码器特征统计：\")\n",
        "for i, (instruction, features) in enumerate(zip(instructions, clip_text_features)):\n",
        "    print(f\"\\n指令 {i+1}: {instruction}\")\n",
        "    print(f\"  特征形状: {features.shape}\")\n",
        "    print(f\"  特征L2范数: {np.linalg.norm(features):.4f} (归一化后应为1.0)\")\n",
        "    print(f\"  与对应视觉特征的相似度: {similarities[i]:.4f}\")\n",
        "\n",
        "print(\"\\nCLIP文本编码器在VLA中的应用示例完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
