{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT详解\n",
        "\n",
        "## 📋 文档说明\n",
        "\n",
        "本文档是GPT（Generative Pre-trained Transformer）的详细理论讲解，比父目录的《语言编码器详解》更加深入和详细。本文档将深入讲解GPT的原理、数学推导和实现细节。通过本文档，你将能够：\n",
        "\n",
        "1. **深入理解GPT的原理**：从自回归生成到语言建模的完整流程\n",
        "2. **掌握语言建模（LM）的数学原理**：理解LM的数学定义、为什么有效、如何优化\n",
        "3. **理解掩码自注意力机制**：理解掩码自注意力的原理和作用\n",
        "4. **掌握GPT的架构**：理解GPT的解码器层、注意力机制等关键组件\n",
        "5. **掌握GPT在VLA中的应用**：理解GPT在VLA模型中的具体应用和优势\n",
        "\n",
        "**学习方式**：本文件是Jupyter Notebook格式，你可以边看边运行代码，通过可视化图表和数学推导更好地理解GPT的原理和过程。\n",
        "\n",
        "**文档结构**：\n",
        "- 父目录：语言编码器详解（见../语言编码器详解.ipynb）\n",
        "- 本文档：GPT详解（本文档）\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 术语表（按出现顺序）\n",
        "\n",
        "### 1. GPT (Generative Pre-trained Transformer)\n",
        "- **中文名称**：GPT\n",
        "- **英文全称**：Generative Pre-trained Transformer\n",
        "- **定义**：GPT是一种基于Transformer的生成式预训练模型，由OpenAI提出。GPT通过自回归方式生成文本，能够根据前面的词语预测下一个词语。GPT的预训练任务是语言建模（Language Modeling），即预测下一个词语的概率。GPT的优势在于：1）生成能力：能够生成连贯的文本；2）预训练模型：使用大规模文本数据预训练，具有强大的语言理解能力；3）可扩展性：通过增加模型大小和数据量，能够持续提升性能；4）通用性：可以用于各种自然语言处理任务。GPT是VLA中的重要语言编码器，在需要生成能力的VLA任务中取得了很好的效果。GPT通过自回归方式生成文本，这是BERT等双向模型无法做到的，这使得GPT能够生成连贯的文本序列。\n",
        "- **核心组成**：GPT的核心组成包括：1）词嵌入层：将词语转换为词向量，包括词嵌入和位置嵌入；2）Transformer解码器：使用多层Transformer解码器编码文本序列；3）掩码自注意力机制：使用掩码自注意力机制，只能看到前面的词语；4）预训练任务：使用语言建模任务预训练模型；5）微调：在特定任务上进行微调。GPT通常使用预训练的权重初始化，然后在VLA任务上进行微调。GPT的架构通常包括12层或24层Transformer解码器，每层包含掩码多头自注意力机制和前馈神经网络，通过残差连接和层归一化加速训练，提高稳定性。\n",
        "- **在VLA中的应用**：在VLA中，GPT是语言编码器的重要方法。VLA模型使用GPT从输入文本中提取语言特征，这些特征将被用于理解语言指令的意图、对象、动作等。GPT的优势在于能够生成连贯的文本，这对于理解复杂的语言指令非常重要。在VLA训练过程中，GPT通常使用预训练的权重初始化，然后在VLA任务上进行微调，以提高特征提取的质量和效率。GPT的输出特征将与视觉编码器的输出特征进行融合，生成多模态表示，最终用于动作生成。GPT还可以用于VLA的动作生成，生成动作序列的描述。GPT的自回归生成能力使得VLA模型能够生成连贯的动作序列描述，这对于理解复杂的语言指令非常重要。\n",
        "- **相关概念**：Transformer、自回归、语言建模、掩码自注意力、语言编码器、生成式模型\n",
        "- **首次出现位置**：本文档标题\n",
        "- **深入学习**：参考父目录的[语言编码器详解](../语言编码器详解.ipynb)和[Transformer编码器详解](../../01_文本特征提取/02_Transformer编码器/理论笔记/Transformer编码器详解.ipynb)\n",
        "- **直观理解**：想象GPT就像一位能够\"续写\"的作家，他能够根据前面的文字预测后面的文字，生成连贯的文本。例如，看到\"拿起\"，GPT能够预测下一个词语可能是\"杯子\"、\"书\"等。在VLA中，GPT帮助模型理解语言指令的语义，从而生成相应的动作。GPT就像语言的\"续写器\"，能够根据前面的词语预测下一个词语，生成连贯的文本序列。\n",
        "\n",
        "### 2. 语言建模 (Language Modeling, LM)\n",
        "- **中文名称**：语言建模\n",
        "- **英文全称**：Language Modeling\n",
        "- **定义**：语言建模是指预测序列中下一个词语的概率分布的任务，是GPT的预训练任务。语言建模的目标是学习语言的概率分布，即给定前面的词语，预测下一个词语的概率。语言建模的数学表示为：$P(w_t | w_{<t})$，其中$w_t$是当前位置的词语，$w_{<t}$是前面的词语序列。语言建模的优势在于：1）无监督学习：不需要标注数据，可以从大规模文本数据中学习；2）通用性：能够学习语言的通用规律，适用于各种下游任务；3）自回归：使用自回归方式生成文本，能够生成连贯的文本；4）可扩展性：通过增加模型大小和数据量，能够持续提升性能。语言建模是GPT等生成式预训练语言模型的基础任务，通过语言建模，模型能够学习语言的概率分布，理解语言的语法、语义、上下文等信息。\n",
        "- **核心组成**：语言建模的核心组成包括：1）序列建模：使用序列模型（如Transformer解码器）建模序列的概率分布；2）条件概率：计算给定前面词语的条件下，下一个词语的概率；3）最大似然估计：最大化训练数据的似然函数，学习语言的概率分布；4）自回归生成：使用自回归方式生成文本，即根据前面的词语预测下一个词语；5）损失函数：使用交叉熵损失函数，最大化下一个词语的条件概率；6）评估指标：使用困惑度（Perplexity）等指标评估模型性能。语言建模通常使用Transformer解码器作为模型架构，通过掩码自注意力机制实现自回归生成，即只能看到前面的词语，不能看到后面的词语。\n",
        "- **在VLA中的应用**：在VLA中，语言建模用于预训练GPT，学习通用的语言表示。VLA模型使用预训练的GPT从输入文本中提取语言特征，这些特征将被用于理解语言指令的意图、对象、动作等。语言建模的通用语言表示使得VLA模型能够更好地理解语言指令，从而生成更准确的动作序列。在某些VLA应用中，语言建模还可以用于生成动作序列的描述，例如生成\"拿起杯子\"、\"放下盘子\"等动作描述。理解语言建模有助于理解GPT等生成式预训练语言模型的工作原理，如何学习语言的概率分布，如何生成连贯的文本。\n",
        "- **相关概念**：GPT、自回归、条件概率、最大似然估计、困惑度、Transformer解码器、掩码自注意力\n",
        "- **首次出现位置**：本文档第2.1节\n",
        "- **深入学习**：参考本文档的语言建模详细讲解部分和[预训练语言模型详解](../../01_文本特征提取/03_预训练语言模型/理论笔记/预训练语言模型详解.ipynb)\n",
        "- **直观理解**：想象语言建模就像一位\"续写专家\"，他能够根据前面的文字预测后面的文字。例如，看到\"拿起\"，语言建模模型会预测下一个词语可能是\"杯子\"、\"盘子\"、\"书\"等。通过这种\"续写\"的方式，语言建模模型学会了语言的规律，理解了词语之间的关系。在VLA中，语言建模帮助模型理解语言指令的语义，从而生成相应的动作。语言建模就像语言的\"预测器\"，能够预测下一个词语，理解语言的概率分布。\n",
        "\n",
        "### 3. 掩码自注意力 (Masked Self-Attention)\n",
        "- **中文名称**：掩码自注意力\n",
        "- **英文全称**：Masked Self-Attention\n",
        "- **定义**：掩码自注意力是指只能看到前面词语的自注意力机制，用于实现自回归生成。掩码自注意力的数学表示为：$\\text{MaskedAttention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}} + M)V$，其中$M$是掩码矩阵，用于屏蔽后面的词语。掩码自注意力的优势在于：1）自回归生成：能够实现自回归生成，即根据前面的词语预测下一个词语；2）训练效率：训练时可以使用并行计算，提高训练效率；3）生成能力：能够生成连贯的文本序列；4）可扩展性：可以扩展到任意长度的序列。掩码自注意力是GPT的核心机制，它使GPT能够实现自回归生成，这是BERT等双向模型无法做到的。\n",
        "- **核心组成**：掩码自注意力的核心组成包括：1）Query、Key、Value：通过线性变换得到Query、Key、Value向量；2）注意力分数：计算Query和Key之间的相似度，数学表示为 $S = \\frac{QK^T}{\\sqrt{d_k}}$；3）掩码矩阵：创建掩码矩阵$M$，用于屏蔽后面的词语，即$M_{ij} = -\\infty$（如果$i < j$），$M_{ij} = 0$（如果$i \\geq j$）；4）掩码注意力分数：将掩码矩阵加到注意力分数上，数学表示为 $S_{masked} = S + M$；5）注意力权重：通过softmax归一化掩码注意力分数，数学表示为 $A = \\text{softmax}(S_{masked})$；6）加权求和：使用注意力权重对Value进行加权求和，得到每个位置的输出，数学表示为 $O = AV$。掩码自注意力通过掩码矩阵实现自回归生成，即每个位置只能看到前面的位置，不能看到后面的位置。\n",
        "- **在VLA中的应用**：在VLA中，掩码自注意力用于GPT语言编码器中，使GPT能够实现自回归生成。VLA模型使用GPT从输入文本中提取语言特征，这些特征将被用于理解语言指令的意图、对象、动作等。掩码自注意力的自回归生成能力使得VLA模型能够生成连贯的动作序列描述，这对于理解复杂的语言指令非常重要。在某些VLA应用中，掩码自注意力还可以用于生成动作序列，例如生成\"拿起杯子\"、\"移动到桌子\"、\"放下杯子\"等动作序列。理解掩码自注意力有助于理解GPT如何实现自回归生成，如何生成连贯的文本序列。\n",
        "- **相关概念**：GPT、自回归、语言建模、自注意力机制、掩码矩阵、Transformer解码器\n",
        "- **首次出现位置**：本文档第1.3节\n",
        "- **深入学习**：参考本文档的掩码自注意力详细讲解部分和[Transformer编码器详解](../../01_文本特征提取/02_Transformer编码器/理论笔记/Transformer编码器详解.ipynb)\n",
        "- **直观理解**：想象掩码自注意力就像一位\"只能向前看\"的读者，他只能看到前面的文字，不能看到后面的文字。例如，在\"拿起桌子上的杯子\"这个句子中，当处理\"桌子\"这个词时，掩码自注意力只能看到\"拿起\"，不能看到\"上\"、\"的\"、\"杯子\"。通过这种\"只能向前看\"的方式，掩码自注意力实现了自回归生成，即根据前面的词语预测下一个词语。在VLA中，掩码自注意力帮助模型生成连贯的动作序列描述，从而生成相应的动作。\n",
        "\n",
        "### 4. 自回归 (Autoregressive)\n",
        "- **中文名称**：自回归\n",
        "- **英文全称**：Autoregressive\n",
        "- **定义**：自回归是指根据前面的元素预测下一个元素的生成方式，是GPT的核心生成方式。自回归的数学表示为：$P(x_t | x_{<t})$，其中$x_t$是当前位置的元素，$x_{<t}$是前面的元素序列。自回归的优势在于：1）生成能力：能够生成连贯的序列；2）训练效率：训练时可以使用并行计算，提高训练效率；3）可扩展性：可以扩展到任意长度的序列；4）通用性：可以用于各种序列生成任务。自回归是GPT的核心生成方式，它使GPT能够生成连贯的文本序列，这是BERT等双向模型无法做到的。\n",
        "- **核心组成**：自回归的核心组成包括：1）序列建模：使用序列模型（如Transformer解码器）建模序列的概率分布；2）条件概率：计算给定前面元素的条件下，下一个元素的概率；3）最大似然估计：最大化训练数据的似然函数，学习序列的概率分布；4）逐步生成：使用逐步生成方式生成序列，即根据前面的元素预测下一个元素；5）损失函数：使用交叉熵损失函数，最大化下一个元素的条件概率；6）评估指标：使用困惑度（Perplexity）等指标评估模型性能。自回归通常使用Transformer解码器作为模型架构，通过掩码自注意力机制实现自回归生成，即只能看到前面的元素，不能看到后面的元素。\n",
        "- **在VLA中的应用**：在VLA中，自回归用于GPT语言编码器中，使GPT能够实现自回归生成。VLA模型使用GPT从输入文本中提取语言特征，这些特征将被用于理解语言指令的意图、对象、动作等。自回归的生成能力使得VLA模型能够生成连贯的动作序列描述，这对于理解复杂的语言指令非常重要。在某些VLA应用中，自回归还可以用于生成动作序列，例如生成\"拿起杯子\"、\"移动到桌子\"、\"放下杯子\"等动作序列。理解自回归有助于理解GPT如何实现自回归生成，如何生成连贯的文本序列。\n",
        "- **相关概念**：GPT、语言建模、掩码自注意力、条件概率、最大似然估计、困惑度、Transformer解码器\n",
        "- **首次出现位置**：本文档第1.2节\n",
        "- **深入学习**：参考本文档的自回归详细讲解部分\n",
        "- **直观理解**：想象自回归就像一位\"续写专家\"，他能够根据前面的文字预测后面的文字。例如，看到\"拿起\"，自回归模型会预测下一个词语可能是\"杯子\"、\"盘子\"、\"书\"等。通过这种\"续写\"的方式，自回归模型学会了语言的规律，理解了词语之间的关系。在VLA中，自回归帮助模型生成连贯的动作序列描述，从而生成相应的动作。自回归就像语言的\"续写器\"，能够根据前面的词语预测下一个词语，生成连贯的文本序列。\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 概述\n",
        "\n",
        "### 什么是GPT\n",
        "\n",
        "GPT是一种基于Transformer的生成式预训练模型，由OpenAI提出。GPT通过自回归方式生成文本，能够根据前面的词语预测下一个词语。\n",
        "\n",
        "### 为什么重要\n",
        "\n",
        "GPT对于VLA学习非常重要，原因包括：\n",
        "\n",
        "1. **生成能力**：能够生成连贯的文本，这对于理解复杂的语言指令非常重要\n",
        "2. **预训练模型**：使用大规模文本数据预训练，具有强大的语言理解能力\n",
        "3. **可扩展性**：通过增加模型大小和数据量，能够持续提升性能\n",
        "4. **通用性**：可以用于各种自然语言处理任务\n",
        "\n",
        "### 学习目标\n",
        "\n",
        "通过本文档的学习，你将能够：\n",
        "\n",
        "1. **深入理解GPT**：理解GPT的原理和架构\n",
        "2. **掌握语言建模**：理解LM的数学定义和计算方法\n",
        "3. **理解掩码自注意力**：理解掩码自注意力的原理和作用\n",
        "4. **掌握GPT在VLA中的应用**：理解GPT在VLA模型中的具体应用\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 导入必要的库\n",
        "# ============================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans', 'Microsoft YaHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 设置图表样式\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "print(\"环境准备完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. GPT的嵌入层详解\n",
        "\n",
        "### 1.1 什么是GPT的嵌入层\n",
        "\n",
        "GPT的嵌入层将输入文本转换为向量表示，包括两个部分：\n",
        "1. **词嵌入（Token Embedding）**：将词语转换为词向量\n",
        "2. **位置嵌入（Position Embedding）**：为每个位置添加位置信息\n",
        "\n",
        "**注意**：GPT不使用段嵌入，因为GPT主要用于单句任务，不像BERT需要处理句子对任务。\n",
        "\n",
        "### 1.2 为什么需要两种嵌入\n",
        "\n",
        "**直观理解**：想象GPT的嵌入层就像给每个词语贴上两个\"标签\"：\n",
        "- **词嵌入**：告诉模型这个词语是什么（语义信息）\n",
        "- **位置嵌入**：告诉模型这个词语在序列中的位置（位置信息）\n",
        "\n",
        "### 1.3 嵌入层的数学表示\n",
        "\n",
        "#### 1.3.1 词嵌入（Token Embedding）\n",
        "\n",
        "**定义**：词嵌入将词语转换为词向量，数学表示为：\n",
        "\n",
        "$$E_t = \\text{Embedding}(w) \\in \\mathbb{R}^{d_{model}}$$\n",
        "\n",
        "其中：\n",
        "- $w$ 是词语（token）\n",
        "- $E_t$ 是词嵌入向量\n",
        "- $d_{model}$ 是模型维度（通常为768或1024）\n",
        "\n",
        "**实现**：词嵌入通常使用可学习的嵌入矩阵，大小为 $V \\times d_{model}$，其中 $V$ 是词汇表大小。\n",
        "\n",
        "#### 1.3.2 位置嵌入（Position Embedding）\n",
        "\n",
        "**定义**：位置嵌入为每个位置添加位置信息，数学表示为：\n",
        "\n",
        "$$E_p = \\text{PositionEmbedding}(pos) \\in \\mathbb{R}^{d_{model}}$$\n",
        "\n",
        "其中：\n",
        "- $pos$ 是位置索引（0, 1, 2, ...）\n",
        "- $E_p$ 是位置嵌入向量\n",
        "\n",
        "**实现**：GPT使用可学习的位置嵌入，大小为 $L_{max} \\times d_{model}$，其中 $L_{max}$ 是最大序列长度（通常为1024或2048）。\n",
        "\n",
        "#### 1.3.3 嵌入层的融合\n",
        "\n",
        "**最终嵌入**：将两种嵌入相加，得到最终的词向量：\n",
        "\n",
        "$$E = E_t + E_p \\in \\mathbb{R}^{d_{model}}$$\n",
        "\n",
        "### 1.4 嵌入层的可视化\n",
        "\n",
        "下面我们通过代码可视化GPT的嵌入层：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# GPT嵌入层的可视化\n",
        "# ============================================\n",
        "\n",
        "# 设置参数\n",
        "vocab_size = 1000  # 词汇表大小\n",
        "max_seq_len = 128  # 最大序列长度\n",
        "d_model = 768  # 模型维度\n",
        "\n",
        "# 创建嵌入层\n",
        "token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "# 示例输入\n",
        "# 假设输入序列为：\"拿起桌子上的杯子\"\n",
        "# 分词后：[\"拿起\", \"桌子\", \"上\", \"的\", \"杯子\"]\n",
        "# 假设词汇表索引：[10, 20, 30, 40, 50]\n",
        "token_ids = torch.tensor([10, 20, 30, 40, 50])  # 词嵌入索引\n",
        "position_ids = torch.arange(len(token_ids))  # 位置索引\n",
        "\n",
        "# 获取嵌入\n",
        "token_emb = token_embedding(token_ids)  # [5, 768]\n",
        "position_emb = position_embedding(position_ids)  # [5, 768]\n",
        "\n",
        "# 融合嵌入\n",
        "final_emb = token_emb + position_emb  # [5, 768]\n",
        "\n",
        "print(f\"词嵌入形状: {token_emb.shape}\")\n",
        "print(f\"位置嵌入形状: {position_emb.shape}\")\n",
        "print(f\"最终嵌入形状: {final_emb.shape}\")\n",
        "\n",
        "# 可视化嵌入层的结构\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. 词嵌入可视化（前10个维度）\n",
        "ax1 = axes[0, 0]\n",
        "token_emb_sample = token_emb[0, :10].detach().numpy()\n",
        "ax1.bar(range(len(token_emb_sample)), token_emb_sample, color='skyblue', alpha=0.7)\n",
        "ax1.set_title('词嵌入（前10个维度）', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('维度索引')\n",
        "ax1.set_ylabel('嵌入值')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. 位置嵌入可视化（前10个维度）\n",
        "ax2 = axes[0, 1]\n",
        "position_emb_sample = position_emb[:, :10].detach().numpy()\n",
        "im1 = ax2.imshow(position_emb_sample.T, aspect='auto', cmap='viridis')\n",
        "ax2.set_title('位置嵌入（前10个维度）', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('位置索引')\n",
        "ax2.set_ylabel('维度索引')\n",
        "ax2.set_yticks(range(10))\n",
        "plt.colorbar(im1, ax=ax2)\n",
        "\n",
        "# 3. 最终嵌入可视化（前10个维度）\n",
        "ax3 = axes[1, 0]\n",
        "final_emb_sample = final_emb[:, :10].detach().numpy()\n",
        "im2 = ax3.imshow(final_emb_sample.T, aspect='auto', cmap='plasma')\n",
        "ax3.set_title('最终嵌入（前10个维度）', fontsize=12, fontweight='bold')\n",
        "ax3.set_xlabel('位置索引')\n",
        "ax3.set_ylabel('维度索引')\n",
        "ax3.set_yticks(range(10))\n",
        "plt.colorbar(im2, ax=ax3)\n",
        "\n",
        "# 4. 嵌入对比（BERT vs GPT）\n",
        "ax4 = axes[1, 1]\n",
        "ax4.axis('off')\n",
        "ax4.text(0.5, 0.7, 'GPT嵌入层组成', fontsize=14, fontweight='bold', ha='center', va='center',\n",
        "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
        "ax4.text(0.5, 0.5, '• 词嵌入 (Token Embedding)', fontsize=12, ha='center', va='center')\n",
        "ax4.text(0.5, 0.4, '• 位置嵌入 (Position Embedding)', fontsize=12, ha='center', va='center')\n",
        "ax4.text(0.5, 0.2, '注意：GPT不使用段嵌入', fontsize=10, ha='center', va='center', \n",
        "         style='italic', color='gray')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('GPT嵌入层可视化', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n嵌入层可视化完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 掩码自注意力机制的可视化\n",
        "# ============================================\n",
        "\n",
        "# 示例：掩码自注意力的注意力权重\n",
        "sentence = [\"拿起\", \"桌子\", \"上\", \"的\", \"杯子\"]\n",
        "seq_len = len(sentence)\n",
        "\n",
        "# 模拟注意力分数（随机生成）\n",
        "np.random.seed(42)\n",
        "attention_scores = np.random.randn(seq_len, seq_len) * 0.5\n",
        "\n",
        "# 创建掩码矩阵（下三角矩阵，上三角为-inf）\n",
        "mask = np.triu(np.ones((seq_len, seq_len)) * -np.inf, k=1)\n",
        "\n",
        "# 应用掩码\n",
        "masked_scores = attention_scores + mask\n",
        "\n",
        "# 计算注意力权重（softmax）\n",
        "attention_weights = F.softmax(torch.tensor(masked_scores), dim=-1).numpy()\n",
        "\n",
        "# 可视化\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# 1. 原始注意力分数\n",
        "im1 = axes[0].imshow(attention_scores, cmap='viridis', aspect='auto', vmin=-2, vmax=2)\n",
        "axes[0].set_title('原始注意力分数', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Key位置')\n",
        "axes[0].set_ylabel('Query位置')\n",
        "axes[0].set_xticks(range(seq_len))\n",
        "axes[0].set_yticks(range(seq_len))\n",
        "axes[0].set_xticklabels(sentence, rotation=45, ha='right')\n",
        "axes[0].set_yticklabels(sentence)\n",
        "plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "# 2. 掩码矩阵\n",
        "im2 = axes[1].imshow(mask, cmap='Reds', aspect='auto', vmin=-10, vmax=0)\n",
        "axes[1].set_title('掩码矩阵（上三角为-∞）', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Key位置')\n",
        "axes[1].set_ylabel('Query位置')\n",
        "axes[1].set_xticks(range(seq_len))\n",
        "axes[1].set_yticks(range(seq_len))\n",
        "axes[1].set_xticklabels(sentence, rotation=45, ha='right')\n",
        "axes[1].set_yticklabels(sentence)\n",
        "plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "# 3. 掩码后的注意力权重\n",
        "im3 = axes[2].imshow(attention_weights, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
        "axes[2].set_title('掩码后的注意力权重', fontsize=12, fontweight='bold')\n",
        "axes[2].set_xlabel('Key位置')\n",
        "axes[2].set_ylabel('Query位置')\n",
        "axes[2].set_xticks(range(seq_len))\n",
        "axes[2].set_yticks(range(seq_len))\n",
        "axes[2].set_xticklabels(sentence, rotation=45, ha='right')\n",
        "axes[2].set_yticklabels(sentence)\n",
        "\n",
        "# 添加数值标注\n",
        "for i in range(seq_len):\n",
        "    for j in range(seq_len):\n",
        "        if j <= i:  # 只标注下三角（非零部分）\n",
        "            axes[2].text(j, i, f'{attention_weights[i, j]:.2f}', \n",
        "                        ha='center', va='center', fontsize=9, fontweight='bold',\n",
        "                        color='white' if attention_weights[i, j] > 0.5 else 'black')\n",
        "\n",
        "plt.colorbar(im3, ax=axes[2])\n",
        "plt.tight_layout()\n",
        "plt.suptitle('掩码自注意力机制可视化', fontsize=14, fontweight='bold', y=1.05)\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"掩码自注意力机制可视化说明：\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. 左图：原始注意力分数，每个位置可以看到所有位置\")\n",
        "print(\"2. 中图：掩码矩阵，上三角为-∞，用于屏蔽后面的词语\")\n",
        "print(\"3. 右图：掩码后的注意力权重，上三角为0，只能看到前面的词语\")\n",
        "print(\"4. 掩码自注意力实现了自回归生成，每个位置只能关注前面的位置\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. 语言建模（LM）详解\n",
        "\n",
        "### 3.1 什么是语言建模\n",
        "\n",
        "**直观理解**：想象语言建模就像一位\"续写专家\"，他能够根据前面的文字预测后面的文字。\n",
        "\n",
        "**定义**：语言建模是指预测序列中下一个词语的概率分布的任务，是GPT的预训练任务。语言建模的目标是学习语言的概率分布，即给定前面的词语，预测下一个词语的概率。\n",
        "\n",
        "### 3.2 为什么需要语言建模\n",
        "\n",
        "语言建模的优势在于：\n",
        "\n",
        "1. **无监督学习**：不需要标注数据，可以从大规模文本数据中学习\n",
        "2. **通用性**：能够学习语言的通用规律，适用于各种下游任务\n",
        "3. **自回归生成**：使用自回归方式生成文本，能够生成连贯的文本\n",
        "4. **可扩展性**：通过增加模型大小和数据量，能够持续提升性能\n",
        "\n",
        "### 3.3 语言建模的数学推导详解\n",
        "\n",
        "#### 3.3.1 从基础概率开始\n",
        "\n",
        "**步骤1：理解条件概率**\n",
        "\n",
        "语言建模的目标是预测下一个词语的条件概率：\n",
        "\n",
        "$$P(w_t | w_{<t})$$\n",
        "\n",
        "其中：\n",
        "- $w_t$ 是当前位置的词语\n",
        "- $w_{<t} = w_1, w_2, \\ldots, w_{t-1}$ 是前面的词语序列\n",
        "\n",
        "**步骤2：理解序列的概率**\n",
        "\n",
        "序列的概率为：\n",
        "\n",
        "$$P(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^{T} P(w_t | w_{<t})$$\n",
        "\n",
        "**步骤3：理解对数似然**\n",
        "\n",
        "为了便于计算，通常使用对数似然：\n",
        "\n",
        "$$\\log P(w_1, w_2, \\ldots, w_T) = \\sum_{t=1}^{T} \\log P(w_t | w_{<t})$$\n",
        "\n",
        "**步骤4：理解损失函数**\n",
        "\n",
        "损失函数是负对数似然：\n",
        "\n",
        "$$\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(w_t | w_{<t}, \\theta)$$\n",
        "\n",
        "其中$\\theta$是模型参数。\n",
        "\n",
        "#### 3.3.2 语言建模的具体计算示例\n",
        "\n",
        "**示例：计算\"拿起桌子上的杯子\"的语言建模概率**\n",
        "\n",
        "假设我们有一个句子：\"拿起 桌子 上 的 杯子\"。\n",
        "\n",
        "**步骤1：计算每个位置的条件概率**\n",
        "\n",
        "对于位置1（\"拿起\"）：\n",
        "- $P(拿起 | \\emptyset) = P(拿起)$（没有前面的词语）\n",
        "\n",
        "对于位置2（\"桌子\"）：\n",
        "- $P(桌子 | 拿起)$\n",
        "\n",
        "对于位置3（\"上\"）：\n",
        "- $P(上 | 拿起, 桌子)$\n",
        "\n",
        "对于位置4（\"的\"）：\n",
        "- $P(的 | 拿起, 桌子, 上)$\n",
        "\n",
        "对于位置5（\"杯子\"）：\n",
        "- $P(杯子 | 拿起, 桌子, 上, 的)$\n",
        "\n",
        "**步骤2：计算序列的概率**\n",
        "\n",
        "序列的概率为：\n",
        "\n",
        "$$P(拿起, 桌子, 上, 的, 杯子) = P(拿起) \\times P(桌子 | 拿起) \\times P(上 | 拿起, 桌子) \\times P(的 | 拿起, 桌子, 上) \\times P(杯子 | 拿起, 桌子, 上, 的)$$\n",
        "\n",
        "**步骤3：计算对数似然**\n",
        "\n",
        "对数似然为：\n",
        "\n",
        "$$\\log P(拿起, 桌子, 上, 的, 杯子) = \\log P(拿起) + \\log P(桌子 | 拿起) + \\log P(上 | 拿起, 桌子) + \\log P(的 | 拿起, 桌子, 上) + \\log P(杯子 | 拿起, 桌子, 上, 的)$$\n",
        "\n",
        "**步骤4：计算损失**\n",
        "\n",
        "损失函数为：\n",
        "\n",
        "$$\\mathcal{L} = -(\\log P(拿起) + \\log P(桌子 | 拿起) + \\log P(上 | 拿起, 桌子) + \\log P(的 | 拿起, 桌子, 上) + \\log P(杯子 | 拿起, 桌子, 上, 的))$$\n",
        "\n",
        "### 3.4 语言建模的可视化\n",
        "\n",
        "下面我们通过代码可视化语言建模的过程：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 语言建模可视化（示例：预测下一个词语）\n",
        "# ============================================\n",
        "np.random.seed(42)\n",
        "\n",
        "# 模拟一个句子：\"拿起 桌子 上 的 杯子\"\n",
        "sentence = ['拿起', '桌子', '上', '的', '杯子']\n",
        "vocab = ['拿起', '桌子', '上', '的', '杯子', '盘子', '书', '放下', '移动', '其他']\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 模拟语言建模模型预测下一个词语的概率\n",
        "# 对于每个位置，模型预测下一个词语的概率分布\n",
        "next_word_probs = []\n",
        "\n",
        "# 位置0：给定\"拿起\"，预测下一个词语\n",
        "probs_0 = np.array([0.05, 0.60, 0.10, 0.05, 0.15, 0.02, 0.01, 0.01, 0.01, 0.00])\n",
        "next_word_probs.append(probs_0)\n",
        "\n",
        "# 位置1：给定\"拿起 桌子\"，预测下一个词语\n",
        "probs_1 = np.array([0.02, 0.05, 0.70, 0.15, 0.05, 0.01, 0.01, 0.00, 0.01, 0.00])\n",
        "next_word_probs.append(probs_1)\n",
        "\n",
        "# 位置2：给定\"拿起 桌子 上\"，预测下一个词语\n",
        "probs_2 = np.array([0.01, 0.02, 0.05, 0.80, 0.10, 0.01, 0.00, 0.00, 0.01, 0.00])\n",
        "next_word_probs.append(probs_2)\n",
        "\n",
        "# 位置3：给定\"拿起 桌子 上 的\"，预测下一个词语\n",
        "probs_3 = np.array([0.01, 0.01, 0.02, 0.05, 0.85, 0.03, 0.02, 0.00, 0.01, 0.00])\n",
        "next_word_probs.append(probs_3)\n",
        "\n",
        "# 可视化\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 左上：位置0的预测概率\n",
        "pred_word_0 = vocab[np.argmax(probs_0)]\n",
        "colors_0 = ['red' if i == np.argmax(probs_0) else 'steelblue' for i in range(vocab_size)]\n",
        "axes[0, 0].bar(range(vocab_size), probs_0, color=colors_0, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_title(f'给定\"拿起\"，预测下一个词语\\\\n预测: {pred_word_0} (概率: {probs_0[np.argmax(probs_0)]:.2f})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('词语')\n",
        "axes[0, 0].set_ylabel('概率')\n",
        "axes[0, 0].set_xticks(range(vocab_size))\n",
        "axes[0, 0].set_xticklabels(vocab, rotation=45, ha='right')\n",
        "axes[0, 0].set_ylim(0, 1.1)\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, prob in enumerate(probs_0):\n",
        "    if prob > 0.05:  # 只标注概率大于0.05的\n",
        "        axes[0, 0].text(i, prob + 0.02, f'{prob:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 右上：位置1的预测概率\n",
        "pred_word_1 = vocab[np.argmax(probs_1)]\n",
        "colors_1 = ['red' if i == np.argmax(probs_1) else 'steelblue' for i in range(vocab_size)]\n",
        "axes[0, 1].bar(range(vocab_size), probs_1, color=colors_1, edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].set_title(f'给定\"拿起 桌子\"，预测下一个词语\\\\n预测: {pred_word_1} (概率: {probs_1[np.argmax(probs_1)]:.2f})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('词语')\n",
        "axes[0, 1].set_ylabel('概率')\n",
        "axes[0, 1].set_xticks(range(vocab_size))\n",
        "axes[0, 1].set_xticklabels(vocab, rotation=45, ha='right')\n",
        "axes[0, 1].set_ylim(0, 1.1)\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "for i, prob in enumerate(probs_1):\n",
        "    if prob > 0.05:\n",
        "        axes[0, 1].text(i, prob + 0.02, f'{prob:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 左下：位置2的预测概率\n",
        "pred_word_2 = vocab[np.argmax(probs_2)]\n",
        "colors_2 = ['red' if i == np.argmax(probs_2) else 'steelblue' for i in range(vocab_size)]\n",
        "axes[1, 0].bar(range(vocab_size), probs_2, color=colors_2, edgecolor='black', alpha=0.7)\n",
        "axes[1, 0].set_title(f'给定\"拿起 桌子 上\"，预测下一个词语\\\\n预测: {pred_word_2} (概率: {probs_2[np.argmax(probs_2)]:.2f})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('词语')\n",
        "axes[1, 0].set_ylabel('概率')\n",
        "axes[1, 0].set_xticks(range(vocab_size))\n",
        "axes[1, 0].set_xticklabels(vocab, rotation=45, ha='right')\n",
        "axes[1, 0].set_ylim(0, 1.1)\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, prob in enumerate(probs_2):\n",
        "    if prob > 0.05:\n",
        "        axes[1, 0].text(i, prob + 0.02, f'{prob:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 右下：位置3的预测概率\n",
        "pred_word_3 = vocab[np.argmax(probs_3)]\n",
        "colors_3 = ['red' if i == np.argmax(probs_3) else 'steelblue' for i in range(vocab_size)]\n",
        "axes[1, 1].bar(range(vocab_size), probs_3, color=colors_3, edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].set_title(f'给定\"拿起 桌子 上 的\"，预测下一个词语\\\\n预测: {pred_word_3} (概率: {probs_3[np.argmax(probs_3)]:.2f})', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('词语')\n",
        "axes[1, 1].set_ylabel('概率')\n",
        "axes[1, 1].set_xticks(range(vocab_size))\n",
        "axes[1, 1].set_xticklabels(vocab, rotation=45, ha='right')\n",
        "axes[1, 1].set_ylim(0, 1.1)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "for i, prob in enumerate(probs_3):\n",
        "    if prob > 0.05:\n",
        "        axes[1, 1].text(i, prob + 0.02, f'{prob:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"语言建模可视化说明：\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. 左上：给定\\\"拿起\\\"，预测下一个词语，模型预测\\\"桌子\\\"的概率最高\")\n",
        "print(\"2. 右上：给定\\\"拿起 桌子\\\"，预测下一个词语，模型预测\\\"上\\\"的概率最高\")\n",
        "print(\"3. 左下：给定\\\"拿起 桌子 上\\\"，预测下一个词语，模型预测\\\"的\\\"的概率最高\")\n",
        "print(\"4. 右下：给定\\\"拿起 桌子 上 的\\\"，预测下一个词语，模型预测\\\"杯子\\\"的概率最高\")\n",
        "print(\"5. 语言建模通过预测下一个词语，学习语言的概率分布，理解语言的规律\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# GPT架构的可视化\n",
        "# ============================================\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 14))\n",
        "ax.axis('off')\n",
        "ax.set_xlim([0, 10])\n",
        "ax.set_ylim([0, 16])\n",
        "ax.set_title('GPT架构示意图', fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "# 颜色定义\n",
        "colors = {\n",
        "    'embedding': 'lightblue',\n",
        "    'decoder': 'lightgreen',\n",
        "    'attention': 'lightcoral',\n",
        "    'ffn': 'lightyellow',\n",
        "    'output': 'wheat'\n",
        "}\n",
        "\n",
        "# 1. 输入层\n",
        "y_pos = 15\n",
        "ax.text(5, y_pos + 0.3, '输入文本', fontsize=12, fontweight='bold', ha='center')\n",
        "input_box = FancyBboxPatch((3, y_pos - 0.3), 4, 0.6,\n",
        "                           boxstyle=\"round,pad=0.05\", facecolor='lightgray', edgecolor='black', linewidth=2)\n",
        "ax.add_patch(input_box)\n",
        "ax.text(5, y_pos, '拿起 桌子 上 的 杯子', fontsize=10, ha='center', va='center')\n",
        "\n",
        "# 2. 嵌入层\n",
        "y_pos = 13\n",
        "ax.text(5, y_pos + 0.5, '嵌入层', fontsize=12, fontweight='bold', ha='center')\n",
        "embed_box = FancyBboxPatch((3, y_pos - 0.4), 4, 0.8,\n",
        "                            boxstyle=\"round,pad=0.05\", facecolor=colors['embedding'], edgecolor='black', linewidth=2)\n",
        "ax.add_patch(embed_box)\n",
        "ax.text(5, y_pos + 0.1, '词嵌入 + 位置嵌入', fontsize=9, ha='center', va='center')\n",
        "\n",
        "# 箭头\n",
        "arrow1 = FancyArrowPatch((5, y_pos - 0.4), (5, y_pos + 0.4), \n",
        "                         arrowstyle='<->', mutation_scale=20, color='black', linewidth=1.5)\n",
        "ax.add_patch(arrow1)\n",
        "\n",
        "# 3. Transformer解码器层（多层）\n",
        "num_layers = 3  # 简化显示，只显示3层\n",
        "layer_height = 1.2\n",
        "y_start = 11\n",
        "\n",
        "for i in range(num_layers):\n",
        "    y_pos = y_start - i * layer_height\n",
        "    \n",
        "    # 解码器层框\n",
        "    decoder_box = FancyBboxPatch((2, y_pos - 0.5), 6, 1.0,\n",
        "                                 boxstyle=\"round,pad=0.05\", facecolor=colors['decoder'], \n",
        "                                 edgecolor='black', linewidth=1.5)\n",
        "    ax.add_patch(decoder_box)\n",
        "    ax.text(5, y_pos + 0.2, f'Transformer解码器层 {i+1}', fontsize=10, fontweight='bold', ha='center', va='center')\n",
        "    \n",
        "    # 掩码多头自注意力\n",
        "    attn_box = FancyBboxPatch((2.2, y_pos - 0.1), 2.6, 0.4,\n",
        "                              boxstyle=\"round,pad=0.02\", facecolor=colors['attention'], \n",
        "                              edgecolor='black', linewidth=1)\n",
        "    ax.add_patch(attn_box)\n",
        "    ax.text(3.5, y_pos + 0.1, '掩码多头自注意力', fontsize=8, ha='center', va='center')\n",
        "    \n",
        "    # 前馈神经网络\n",
        "    ffn_box = FancyBboxPatch((5.2, y_pos - 0.1), 2.6, 0.4,\n",
        "                             boxstyle=\"round,pad=0.02\", facecolor=colors['ffn'], \n",
        "                             edgecolor='black', linewidth=1)\n",
        "    ax.add_patch(ffn_box)\n",
        "    ax.text(6.5, y_pos + 0.1, '前馈神经网络', fontsize=8, ha='center', va='center')\n",
        "    \n",
        "    # 残差连接和层归一化（用虚线表示）\n",
        "    if i < num_layers - 1:\n",
        "        arrow = FancyArrowPatch((5, y_pos - 0.5), (5, y_pos - 0.7), \n",
        "                               arrowstyle='->', mutation_scale=15, color='gray', \n",
        "                               linewidth=1, linestyle='--', alpha=0.5)\n",
        "        ax.add_patch(arrow)\n",
        "\n",
        "# 4. 输出层\n",
        "y_pos = 6.5\n",
        "ax.text(5, y_pos + 0.3, '输出层', fontsize=12, fontweight='bold', ha='center')\n",
        "output_box = FancyBboxPatch((3, y_pos - 0.3), 4, 0.6,\n",
        "                             boxstyle=\"round,pad=0.05\", facecolor=colors['output'], edgecolor='black', linewidth=2)\n",
        "ax.add_patch(output_box)\n",
        "ax.text(5, y_pos, '语言特征向量', fontsize=10, ha='center', va='center')\n",
        "\n",
        "# 箭头\n",
        "arrow2 = FancyArrowPatch((5, y_pos - 0.3), (5, y_pos + 0.3), \n",
        "                         arrowstyle='<->', mutation_scale=20, color='black', linewidth=1.5)\n",
        "ax.add_patch(arrow2)\n",
        "\n",
        "# 5. 预训练任务\n",
        "ax.text(8.5, 13, '预训练任务', fontsize=11, fontweight='bold', ha='center')\n",
        "lm_box = FancyBboxPatch((7.5, 11.5), 2, 0.8,\n",
        "                         boxstyle=\"round,pad=0.05\", facecolor='lightpink', edgecolor='black', linewidth=1.5)\n",
        "ax.add_patch(lm_box)\n",
        "ax.text(8.5, 12, '语言建模\\n(LM)', fontsize=9, ha='center', va='center', fontweight='bold')\n",
        "\n",
        "# 连接线（从输出层到预训练任务）\n",
        "arrow3 = FancyArrowPatch((7, 6.5), (7.5, 12), \n",
        "                         arrowstyle='->', mutation_scale=15, color='red', linewidth=1.5, alpha=0.7)\n",
        "ax.add_patch(arrow3)\n",
        "\n",
        "# 6. 标注掩码自注意力的特点\n",
        "ax.text(8.5, 9, '掩码自注意力特点', fontsize=10, fontweight='bold', ha='center', \n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "ax.text(8.5, 8.5, '• 只能看到前面的词语', fontsize=9, ha='center', va='center')\n",
        "ax.text(8.5, 8.2, '• 实现自回归生成', fontsize=9, ha='center', va='center')\n",
        "ax.text(8.5, 7.9, '• 训练时并行计算', fontsize=9, ha='center', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nGPT架构可视化完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# GPT在VLA中的应用示例\n",
        "# ============================================\n",
        "\n",
        "# 模拟GPT在VLA中的使用流程\n",
        "\n",
        "# 1. 语言指令\n",
        "instructions = [\n",
        "    \"拿起桌子上的红色杯子\",\n",
        "    \"将杯子放在架子上\",\n",
        "    \"打开门并走进房间\",\n",
        "]\n",
        "\n",
        "# 2. 模拟GPT编码（简化版）\n",
        "d_model = 768\n",
        "seq_len = 10  # 假设序列长度为10\n",
        "\n",
        "# 模拟GPT的输出特征\n",
        "np.random.seed(42)\n",
        "gpt_features = []\n",
        "for i, instruction in enumerate(instructions):\n",
        "    # 模拟GPT编码后的特征\n",
        "    features = np.random.randn(seq_len, d_model)\n",
        "    gpt_features.append(features)\n",
        "\n",
        "# 3. 可视化GPT特征\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
        "\n",
        "for i, (instruction, features) in enumerate(zip(instructions, gpt_features)):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # 可视化特征的前10个维度\n",
        "    im = ax.imshow(features[:, :10].T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
        "    ax.set_title(f'GPT特征：\"{instruction}\"', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('序列位置', fontsize=10)\n",
        "    ax.set_ylabel('特征维度（前10维）', fontsize=10)\n",
        "    ax.set_yticks(range(10))\n",
        "    \n",
        "    # 添加颜色条\n",
        "    plt.colorbar(im, ax=ax, fraction=0.02, pad=0.04)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('GPT在VLA中的应用：语言特征提取', fontsize=14, fontweight='bold', y=1.0)\n",
        "plt.show()\n",
        "\n",
        "# 4. 特征统计\n",
        "print(\"\\nGPT特征统计：\")\n",
        "for i, (instruction, features) in enumerate(zip(instructions, gpt_features)):\n",
        "    print(f\"\\n指令 {i+1}: {instruction}\")\n",
        "    print(f\"  特征形状: {features.shape}\")\n",
        "    print(f\"  特征均值: {features.mean():.4f}\")\n",
        "    print(f\"  特征标准差: {features.std():.4f}\")\n",
        "    print(f\"  特征范围: [{features.min():.4f}, {features.max():.4f}]\")\n",
        "\n",
        "print(\"\\nGPT在VLA中的应用示例完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. 总结\n",
        "\n",
        "### 6.1 GPT的核心思想\n",
        "\n",
        "GPT的核心思想包括：\n",
        "\n",
        "1. **自回归生成**：通过自回归方式生成文本，根据前面的词语预测下一个词语\n",
        "2. **预训练+微调**：使用大规模文本数据预训练，然后在特定任务上进行微调\n",
        "3. **语言建模**：通过预测下一个词语，学习语言的概率分布\n",
        "4. **掩码自注意力**：通过掩码自注意力机制实现自回归生成\n",
        "\n",
        "### 6.2 GPT的优势\n",
        "\n",
        "GPT的优势包括：\n",
        "\n",
        "1. **生成能力**：能够生成连贯的文本序列\n",
        "2. **预训练模型**：使用大规模文本数据预训练，具有强大的语言理解能力\n",
        "3. **自回归生成**：能够生成连贯的动作序列描述\n",
        "4. **通用性**：可以用于各种自然语言处理任务\n",
        "\n",
        "### 6.3 GPT在VLA中的重要性\n",
        "\n",
        "GPT在VLA中的重要性体现在：\n",
        "\n",
        "1. **语言理解**：GPT是VLA中重要的语言编码器，能够理解语言指令的语义\n",
        "2. **特征提取**：GPT能够从输入文本中提取高质量的语言特征，用于后续的多模态融合和动作生成\n",
        "3. **预训练优势**：GPT的预训练权重能够提供强大的语言理解能力，减少VLA任务的训练数据需求\n",
        "4. **生成能力优势**：GPT的自回归生成能力使得VLA模型能够生成连贯的动作序列描述，这对于理解复杂的语言指令非常重要\n",
        "\n",
        "### 6.4 GPT vs BERT\n",
        "\n",
        "GPT和BERT的主要区别：\n",
        "\n",
        "| 特性 | GPT | BERT |\n",
        "|------|-----|------|\n",
        "| **编码方式** | 单向（只能看到前面的词语） | 双向（可以看到前后文的词语） |\n",
        "| **预训练任务** | 语言建模（LM） | 掩码语言模型（MLM）+ 下一句预测（NSP） |\n",
        "| **生成能力** | 强（能够生成文本） | 弱（主要用于理解） |\n",
        "| **理解能力** | 强（能够理解语言语义） | 强（能够理解语言语义） |\n",
        "| **应用场景** | 生成任务、理解任务 | 理解任务 |\n",
        "\n",
        "### 6.5 学习建议\n",
        "\n",
        "1. **深入理解自回归生成**：理解GPT如何通过自回归方式生成文本\n",
        "2. **掌握掩码自注意力**：理解掩码自注意力的数学原理和计算方法\n",
        "3. **理解语言建模**：理解LM的数学原理和计算方法\n",
        "4. **实践应用**：在VLA任务中实践使用GPT，理解其在VLA中的具体应用\n",
        "\n",
        "---\n",
        "\n",
        "## 📝 文档信息\n",
        "\n",
        "- **创建时间**：2024年\n",
        "- **文档版本**：v1.0\n",
        "- **维护者**：VLA学习团队\n",
        "- **相关文档**：\n",
        "  - [语言编码器详解](../语言编码器详解.ipynb)\n",
        "  - [Transformer编码器详解](../../01_文本特征提取/02_Transformer编码器/理论笔记/Transformer编码器详解.ipynb)\n",
        "  - [预训练语言模型详解](../../01_文本特征提取/03_预训练语言模型/理论笔记/预训练语言模型详解.ipynb)\n",
        "  - [BERT详解](../01_BERT/理论笔记/BERT详解.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
