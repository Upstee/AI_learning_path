# 线性代数

## 1. 课程概述

### 课程目标
1. 理解向量、矩阵的基本概念和运算
2. 掌握矩阵的乘法、转置、逆等基本操作
3. 理解线性变换和矩阵表示
4. 掌握特征值和特征向量的计算与应用
5. 能够使用NumPy进行线性代数计算

### 预计学习时间
- **理论学习**：12-15小时
- **代码实践**：10-12小时
- **练习巩固**：8-10小时
- **总计**：30-37小时（约3-4周）

### 难度等级
- **中等** - 需要理解抽象概念，但通过可视化可以掌握

### 课程定位
- **前置课程**：高中数学（向量、矩阵基础）
- **后续课程**：02_概率统计、03_微积分、04_机器学习基础
- **在体系中的位置**：AI的基础数学，几乎所有AI算法都依赖线性代数

### 学完能做什么
- 能够理解和使用向量、矩阵表示数据
- 能够进行矩阵运算和线性变换
- 能够理解神经网络中的矩阵运算
- 能够使用NumPy进行线性代数计算

---

## 2. 前置知识检查

### 必备前置概念清单
- **高中数学**：向量、矩阵的基本概念
- **代数运算**：加减乘除、指数运算
- **Python基础**：变量、列表、函数
- **NumPy基础**：数组创建、基本操作

### 回顾链接/跳转
- 如果不熟悉NumPy：`03_数据处理基础/01_NumPy/`
- 如果不熟悉Python：`01_Python进阶/`

### 入门小测

**选择题**（每题2分，共10分）

1. 向量是什么？
   A. 一个数字  B. 一组有序数字  C. 一个矩阵  D. 一个函数
   **答案**：B

2. 矩阵的维度通常如何表示？
   A. 行数×列数  B. 列数×行数  C. 元素个数  D. 对角线长度
   **答案**：A

3. 两个矩阵可以相乘的条件是？
   A. 行数相同  B. 列数相同  C. 第一个矩阵的列数等于第二个矩阵的行数  D. 维度相同
   **答案**：C

4. 单位矩阵的特点是什么？
   A. 所有元素都是1  B. 主对角线为1，其他为0  C. 所有元素都是0  D. 对称矩阵
   **答案**：B

5. 向量的点积结果是什么？
   A. 向量  B. 矩阵  C. 标量  D. 张量
   **答案**：C

**简答题**（每题5分，共10分）

1. 解释向量和标量的区别。
   **参考答案**：标量是单个数字，向量是一组有序数字。

2. 说明矩阵乘法的几何意义。
   **参考答案**：矩阵乘法表示线性变换，可以旋转、缩放、剪切等。

**评分标准**：≥16分（80%）为通过

### 不会时的补救指引
如果小测不通过，建议：
1. 复习高中数学（向量、矩阵）
2. 学习NumPy基础
3. 完成基础练习后再继续

---

## 3. 核心知识点详解

### 3.1 向量

#### 概念引入与直观类比

**类比**：向量就像"箭头"，有方向和长度。

- **方向**：指向哪里
- **长度（模）**：有多长
- **位置**：从哪里开始

例如：
- 位移向量：从A点到B点的位移
- 速度向量：物体运动的方向和速度

#### 逐步理论推导

**步骤1：向量的表示**
```
向量 a = [a₁, a₂, ..., aₙ]
```

**步骤2：向量的加法**
```
a + b = [a₁+b₁, a₂+b₂, ..., aₙ+bₙ]
```

**步骤3：标量乘法**
```
k·a = [k·a₁, k·a₂, ..., k·aₙ]
```

**步骤4：向量的模（长度）**
```
|a| = √(a₁² + a₂² + ... + aₙ²)
```

**步骤5：点积（内积）**
```
a·b = a₁b₁ + a₂b₂ + ... + aₙbₙ
```

#### 数学公式与必要证明

**向量的模**：
对于向量 **a** = [a₁, a₂, ..., aₙ]，
|**a**| = √(∑ᵢ aᵢ²)

**点积公式**：
**a**·**b** = ∑ᵢ aᵢbᵢ = |**a**||**b**|cos(θ)

其中θ是两向量之间的夹角。

**几何意义**：
- 点积为0：两向量垂直
- 点积>0：夹角<90°
- 点积<0：夹角>90°

#### 图解/可视化

```
向量表示：
     y
     ↑
     |    b
     |   ↗
     |  /
     | /
     |/__→ x
    a

向量加法：
a + b = c
```

#### 关键性质

**优点**：
- **直观**：可以用图形表示
- **计算简单**：运算规则清晰
- **应用广泛**：AI中大量使用

**适用场景**：
- 数据表示（特征向量）
- 相似度计算（点积）
- 降维（投影）

---

### 3.2 矩阵

#### 概念引入与直观类比

**类比**：矩阵就像"表格"，是数据的二维排列。

- **行**：一个样本的所有特征
- **列**：所有样本的某个特征
- **元素**：具体的数据值

例如：
- 图像：每个像素是一个元素
- 数据集：每行是一个样本，每列是一个特征

#### 逐步理论推导

**步骤1：矩阵的表示**
```
A = [a₁₁  a₁₂  ...  a₁ₙ]
    [a₂₁  a₂₂  ...  a₂ₙ]
    [...  ...  ...  ...]
    [aₘ₁  aₘ₂  ...  aₘₙ]
```

**步骤2：矩阵的加法**
```
(A + B)ᵢⱼ = Aᵢⱼ + Bᵢⱼ
```

**步骤3：标量乘法**
```
(kA)ᵢⱼ = k·Aᵢⱼ
```

**步骤4：矩阵乘法**
```
(AB)ᵢⱼ = ∑ₖ AᵢₖBₖⱼ
```

**步骤5：矩阵转置**
```
(Aᵀ)ᵢⱼ = Aⱼᵢ
```

#### 数学公式与必要证明

**矩阵乘法**：
如果A是m×n矩阵，B是n×p矩阵，则AB是m×p矩阵：
(AB)ᵢⱼ = ∑ₖ₌₁ⁿ AᵢₖBₖⱼ

**矩阵乘法的性质**：
- 结合律：(AB)C = A(BC)
- 分配律：A(B+C) = AB + AC
- **不满足交换律**：AB ≠ BA（一般情况）

**单位矩阵**：
I = [1  0  ...  0]
    [0  1  ...  0]
    [... ... ... ...]
    [0  0  ...  1]

性质：AI = IA = A

#### 图解/可视化

```
矩阵乘法：
A (m×n) × B (n×p) = C (m×p)

    B
    ↓
A → C
```

#### 关键性质

**优点**：
- **高效计算**：可以用矩阵运算批量处理
- **表示线性变换**：旋转、缩放、投影等
- **并行计算**：适合GPU加速

**适用场景**：
- 神经网络（权重矩阵）
- 图像处理（像素矩阵）
- 数据变换（特征矩阵）

---

### 3.3 线性变换

#### 概念引入与直观类比

**类比**：线性变换就像"变形"，对空间进行旋转、缩放、剪切等操作。

- **旋转**：绕原点旋转
- **缩放**：放大或缩小
- **剪切**：倾斜变形
- **投影**：降维

#### 逐步理论推导

**步骤1：线性变换的定义**
对于向量**x**，线性变换T满足：
- T(**x** + **y**) = T(**x**) + T(**y**)
- T(k**x**) = kT(**x**)

**步骤2：矩阵表示线性变换**
任何线性变换都可以用矩阵表示：
T(**x**) = A**x**

**步骤3：常见变换矩阵**

**旋转矩阵**（2D，逆时针旋转θ）：
```
R = [cos(θ)  -sin(θ)]
    [sin(θ)   cos(θ)]
```

**缩放矩阵**：
```
S = [sₓ  0 ]
    [0   sᵧ]
```

**步骤4：组合变换**
多个变换可以组合：
T(**x**) = T₂(T₁(**x**)) = A₂(A₁**x**) = (A₂A₁)**x**

---

### 3.4 特征值与特征向量

#### 概念引入与直观类比

**类比**：特征向量就像"不变的方向"，在这个方向上，变换只是缩放。

- **特征向量**：变换后方向不变
- **特征值**：在这个方向上的缩放倍数

#### 逐步理论推导

**步骤1：特征值方程**
对于矩阵A，如果存在非零向量**v**和标量λ，使得：
A**v** = λ**v**

则λ是特征值，**v**是对应的特征向量。

**步骤2：求解特征值**
从特征值方程得到：
(A - λI)**v** = 0

有非零解的条件是：
det(A - λI) = 0

这是特征多项式，解出λ。

**步骤3：求解特征向量**
对每个特征值λᵢ，求解：
(A - λᵢI)**v** = 0

得到对应的特征向量**v**ᵢ。

#### 数学公式与必要证明

**特征值分解**：
如果A是n×n矩阵，有n个线性无关的特征向量，则：
A = PDP⁻¹

其中：
- P：特征向量组成的矩阵
- D：特征值组成的对角矩阵

**应用**：
- **主成分分析（PCA）**：降维
- **矩阵幂**：Aⁿ = PDⁿP⁻¹
- **矩阵函数**：eᴬ = PeᴰP⁻¹

---

## 4. Python代码实践

### 4.1 环境与依赖版本

- **Python版本**：3.8+
- **依赖**：
  - numpy（需要安装：`pip install numpy`）
  - matplotlib（可选，用于可视化：`pip install matplotlib`）

### 4.2 从零开始的完整可运行示例

#### 示例1：向量运算

```python
import numpy as np

# 创建向量
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# 向量加法
c = a + b
print(f"a + b = {c}")  # [5 7 9]

# 标量乘法
d = 2 * a
print(f"2 * a = {d}")  # [2 4 6]

# 点积
dot_product = np.dot(a, b)
print(f"a · b = {dot_product}")  # 32

# 向量的模
norm_a = np.linalg.norm(a)
print(f"|a| = {norm_a:.2f}")  # 3.74

# 单位向量
unit_a = a / norm_a
print(f"单位向量: {unit_a}")
```

#### 示例2：矩阵运算

```python
import numpy as np

# 创建矩阵
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# 矩阵加法
C = A + B
print("A + B =")
print(C)

# 矩阵乘法
D = np.dot(A, B)
print("\nA × B =")
print(D)

# 矩阵转置
A_T = A.T
print("\nA的转置 =")
print(A_T)

# 单位矩阵
I = np.eye(2)
print("\n单位矩阵 =")
print(I)

# 矩阵与单位矩阵相乘
print("\nA × I =")
print(np.dot(A, I))
```

#### 示例3：线性变换

```python
import numpy as np
import matplotlib.pyplot as plt

# 定义原始向量
v = np.array([1, 0])

# 旋转矩阵（旋转45度）
theta = np.pi / 4
R = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta), np.cos(theta)]])

# 应用旋转
v_rotated = np.dot(R, v)

print(f"原始向量: {v}")
print(f"旋转后: {v_rotated}")

# 缩放矩阵
S = np.array([[2, 0],
              [0, 0.5]])

# 应用缩放
v_scaled = np.dot(S, v)
print(f"缩放后: {v_scaled}")
```

#### 示例4：特征值与特征向量

```python
import numpy as np

# 创建矩阵
A = np.array([[4, 1],
              [2, 3]])

# 计算特征值和特征向量
eigenvalues, eigenvectors = np.linalg.eig(A)

print("特征值:")
print(eigenvalues)

print("\n特征向量（列向量）:")
print(eigenvectors)

# 验证：Av = λv
for i in range(len(eigenvalues)):
    lambda_i = eigenvalues[i]
    v_i = eigenvectors[:, i]
    Av = np.dot(A, v_i)
    lambda_v = lambda_i * v_i
    print(f"\n验证特征值 {lambda_i:.2f}:")
    print(f"A·v = {Av}")
    print(f"λ·v = {lambda_v}")
    print(f"是否相等: {np.allclose(Av, lambda_v)}")
```

### 4.3 常见错误与排查

**错误1**：矩阵维度不匹配
```python
# 错误
A = np.array([[1, 2], [3, 4]])  # 2×2
B = np.array([[1, 2, 3]])       # 1×3
C = np.dot(A, B)  # 错误：维度不匹配

# 正确
B = np.array([[1, 2], [3, 4]])   # 2×2
C = np.dot(A, B)  # 正确
```

**错误2**：混淆点积和元素乘积
```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# 点积（标量）
dot = np.dot(a, b)  # 32

# 元素乘积（向量）
element_wise = a * b  # [4, 10, 18]
```

**错误3**：矩阵乘法使用*而不是dot
```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# 错误：元素乘积
C = A * B  # [[5, 12], [21, 32]]

# 正确：矩阵乘法
C = np.dot(A, B)  # [[19, 22], [43, 50]]
```

### 4.4 性能/工程化小技巧

1. **使用NumPy的向量化操作**
```python
# 慢：循环
result = []
for i in range(len(a)):
    result.append(a[i] * b[i])

# 快：向量化
result = a * b
```

2. **使用广播机制**
```python
# 矩阵的每一行都加上向量
A = np.array([[1, 2], [3, 4]])
v = np.array([1, 1])
result = A + v  # 广播
```

3. **使用矩阵乘法代替循环**
```python
# 慢：循环
result = []
for row in A:
    result.append(np.dot(row, x))

# 快：矩阵乘法
result = np.dot(A, x)
```

---

## 5. 动手练习

### 基础练习（3-5题）

**练习1：向量运算**
实现向量的加法、减法、点积、模长计算。

**练习2：矩阵运算**
实现矩阵的加法、乘法、转置、行列式计算。

**练习3：线性变换**
实现旋转、缩放、剪切等线性变换。

### 进阶练习（2-3题）

**练习1：特征值分解**
实现矩阵的特征值分解，并验证结果。

**练习2：矩阵的幂**
使用特征值分解计算矩阵的高次幂。

### 挑战练习（1-2题）

**练习1：PCA实现**
使用特征值分解实现主成分分析（PCA）。

---

## 6. 实际案例

### 案例：图像旋转和缩放

**业务背景**：
需要对图像进行旋转和缩放操作。

**问题抽象**：
- 图像可以表示为像素矩阵
- 旋转和缩放是线性变换
- 可以用矩阵乘法实现

**端到端实现**：
```python
import numpy as np
import matplotlib.pyplot as plt

def rotate_point(x, y, angle):
    """旋转点"""
    R = np.array([[np.cos(angle), -np.sin(angle)],
                  [np.sin(angle), np.cos(angle)]])
    point = np.array([x, y])
    rotated = np.dot(R, point)
    return rotated[0], rotated[1]

def scale_point(x, y, sx, sy):
    """缩放点"""
    S = np.array([[sx, 0],
                  [0, sy]])
    point = np.array([x, y])
    scaled = np.dot(S, point)
    return scaled[0], scaled[1]

# 示例：旋转和缩放一个矩形
points = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])

# 旋转45度
angle = np.pi / 4
rotated_points = np.array([rotate_point(p[0], p[1], angle) for p in points])

# 缩放2倍
scaled_points = np.array([scale_point(p[0], p[1], 2, 2) for p in points])

print("原始点:", points)
print("旋转后:", rotated_points)
print("缩放后:", scaled_points)
```

**结果解读**：
- 线性变换可以用于图像处理
- 矩阵乘法实现高效的批量变换

**改进方向**：
- 实现完整的图像处理函数
- 添加插值处理
- 优化性能

---

## 7. 自我评估

### 概念题

**选择题**（每题2分，共20分）

1. 向量的点积结果是？
   A. 向量  B. 矩阵  C. 标量  D. 张量
   **答案**：C

2. 矩阵A(m×n)和B(n×p)相乘，结果是？
   A. m×p  B. n×n  C. m×n  D. p×p
   **答案**：A

3. 单位矩阵的特点？
   A. 所有元素为1  B. 主对角线为1  C. 对称  D. 可逆
   **答案**：B

4. 特征值方程是？
   A. Av = v  B. Av = λv  C. A = λv  D. v = λA
   **答案**：B

5. 矩阵转置的作用是？
   A. 旋转  B. 交换行列  C. 求逆  D. 缩放
   **答案**：B

**简答题**（每题10分，共40分）

1. 解释向量的几何意义。
   **参考答案**：向量是有方向和大小的量，可以用箭头表示。

2. 说明矩阵乘法的几何意义。
   **参考答案**：矩阵乘法表示线性变换，可以对空间进行旋转、缩放、剪切等操作。

3. 解释特征值和特征向量的意义。
   **参考答案**：特征向量是变换后方向不变的向量，特征值是缩放倍数。

4. 说明线性代数在AI中的应用。
   **参考答案**：神经网络中的权重矩阵、数据表示、降维、相似度计算等。

### 编程实践题（20分）

使用NumPy实现矩阵的特征值分解。

**参考答案**：
```python
import numpy as np

def eig_decomposition(A):
    eigenvalues, eigenvectors = np.linalg.eig(A)
    P = eigenvectors
    D = np.diag(eigenvalues)
    P_inv = np.linalg.inv(P)
    A_reconstructed = np.dot(np.dot(P, D), P_inv)
    return eigenvalues, eigenvectors, A_reconstructed
```

### 综合应用题（20分）

实现一个简单的PCA算法，使用特征值分解进行降维。

**总分**：100分，≥80分为通过

---

## 8. 拓展学习

### 论文/书籍/优质课程

**书籍推荐**：
- 《线性代数应该这样学》- Sheldon Axler
- 《线性代数的本质》- 3Blue1Brown（视频）
- 《深度学习》- Ian Goodfellow（第2章）

**在线资源**：
- 3Blue1Brown线性代数系列（强烈推荐可视化）
- Khan Academy线性代数课程
- MIT线性代数公开课

### 相关工具与库

- **NumPy**：线性代数计算
- **SciPy**：科学计算（包含更多线性代数函数）
- **Matplotlib**：可视化

### 进阶话题指引

完成本课程后，可以学习：
- **奇异值分解（SVD）**：矩阵分解
- **张量**：多维数组
- **矩阵微积分**：梯度计算

### 下节课预告

下一课将学习：
- **02_概率统计**：概率分布、统计推断
- 概率统计是机器学习的理论基础

### 学习建议

1. **可视化理解**：多看3Blue1Brown的视频
2. **多练习**：通过计算加深理解
3. **结合应用**：理解在AI中的实际应用
4. **持续学习**：线性代数是AI的基础，需要扎实掌握

---

**恭喜完成第一课！你已经掌握了线性代数的基础，准备好学习概率统计了！**

